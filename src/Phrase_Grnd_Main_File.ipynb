{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Phrase Grounding:\n",
    "# flickr has sentences and then refering query you get that query and pass sentences\n",
    "# vg, referit has sentence which is query you train on that and test on that\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0511 04:33:44.028072 140472653494080 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import done\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow_hub as hub\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "config = tf.ConfigProto(gpu_options=gpu_options,log_device_placement=True,allow_soft_placement=True)\n",
    "import lmdb\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pycocotools.mask as maskUtils\n",
    "options = tf.RunOptions(trace_level=tf.RunOptions.SOFTWARE_TRACE)\n",
    "\n",
    "sys.path.append('../utilities/')\n",
    "if 'utils_data_gen' in sys.modules:\n",
    "    del sys.modules['utils_data_gen']\n",
    "from utils_data_gen import *\n",
    "\n",
    "if 'utils_evaluation' in sys.modules:\n",
    "    del sys.modules['utils_evaluation']\n",
    "from utils_evaluation import *\n",
    "\n",
    "if 'utils_model' in sys.modules:\n",
    "    del sys.modules['utils_model']\n",
    "from utils_model import *\n",
    "\n",
    "print('Import done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "cloud = 0\n",
    "print(tf.contrib.eager.num_gpus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cloud ==1:\n",
    "    os.environ[\"TFHUB_CACHE_DIR\"] = \"/home/sonamgoenka/tfhub_modules\"\n",
    "    slim_models_path = '/home/david_hc95/grounding/iccv19_grounding/models/'\n",
    "    sys.path.append(slim_models_path)\n",
    "    data_path = '/home/ha2436/data/'\n",
    "else:\n",
    "    os.environ[\"TFHUB_CACHE_DIR\"] = \"/home/sonam/tfhub_modules\"\n",
    "    slim_models_path = '/home/sonam/models/'\n",
    "    sys.path.append(slim_models_path)\n",
    "    data_path = '/dvmm-filer2/datasets/Groundings/data/'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coco sentences has sentence, segmentation, Vg has query\n",
    "# MS COCO\n",
    "data_path_coco = data_path + 'mscoco/'\n",
    "dict_paths_coco = [data_path_coco + 'train2014.pickle',\n",
    "                  data_path_coco + 'val2014.pickle']\n",
    "lmdb_path_coco = data_path_coco + 'MSCOCO_jpg.lmdb'\n",
    "\n",
    "# Flickr It\n",
    "data_path_flickr = data_path + 'flickr30k/'\n",
    "dict_paths_flickr = [data_path_flickr + 'flickr30k_val.pickle',\n",
    "                  data_path_flickr + 'flickr30k_test.pickle']\n",
    "#lmdb_path_refer = data_path_refer + 'MSCOCO_jpg.lmdb' \n",
    "\n",
    "\n",
    "# Visual Genome\n",
    "data_path_vg  = data_path + 'visualgenome/'\n",
    "dict_paths_vg = [data_path_vg + 'train.pickle',\n",
    "                 data_path_vg + 'val2014.pickle',\n",
    "                data_path_vg + 'test.pickle']\n",
    "\n",
    "lmdb_path_vg = data_path_vg + 'data.lmdb'\n",
    "\n",
    "# Refer It\n",
    "\n",
    "ref_path = data_path + 'referit/referit_splits/'\n",
    "lmdb_path_ref =  data_path + 'referit/refclef_data/all_data.lmdb'\n",
    "\n",
    "ref_dict_path = ref_path + 'test.pickle' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Mscoco...\n",
      "Loading Flickr30k...\n",
      "Loading VG...\n",
      "Loading Refer...\n",
      "Loading Done\n"
     ]
    }
   ],
   "source": [
    "#Loading mscoco data\n",
    "print('Loading Mscoco...')\n",
    "with open(dict_paths_coco[0], 'rb') as f:\n",
    "    dict_train_coco = pickle.load(f, encoding='latin1')\n",
    "    ids_train_coco = list(dict_train_coco.keys())   \n",
    "# with open(dict_paths_coco[1], 'rb') as f:\n",
    "#     dict_val_coco = pickle.load(f, encoding='latin1')\n",
    "#     ids_val_coco = list(dict_val_coco.keys())\n",
    "lmdb_env_coco = lmdb.open(lmdb_path_coco, map_size=int(1e11), readonly=True, lock=False)\n",
    "txn_coco = lmdb_env_coco.begin(write=False)\n",
    "\n",
    "#Loading Flickr, dict contains image\n",
    "print('Loading Flickr30k...')\n",
    "with open(dict_paths_flickr[1], 'rb') as f:\n",
    "    dict_test_flickr = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "#loading VG\n",
    "print('Loading VG...')\n",
    "with open(dict_paths_vg[0], 'rb') as f:\n",
    "    dict_train_vg = pickle.load(f, encoding='latin1')\n",
    "    ids_train_vg = list(dict_train_vg.keys())\n",
    "    \n",
    "with open(dict_paths_vg[2], 'rb') as f:\n",
    "    dict_test_vg = pickle.load(f, encoding='latin1')\n",
    "    ids_test_vg = list(dict_test_vg.keys())\n",
    "\n",
    "\n",
    "lmdb_env_vg = lmdb.open(lmdb_path_vg, map_size=int(1e11), readonly=True, lock=False)\n",
    "txn_vg = lmdb_env_vg.begin(write=False)\n",
    "\n",
    "#loading Refer\n",
    "print('Loading Refer...')\n",
    "with open(ref_dict_path, 'rb') as f:\n",
    "    dict_test_ref = pickle.load(f, encoding='latin1')\n",
    "    ids_test_ref = list(dict_test_ref.keys())\n",
    "    \n",
    "lmdb_env_ref = lmdb.open(lmdb_path_ref, map_size=int(1e11), readonly=True, lock=False)\n",
    "txn_ref = lmdb_env_ref.begin(write=False)\n",
    "\n",
    "print('Loading Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on which dataset\n",
    "dict_train = dict_train_vg\n",
    "ids_train = ids_train_vg\n",
    "txn = txn_vg\n",
    "\n",
    "\n",
    "n_batch = 32\n",
    "gamma_1 = 5.0\n",
    "gamma_2 = 10.0\n",
    "n_iter_per_epoch = int(len(dict_train)/n_batch)\n",
    "n_iter_per_epoch_val = 2  #int(5000/n_batch)\n",
    "n_epochs = 15 #N of epochs\n",
    "reg_val = .0005\n",
    "# visual params\n",
    "lr_value_grnd = .001/2\n",
    "lr_value_seg = .001/4\n",
    "log_file = './logs/vgg/vgg'\n",
    "\n",
    "# Architecture\n",
    "#Temp :'Phrase_Grounding_NewAttn_VG_Seg_COCO_Pyramid_VGG'\n",
    "model_name =  'PG_VGG_New_Pyramid_VG_Seg_context'\n",
    "conv_kernel_size = 3 #1\n",
    "n_layers = 1 #3\n",
    "vis_method = depth_selection_vgg_pyramid # Pyramid\n",
    "segment_branch = 1\n",
    "msk_size = 37 #38 for pnasnet, 37 for vgg\n",
    "visual_model = 'vgg' #pnasnet, vgg\n",
    "batch_gen_method = batch_gen_old#batch_gen[preload images], batch_gen_old , batch_gen_token\n",
    "preload = 0\n",
    "token = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Visual Model...\n",
      "Building Text Model...\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0511 04:37:33.868427 140472653494080 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Heatmaps\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "try:\n",
    "    sess.close()\n",
    "except:\n",
    "    print('Initialising for first time')\n",
    "    \n",
    "sess = tf.InteractiveSession(config=config)\n",
    "mode = tf.placeholder(tf.string, name='mode')\n",
    "isTraining = tf.equal(mode, 'train')\n",
    "regularizer = tf.contrib.layers.l2_regularizer(reg_val)\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    #Building visual model\n",
    "    print('Building Visual Model...')\n",
    "    input_img = tf.placeholder(tf.float32, (None,299,299,3), name='input_img')\n",
    "    if visual_model == 'vgg':\n",
    "        pre_processed_img = pre_process(input_img, 'vgg_preprocessing')\n",
    "        vis_model = pre_trained_load(model_name='vgg_16', image_shape=(None,299,299,3),\n",
    "                                  input_tensor=pre_processed_img, session=sess, is_training=False, global_pool=False)\n",
    "    if visual_model == 'pnasnet':\n",
    "        pre_processed_img = pre_process(input_img, 'inception_preprocessing')\n",
    "        vis_model = pre_trained_load(model_name='pnasnet_large', image_shape=(None,299,299,3),\n",
    "                                 input_tensor=pre_processed_img, session=sess, is_training=False, global_pool=True)\n",
    "        \n",
    "    # Type of Depth Selection\n",
    "    v = vis_method(vis_model,regularizer, conv_kernel_size, n_layers) #(?,1225,1,1024)\n",
    "    \n",
    "    #Building text model\n",
    "    print('Building Text Model...')\n",
    "    text_batch = tf.placeholder('string', shape=[None], name='text_input')\n",
    "    elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "    embeddings = elmo(text_batch, signature=\"default\", as_dict=True)\n",
    "    lstm1_embd = embeddings['lstm_outputs1'] #?xTXD\n",
    "    lstm2_embd = embeddings['lstm_outputs2'] #?xTXD\n",
    "    \n",
    "    \n",
    "    if segment_branch == 1:\n",
    "        seg_mask_in = tf.placeholder(tf.float32, (None,None,msk_size,msk_size), name='seg_mask')\n",
    "        is_seg = tf.placeholder(tf.int32,(1) ,name='seg_vs_ground_train')\n",
    "        is_seg = tf.reshape(is_seg, [])\n",
    "        w_embedding = tf.identity(embeddings['elmo'], name='elmo_word_embd')\n",
    "        w_embedding  = tf.cond(tf.equal(is_seg, 1), \n",
    "                               lambda: tf.identity(tf.concat([embeddings['word_emb'],embeddings['word_emb']],-1), name='elmo_word'),\n",
    "                               lambda: tf.identity(embeddings['elmo'], name='elmo_word_embd'))\n",
    "    else:\n",
    "        w_embedding = tf.identity(embeddings['elmo'], name='elmo_word_embd') #?xTXD  \n",
    "   \n",
    "   \n",
    "    \n",
    "    idx = embeddings['sequence_len']-1\n",
    "    # generates first dimension batch sixe, other length \n",
    "    batch_idx = tf.stack([tf.range(0,tf.size(idx),1),idx],axis=1)\n",
    "\n",
    "    dim = lstm1_embd.get_shape().as_list()[-1]\n",
    "    \n",
    "    # Concatenate first of backward with last of forward to get sentence embeddings\n",
    "    sen_lstm_1 = tf.concat([lstm1_embd[:,0,int(dim/2):],\n",
    "                            tf.gather_nd(lstm1_embd[:,:,:int(dim/2)],batch_idx)], axis=-1) #[batch,dim]\n",
    "    sen_lstm_2 = tf.concat([lstm2_embd[:,0,int(dim/2):],\n",
    "                            tf.gather_nd(lstm2_embd[:,:,:int(dim/2)],batch_idx)], axis=-1) #[batch,dim]\n",
    "    sen_lstm_both = tf.concat([tf.expand_dims(sen_lstm_1,axis=2),\n",
    "                               tf.expand_dims(sen_lstm_2,axis=2)], axis=2, name='elmo_sen_embd') #[batch,dim,2]\n",
    "    \n",
    "    sen_embedding = tf.layers.dense(sen_lstm_both, units=1,use_bias=False) #?xDx1\n",
    "    sen_embedding = tf.squeeze(sen_embedding,axis=2) \n",
    "    sen_embedding = tf.layers.dense(sen_embedding, units=1024) #batch*dim\n",
    "    sen_embedding = tf.nn.leaky_relu(sen_embedding,alpha=.25)\n",
    "    sen_embedding = tf.layers.dense(sen_embedding, units=1024)\n",
    "    sen_embedding = tf.nn.leaky_relu(sen_embedding,alpha=.25)\n",
    "    sen_embedding = tf.nn.l2_normalize(sen_embedding, axis=-1, name='sen_embedding')\n",
    "        \n",
    "    w_embedding = tf.layers.dense(w_embedding, units=1024)\n",
    "    w_embedding = tf.nn.leaky_relu(w_embedding,alpha=.25)\n",
    "    w_embedding = tf.layers.dense(w_embedding, units=1024)\n",
    "    w_embedding = tf.nn.leaky_relu(w_embedding,alpha=.25)\n",
    "    w_embedding = tf.nn.l2_normalize(w_embedding, axis=-1, name='w_embedding')\n",
    "    \n",
    "    print('Generating Heatmaps')\n",
    "    heatmap_w,heatmap_s,R_i,R_s = attn(w_embedding,v,sen_embedding)\n",
    "    \n",
    "    if segment_branch == 1:\n",
    "        with tf.variable_scope('segmentation'):\n",
    "            h_w, _, score_w, _ = attn(w_embedding,v,sen_embedding)\n",
    "            h_w = tf.reshape(h_w,[tf.shape(h_w)[0],tf.shape(h_w)[1],h_w.shape[2]*h_w.shape[3]])\n",
    "            #flatten\n",
    "            seg_mask_r = tf.reshape(seg_mask_in,[tf.shape(seg_mask_in)[0],tf.shape(seg_mask_in)[1],\n",
    "                                        seg_mask_in.shape[2]*seg_mask_in.shape[3]])\n",
    "            seg_mask_r = seg_mask_r[:,:tf.shape(h_w)[1],:]\n",
    "            \n",
    "        loss_seg = seg_loss(h_w, seg_mask_r) + tf.losses.get_regularization_loss()\n",
    "        \n",
    "    if token == 1:\n",
    "        word_mask = tf.placeholder(tf.float32, (None,None), name='word_mask')\n",
    "        loss_grnd = attn_loss_token(w_embedding,v,sen_embedding, gamma_1, gamma_2,word_mask) + tf.losses.get_regularization_loss()\n",
    "    else:\n",
    "        loss_grnd = attn_loss(w_embedding,v,sen_embedding, gamma_1, gamma_2) + tf.losses.get_regularization_loss()\n",
    "\n",
    "    \n",
    "lr = tf.placeholder(tf.float32, shape=[], name='learning_rate')\n",
    "opt = tf.train.AdamOptimizer(lr)\n",
    "train_vars = list(set(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)) -  set(vis_model.model_weights))\n",
    "train_op_grnd = opt.minimize(loss_grnd, var_list=train_vars)\n",
    "\n",
    "if segment_branch == 1:\n",
    "    train_op_seg = opt.minimize(loss_seg, var_list=train_vars)\n",
    "\n",
    "global_saver = tf.train.Saver()\n",
    "train_writer = tf.summary.FileWriter(log_file, sess.graph)\n",
    "print('Model Built')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if preload == 1:\n",
    "    valid_ids_train, train_images = preload_images(ids_train[:100], txn)    \n",
    "    print('Done pre-processing training images')\n",
    "    print('train images before',len(ids_train), 'after', len(valid_ids_train))\n",
    "# if segment_branch == 1:\n",
    "#     valid_ids_train_coco, train_images_coco = preload_images(ids_train_coco, txn_coco) \n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "Loading visual path model (InceptionV4)...\n",
      "INFO:tensorflow:Restoring parameters from /home/sonam/models/vgg_16.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0511 04:37:47.699005 140472653494080 saver.py:1270] Restoring parameters from /home/sonam/models/vgg_16.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "print('Initializing...')\n",
    "_ = sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#loading pretrained inception weights\n",
    "print('Loading visual path model (InceptionV4)...')\n",
    "vis_model.load_weights()\n",
    "print('Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    }
   ],
   "source": [
    "print('Start training...')\n",
    "validation_loss = np.zeros((n_epochs,))\n",
    "train_loss = np.zeros((n_epochs,))\n",
    "train_grnd_loss = np.zeros((n_epochs,))\n",
    "train_seg_loss = np.zeros((n_epochs,))\n",
    "val_grnd_loss = np.zeros((n_epochs,))\n",
    "val_seg_loss = np.zeros((n_epochs,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=====Epoch: 0\n",
      "===Train\n",
      "saving model/77376, grnd_loss_value:10.66, seg_loss_value:1.01, overall_loss:11.67\n",
      "model saved\n",
      "\n",
      "\n",
      "=====Epoch: 1\n",
      "===Train\n",
      "Sample 19392/77376, grnd_loss_value:10.02, seg_loss_value:0.97, overall_loss:10.99\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 77344/77376, grnd_loss_value:9.61, seg_loss_value:0.96, overall_loss:10.57\n",
      "\n",
      "=====Epoch: 3\n",
      "===Train\n",
      "Sample 77344/77376, grnd_loss_value:9.40, seg_loss_value:0.97, overall_loss:10.37\n",
      "\n",
      "=====Epoch: 4\n",
      "===Train\n",
      "saving model/77376, grnd_loss_value:9.30, seg_loss_value:0.96, overall_loss:10.26\n",
      "model saved\n",
      "\n",
      "\n",
      "=====Epoch: 5\n",
      "===Train\n",
      "Sample 77344/77376, grnd_loss_value:9.21, seg_loss_value:0.97, overall_loss:10.18\n",
      "\n",
      "=====Epoch: 6\n",
      "===Train\n",
      "Sample 38112/77376, grnd_loss_value:9.14, seg_loss_value:0.98, overall_loss:10.12\r"
     ]
    }
   ],
   "source": [
    "for e in range(n_epochs):\n",
    "    print('\\n\\n=====Epoch: %d'%e)\n",
    "    #train phase\n",
    "    avg_loss = [0,0]\n",
    "   \n",
    "    \n",
    "    if e < 9:\n",
    "        lr_value_grnd = lr_value_grnd\n",
    "        lr_value_seg = lr_value_seg\n",
    "        \n",
    "    elif 9 <= e < 14:\n",
    "        lr_value_grnd = lr_value_grnd/2.0\n",
    "        lr_value_seg = lr_value_seg/2.0\n",
    "       \n",
    "    elif e >= 14:\n",
    "        lr_value_grnd = lr_value_grnd/4.0\n",
    "        lr_value_seg = lr_value_seg/4.0\n",
    "         \n",
    "   \n",
    "    print('===Train')\n",
    "    \n",
    "    if segment_branch == 1:\n",
    "        for i in range(n_iter_per_epoch):\n",
    "            \n",
    "            if preload ==0:\n",
    "                img_batch, cap_batch = batch_gen_method(ids_train, dict_train, txn_vg, n_batch)\n",
    "            else:\n",
    "                img_batch, cap_batch = batch_gen_method(valid_ids_train, dict_train, train_images, n_batch)\n",
    "                \n",
    "            cat_coco_batch, msk_batch, img_coco_batch  = batch_gen_seg(ids_train_coco, dict_train_coco, txn_coco, n_batch, msk_size)\n",
    "        \n",
    "            feed_dict = {input_img: img_batch, text_batch: cap_batch, mode: 'train', lr: lr_value_grnd,is_seg: 0}\n",
    "            loss_grnd_value, _ = sess.run([loss_grnd, train_op_grnd], feed_dict)\n",
    "            \n",
    "            feed_dict.update({input_img: img_coco_batch, text_batch: cat_coco_batch, seg_mask_in:msk_batch,is_seg :1,lr: lr_value_seg})\n",
    "            loss_seg_value, _ = sess.run([loss_seg, train_op_seg], feed_dict) \n",
    "            avg_loss[0]+=loss_grnd_value\n",
    "            avg_loss[1]+=loss_seg_value\n",
    "            \n",
    "            v = [i*n_batch,n_iter_per_epoch*n_batch,avg_loss[0]/float(i+1),avg_loss[1]/float(i+1), (avg_loss[0]+avg_loss[1])/float(i+1)]\n",
    "            sys.stdout.write(f'Sample {v[0]}/{v[1]}, grnd_loss_value:{v[2]:.2f}, seg_loss_value:{v[3]:.2f}, overall_loss:{v[4]:.2f}\\r')\n",
    "            sys.stdout.flush()\n",
    "            train_loss[e] = (avg_loss[0]+avg_loss[1])/float(n_iter_per_epoch+1)\n",
    "            train_grnd_loss[e] = (avg_loss[0])/float(n_iter_per_epoch+1)\n",
    "            train_seg_loss[e] = (avg_loss[1])/float(n_iter_per_epoch+1)         \n",
    "    else:\n",
    "        for i in range(n_iter_per_epoch):\n",
    "            if token == 1:\n",
    "                img_batch, cap_batch, wrd_mask_batch  = batch_gen_token(valid_ids_train, dict_train, train_images, n_batch)\n",
    "                feed_dict = {input_img: img_batch, text_batch: cap_batch, mode: 'train', lr: lr_value_grnd, word_mask:wrd_mask_batch}\n",
    "            \n",
    "            else:\n",
    "                if preload ==0:\n",
    "                    img_batch, cap_batch = batch_gen_method(ids_train, dict_train, txn_vg, n_batch)\n",
    "                else:\n",
    "                    img_batch, cap_batch = batch_gen_method(valid_ids_train, dict_train, train_images, n_batch)\n",
    "\n",
    "                feed_dict = {input_img: img_batch, text_batch: cap_batch, mode: 'train', lr: lr_value_grnd}\n",
    "            loss_grnd_value, _ = sess.run([loss_grnd, train_op_grnd], feed_dict)\n",
    "            avg_loss[0]+=loss_grnd_value\n",
    "            v = [i*n_batch,n_iter_per_epoch*n_batch,avg_loss[0]/float(i+1),avg_loss[1]/float(i+1), (avg_loss[0]+avg_loss[1])/float(i+1)]\n",
    "            sys.stdout.write(f'Sample {v[0]}/{v[1]}, grnd_loss_value:{v[2]:.2f}, seg_loss_value:{v[3]:.2f}, overall_loss:{v[4]:.2f}\\r')\n",
    "            sys.stdout.flush()\n",
    "            train_loss[e] = (avg_loss[0]+avg_loss[1])/float(n_iter_per_epoch+1)\n",
    "    \n",
    "    if e %4 == 0:\n",
    "        print('saving model')\n",
    "        global_saver.save(sess, model_name + str(e))\n",
    "        print('model saved')\n",
    "    \n",
    "print('Training done.')\n",
    "#saving the session\n",
    "print('Saving model...')\n",
    "global_saver.save(sess,   model_name + str(n_epochs))\n",
    "print('Saving done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(validation_loss, label = 'total_val')\n",
    "plt.plot(train_seg_loss, label='train_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If loading pretrain model run this\n",
    "load_name = 'PG_VGG_New_Pyramid_VG_Seg15'\n",
    "model_path = '../Models_Train/' + load_name\n",
    "print('Loading grounding pretrained model...')\n",
    "print(model_path)\n",
    "sess, graph = load_model(model_path,config)\n",
    "print('Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Right Tensors\n",
    "R_i = sess.graph.get_tensor_by_name(\"attention/score_word:0\")#/transpose_2\n",
    "R_s = sess.graph.get_tensor_by_name(\"attention/score_sentence:0\")\n",
    "heatmap_w = sess.graph.get_tensor_by_name(\"attention/heatmap_word:0\")\n",
    "heatmap_s = sess.graph.get_tensor_by_name(\"attention/heatmap_sentence:0\")\n",
    "lvl_idx_wrd = sess.graph.get_tensor_by_name('attention/level_index_word:0')\n",
    "lvl_scr_wrd = sess.graph.get_tensor_by_name('attention/level_score_word:0')\n",
    "lvl_idx_sen = sess.graph.get_tensor_by_name('attention/level_index_sentence:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flickr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flickr\n",
    "print('Test on Flickr30k\\n')\n",
    "num_tst = len(dict_test_flickr)\n",
    "seg = 0\n",
    "token = 0\n",
    "iou_acc_f30k, hit_acc_f30k, att_acc_f30k = validate_flickr30k(dict_test_flickr, num_tst,heatmap_w, R_i,sess, seg, token)\n",
    "print('done testing')\n",
    "# Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Iou',iou_acc_f30k, 'Hit',hit_acc_f30k, 'Att', att_acc_f30k  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../evaluations/train_vg/test_flickr/'+ model_name +'.pickle', 'wb') as f:\n",
    "    pickle.dump({'iou_acc_w':iou_acc_w_f30k,\n",
    "                 'hit_acc_w':hit_acc_w_f30k,\n",
    "                 'att_acc_w':att_acc_w_f30k\n",
    "                },f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refer It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_ids_test_ref, test_images_ref = pre_images(ids_test_ref, txn_ref) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tst_ref = len(dict_test_ref)\n",
    "seg = 0\n",
    "iou_acc_w_ref, hit_acc_w_ref, att_acc_w_ref, iou_acc_s_ref, hit_acc_s_ref, att_acc_s_ref = validate_referit(\n",
    "    dict_test_ref,txn_ref,num_tst_ref, heatmap_w,heatmap_s, R_i,  sess, seg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Iou_w', iou_acc_w_ref,'hit_w', hit_acc_w_ref, 'att_w',att_acc_w_ref,\n",
    "      'iou_s' , iou_acc_s_ref,'hit_s', hit_acc_s_ref,'att_s', att_acc_s_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../evaluations/train_vg/test_referit/'+ model_name +'.pickle', 'wb') as f:\n",
    "    pickle.dump({'iou_acc_w':iou_acc_w_ref,\n",
    "                 'hit_acc_w':hit_acc_w_ref,\n",
    "                 'iou_acc_s':iou_acc_s_ref,\n",
    "                 'hit_acc_s':hit_acc_s_ref,\n",
    "                 'att_acc_w':att_acc_w_ref,\n",
    "                 'att_acc_s':att_acc_s_ref,  \n",
    "                },f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tst_vg = len(dict_test_vg)\n",
    "seg = 1\n",
    "iou_acc_w_vg, hit_acc_w_vg, att_acc_w_vg,iou_acc_s_vg, hit_acc_s_vg ,att_acc_s_vg = validate_vg(\n",
    "    dict_test_vg, txn_vg,num_tst_vg,heatmap_w,heatmap_s, R_i, sess,seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Iou_w', iou_acc_w_vg,'hit_w', hit_acc_w_vg, 'att_w',att_acc_w_vg,\n",
    "      'iou_s' , iou_acc_s_vg,'hit_s', hit_acc_s_vg,'att_s', att_acc_s_vg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../evaluations/train_vg/test_vg/'+ model_name +'.pickle', 'wb') as f:\n",
    "    pickle.dump({'iou_acc_w':iou_acc_w_vg,\n",
    "                 'hit_acc_w':hit_acc_w_vg,\n",
    "                 'iou_acc_s':iou_acc_s_vg,\n",
    "                 'hit_acc_s':hit_acc_s_vg,\n",
    "                 'att_acc_w':att_acc_w_vg,\n",
    "                 'att_acc_s':att_acc_s_vg,  \n",
    "                },f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_name = model_name \n",
    "num_tst = 5\n",
    "generate_report_new(pdf_name, dict_test_flickr,heatmap_w, heatmap_s, R_i, R_s, lvl_idx_wrd, lvl_idx_sen,sess, num_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ms coco Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_name = load_name + '_object_detectiom.pdf'\n",
    "num_tst = 180\n",
    "seg = 1\n",
    "token = 0\n",
    "generate_segmaps(pdf_name, dict_val_coco, txn_coco, heatmap_w, heatmap_s, R_i, R_s, lvl_idx_wrd, lvl_idx_sen,\n",
    "                 sess, num_tst, seg,token)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
