{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import done\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow_hub as hub\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "config = tf.ConfigProto(gpu_options=gpu_options,log_device_placement=True,allow_soft_placement=True)\n",
    "import lmdb\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pycocotools.mask as maskUtils\n",
    "\n",
    "sys.path.append('../utilities/')\n",
    "if 'utils_data_gen' in sys.modules:\n",
    "    del sys.modules['utils_data_gen']\n",
    "from utils_data_gen import *\n",
    "\n",
    "if 'utils_evaluation' in sys.modules:\n",
    "    del sys.modules['utils_evaluation']\n",
    "from utils_evaluation import *\n",
    "\n",
    "if 'utils_model' in sys.modules:\n",
    "    del sys.modules['utils_model']\n",
    "from utils_model import *\n",
    "\n",
    "print('Import done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "cloud = 0\n",
    "print(tf.contrib.eager.num_gpus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cloud ==1:\n",
    "    os.environ[\"TFHUB_CACHE_DIR\"] = \"/home/sonamgoenka/tfhub_modules\"\n",
    "    slim_models_path = '/home/david_hc95/grounding/iccv19_grounding/models/'\n",
    "    sys.path.append(slim_models_path)\n",
    "    data_path = '/home/ha2436/data/'\n",
    "else:\n",
    "    os.environ[\"TFHUB_CACHE_DIR\"] = \"/home/sonam/tfhub_modules\"\n",
    "    slim_models_path = '/home/sonam/models/'\n",
    "    sys.path.append(slim_models_path)\n",
    "    data_path = '/dvmm-filer2/datasets/Groundings/data/'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coco sentences has sentence, segmentation, Vg has query\n",
    "# MS COCO\n",
    "data_path_coco = data_path + 'mscoco/'\n",
    "dict_paths_coco = [data_path_coco + 'train2014.pickle',\n",
    "                  data_path_coco + 'val2014.pickle']\n",
    "lmdb_path_coco = data_path_coco + 'MSCOCO_jpg.lmdb'\n",
    "\n",
    "# Flickr It\n",
    "data_path_flickr = data_path + 'flickr30k/'\n",
    "dict_paths_flickr = [data_path_flickr + 'flickr30k_val.pickle',\n",
    "                  data_path_flickr + 'flickr30k_test.pickle']\n",
    "#lmdb_path_refer = data_path_refer + 'MSCOCO_jpg.lmdb' ?\n",
    "\n",
    "\n",
    "# Visual Genome\n",
    "data_path_vg  = data_path + 'visualgenome/'\n",
    "dict_paths_vg = [data_path_vg + 'train.pickle',\n",
    "                 data_path_vg + 'val2014.pickle',\n",
    "                data_path_vg + 'test.pickle']\n",
    "\n",
    "lmdb_path_vg = data_path_vg + 'data.lmdb'\n",
    "\n",
    "# Refer It\n",
    "\n",
    "ref_path = data_path + 'referit/referit_splits/'\n",
    "lmdb_path_ref =  data_path + 'referit/refclef_data/all_data.lmdb'\n",
    "\n",
    "ref_dict_path = ref_path + 'test.pickle' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Mscoco...\n",
      "Loading Flickr30k...\n",
      "Loading VG...\n",
      "Loading Refer...\n",
      "Loading Done\n"
     ]
    }
   ],
   "source": [
    "#Loading mscoco data\n",
    "print('Loading Mscoco...')\n",
    "with open(dict_paths_coco[0], 'rb') as f:\n",
    "    dict_train_coco = pickle.load(f, encoding='latin1')\n",
    "    ids_train_coco = list(dict_train_coco.keys())   \n",
    "with open(dict_paths_coco[1], 'rb') as f:\n",
    "    dict_val_coco = pickle.load(f, encoding='latin1')\n",
    "    ids_val_coco = list(dict_val_coco.keys())\n",
    "lmdb_env_coco = lmdb.open(lmdb_path_coco, map_size=int(1e11), readonly=True, lock=False)\n",
    "txn_coco = lmdb_env_coco.begin(write=False)\n",
    "\n",
    "#Loading Flickr, dict contains image\n",
    "print('Loading Flickr30k...')\n",
    "with open(dict_paths_flickr[1], 'rb') as f:\n",
    "    dict_test_flickr = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "#loading VG\n",
    "print('Loading VG...')\n",
    "with open(dict_paths_vg[0], 'rb') as f:\n",
    "    dict_train_vg = pickle.load(f, encoding='latin1')\n",
    "    ids_train_vg = list(dict_train_vg.keys())\n",
    "    \n",
    "with open(dict_paths_vg[2], 'rb') as f:\n",
    "    dict_test_vg = pickle.load(f, encoding='latin1')\n",
    "    ids_test_vg = list(dict_test_vg.keys())\n",
    "\n",
    "\n",
    "lmdb_env_vg = lmdb.open(lmdb_path_vg, map_size=int(1e11), readonly=True, lock=False)\n",
    "txn_vg = lmdb_env_vg.begin(write=False)\n",
    "\n",
    "#loading Refer\n",
    "print('Loading Refer...')\n",
    "with open(ref_dict_path, 'rb') as f:\n",
    "    dict_test_ref = pickle.load(f, encoding='latin1')\n",
    "    ids_test_ref = list(dict_test_ref.keys())\n",
    "    \n",
    "lmdb_env_ref = lmdb.open(lmdb_path_ref, map_size=int(1e11), readonly=True, lock=False)\n",
    "txn_ref = lmdb_env_ref.begin(write=False)\n",
    "\n",
    "print('Loading Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(dict_train_vg.keys())[:5])\n",
    "#dict_train_vg['2344382']\n",
    "#print(list(dict_train_coco.keys())[:5])\n",
    "#dict_train_coco['329242']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on which dataset\n",
    "dict_train = dict_train_vg\n",
    "ids_train = ids_train_vg\n",
    "txn = txn_vg\n",
    "\n",
    "n_batch = 4\n",
    "gamma_1 = 5.0\n",
    "gamma_2 = 10.0\n",
    "n_iter_per_epoch = 2  #int(len(dict_train)/n_batch)\n",
    "n_iter_per_epoch_val = 2  #int(5000/n_batch)\n",
    "n_epochs = 1 #N of epochs\n",
    "reg_val = .0005\n",
    "# visual params\n",
    "lr_value = .001\n",
    "log_file = './logs/vgg/vgg'\n",
    "\n",
    "# Architecture\n",
    "#Temp :'Phrase_Grounding_NewAttn_VG_Seg_COCO_Pyramid_VGG'\n",
    "model_name =  './Model_' + 'Phrase_Grounding_NewAttn_VG_VGG'\n",
    "conv_kernel_size = 3 #1\n",
    "n_layers = 1 #3\n",
    "vis_method = depth_selection_newattn_vgg # Pyramid\n",
    "segment_branch = 0\n",
    "msk_size = 38 #38 for pnasnet, 37 for vgg\n",
    "visual_model = 'vgg' #pnasnet, vgg\n",
    "use_new_attn = False\n",
    "latent_space_dim = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Visual Model...\n",
      "WARNING:tensorflow:From ../utilities/utils_model.py:251: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0504 14:34:57.992147 140263084984128 deprecation.py:323] From ../utilities/utils_model.py:251: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Text Model...\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0504 14:34:59.094756 140263084984128 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Heatmaps\n",
      "WARNING:tensorflow:From /home/sonam/anaconda3/envs/tf_gpu_1_13/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0504 14:34:59.890670 140263084984128 deprecation.py:323] From /home/sonam/anaconda3/envs/tf_gpu_1_13/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sonam/anaconda3/envs/tf_gpu_1_13/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0504 14:34:59.907364 140263084984128 deprecation.py:323] From /home/sonam/anaconda3/envs/tf_gpu_1_13/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "try:\n",
    "    sess.close()\n",
    "except:\n",
    "    print('Initialising for first time')\n",
    "    \n",
    "sess = tf.InteractiveSession(config=config)\n",
    "mode = tf.placeholder(tf.string, name='mode')\n",
    "isTraining = tf.equal(mode, 'train')\n",
    "regularizer = tf.contrib.layers.l2_regularizer(reg_val)\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    #Building visual model\n",
    "    print('Building Visual Model...')\n",
    "    input_img = tf.placeholder(tf.float32, (None,299,299,3), name='input_img')\n",
    "    if visual_model == 'vgg':\n",
    "        pre_processed_img = pre_process(input_img, 'vgg_preprocessing')\n",
    "        vis_model = pre_trained_load(model_name='vgg_16', image_shape=(None,299,299,3),\n",
    "                                  input_tensor=pre_processed_img, session=sess, is_training=False, global_pool=False)\n",
    "    if visual_model == 'pnasnet':\n",
    "        pre_processed_img = pre_process(input_img, 'inception_preprocessing')\n",
    "        vis_model = pre_trained_load(model_name='pnasnet_large', image_shape=(None,299,299,3),\n",
    "                                 input_tensor=pre_processed_img, session=sess, is_training=False, global_pool=True)\n",
    "        \n",
    "\n",
    "\n",
    "        # Type of Depth Selection\n",
    "    v = vis_method(vis_model,regularizer, conv_kernel_size, n_layers) #(?,1225,4,1024)\n",
    "    \n",
    "    #building text model\n",
    "    print('Building Text Model...')\n",
    "    text_batch = tf.placeholder('string', shape=[None], name='text_input')\n",
    "    elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "    embeddings = elmo(text_batch, signature=\"default\", as_dict=True)\n",
    "    lstm1_embd = embeddings['lstm_outputs1'] #?xTXD\n",
    "    lstm2_embd = embeddings['lstm_outputs2'] #?xTXD\n",
    "    \n",
    "    \n",
    "    if segment_branch == 1:\n",
    "        seg_mask_in = tf.placeholder(tf.float32, (None,None,msk_size,msk_size), name='seg_mask')\n",
    "        is_seg = tf.placeholder(tf.int32,(1) ,name='seg_vs_ground_train')\n",
    "        is_seg = tf.reshape(is_seg, [])\n",
    "        w_embedding  = tf.cond(tf.equal(is_seg, 1), lambda: tf.identity(embeddings['elmo'], name='elmo_word'), lambda: tf.identity(embeddings['elmo'], name='elmo_word_embd'))\n",
    "    else:\n",
    "        w_embedding = tf.identity(embeddings['elmo'], name='elmo_word_embd') #?xTXD\n",
    "    \n",
    "    idx = embeddings['sequence_len']-1\n",
    "    # generates first dimension batch sixe, other length \n",
    "    batch_idx = tf.stack([tf.range(0,tf.size(idx),1),idx],axis=1)\n",
    "\n",
    "    dim = lstm1_embd.get_shape().as_list()[-1]\n",
    "    \n",
    "    # Concatenate first of backward with last of forward to get sentence embeddings\n",
    "    sen_lstm_1 = tf.concat([lstm1_embd[:,0,int(dim/2):],\n",
    "                            tf.gather_nd(lstm1_embd[:,:,:int(dim/2)],batch_idx)], axis=-1) #[batch,dim]\n",
    "    sen_lstm_2 = tf.concat([lstm2_embd[:,0,int(dim/2):],\n",
    "                            tf.gather_nd(lstm2_embd[:,:,:int(dim/2)],batch_idx)], axis=-1) #[batch,dim]\n",
    "    sen_lstm_both = tf.concat([tf.expand_dims(sen_lstm_1,axis=2),\n",
    "                               tf.expand_dims(sen_lstm_2,axis=2)], axis=2, name='elmo_sen_embd') #[batch,dim,2]\n",
    "    \n",
    "    sen_embedding = tf.layers.dense(sen_lstm_both, units=1,use_bias=False) #?xDx1\n",
    "    sen_embedding = tf.squeeze(sen_embedding,axis=2) \n",
    "    sen_embedding = tf.layers.dense(sen_embedding, units=latent_space_dim) #batch*dim\n",
    "    sen_embedding = tf.nn.leaky_relu(sen_embedding,alpha=.25)\n",
    "    sen_embedding = tf.layers.dense(sen_embedding, units=latent_space_dim)\n",
    "    sen_embedding = tf.nn.leaky_relu(sen_embedding,alpha=.25)\n",
    "    sen_embedding = tf.nn.l2_normalize(sen_embedding, axis=-1, name='sen_embedding')\n",
    "        \n",
    "    w_embedding = tf.layers.dense(w_embedding, units=latent_space_dim)\n",
    "    w_embedding = tf.nn.leaky_relu(w_embedding,alpha=.25)\n",
    "    w_embedding = tf.layers.dense(w_embedding, units=latent_space_dim)\n",
    "    w_embedding = tf.nn.leaky_relu(w_embedding,alpha=.25)\n",
    "    w_embedding = tf.nn.l2_normalize(w_embedding, axis=-1, name='w_embedding')\n",
    "    \n",
    "   \n",
    "    print('Generating Heatmaps')\n",
    "    if use_new_attn:\n",
    "        heatmap_w,heatmap_s = attn_new(w_embedding,v,sen_embedding)\n",
    "    else:\n",
    "        heatmap_w,heatmap_s,R_i,R_s = attn(w_embedding,v,sen_embedding)\n",
    "    \n",
    "    \n",
    "    if segment_branch == 1:\n",
    "        with tf.variable_scope('segmentation'):\n",
    "            if use_new_attn:\n",
    "                heatmap_w, _, _ ,_  = attn_new(w_embedding,v,sen_embedding)\n",
    "            else:\n",
    "                h_w, _, score_w, _ = attn(w_embedding,v,sen_embedding)\n",
    "            h_w = tf.reshape(h_w,[tf.shape(h_w)[0],tf.shape(h_w)[1],h_w.shape[2]*h_w.shape[3]])\n",
    "            #flatten\n",
    "            seg_mask_r = tf.reshape(seg_mask_in,[tf.shape(seg_mask_in)[0],tf.shape(seg_mask_in)[1],\n",
    "                                        seg_mask_in.shape[2]*seg_mask_in.shape[3]])\n",
    "            seg_mask_r = seg_mask_r[:,:tf.shape(h_w)[1],:]\n",
    "            \n",
    "        loss_seg = seg_loss(h_w, seg_mask_r) + tf.losses.get_regularization_loss()\n",
    "        \n",
    "if use_new_attn:   \n",
    "    loss_grnd = attn_loss_new(w_embedding,v,sen_embedding, gamma_1, gamma_2) + tf.losses.get_regularization_loss()\n",
    "else:\n",
    "    loss_grnd = attn_loss(w_embedding,v,sen_embedding, gamma_1, gamma_2) + tf.losses.get_regularization_loss()\n",
    "    \n",
    "lr = tf.placeholder(tf.float32, shape=[], name='learning_rate')\n",
    "opt = tf.train.AdamOptimizer(lr)\n",
    "train_vars = list(set(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)) -  set(vis_model.model_weights))\n",
    "train_op_grnd = opt.minimize(loss_grnd, var_list=train_vars)\n",
    "\n",
    "if segment_branch == 1:\n",
    "    train_op_seg = opt.minimize(loss_seg, var_list=train_vars)\n",
    "\n",
    "global_saver = tf.train.Saver()\n",
    "train_writer = tf.summary.FileWriter(log_file, sess.graph)\n",
    "print('Model Built')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  0  images\n",
      "Finished preprocessing images.\n",
      "Done pre-processing training images\n",
      "train images before 77398 after 98\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "valid_ids_train, train_images = preload_images(ids_train[:100], txn)    \n",
    "print('Done pre-processing training images')\n",
    "print('train images before',len(ids_train), 'after', len(valid_ids_train))\n",
    "if segment_branch == 1:\n",
    "    valid_ids_train_coco, train_images_coco = preload_images(ids_train_coco[:100], txn_coco) \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading visual path model (InceptionV4)...\n",
      "WARNING:tensorflow:From /home/sonam/anaconda3/envs/tf_gpu_1_13/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0504 14:35:15.530646 140263084984128 deprecation.py:323] From /home/sonam/anaconda3/envs/tf_gpu_1_13/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/sonam/models/vgg_16.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0504 14:35:15.537753 140263084984128 saver.py:1270] Restoring parameters from /home/sonam/models/vgg_16.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "# print('Initializing...')\n",
    "_ = sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#loading pretrained inception weights\n",
    "print('Loading visual path model (InceptionV4)...')\n",
    "vis_model.load_weights()\n",
    "print('Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "\n",
      "=====Epoch: 0\n",
      "===Train\n",
      "Training done.nd_loss_value:6.53, seg_loss_value:0.00, overall_loss:6.53\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "print('Start training...')\n",
    "validation_loss = np.zeros((n_epochs,))\n",
    "train_loss = np.zeros((n_epochs,))\n",
    "train_grnd_loss = np.zeros((n_epochs,))\n",
    "train_seg_loss = np.zeros((n_epochs,))\n",
    "val_grnd_loss = np.zeros((n_epochs,))\n",
    "val_seg_loss = np.zeros((n_epochs,))\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    print('\\n\\n=====Epoch: %d'%e)\n",
    "    #train phase\n",
    "    avg_loss = [0,0]\n",
    "    \n",
    "    if e < 9:\n",
    "        lr_value = lr_value\n",
    "    elif 9 <= e < 14:\n",
    "        lr_value = lr_value / 2.0\n",
    "    elif e >= 14:\n",
    "        lr_value = lr_value/ 4.0 \n",
    "   \n",
    "    print('===Train')\n",
    "    \n",
    "    if segment_branch == 1:\n",
    "        for i in range(n_iter_per_epoch):\n",
    "            \n",
    "            img_batch, cap_batch = batch_gen(valid_ids_train, dict_train, train_images, n_batch)\n",
    "            cat_batch, msk_batch  = batch_gen_seg_add_cat_map(valid_ids_train_coco, dict_train_coco, train_images_coco, n_batch, msk_size)\n",
    "            feed_dict = {input_img: img_batch, text_batch: cap_batch, mode: 'train', lr: lr_value,is_seg: 0}\n",
    "            loss_grnd_value, _ = sess.run([loss_grnd, train_op_grnd], feed_dict)\n",
    "            feed_dict.update({text_batch: cat_batch, seg_mask_in:msk_batch,is_seg :1})\n",
    "            loss_seg_value, _ = sess.run([loss_seg, train_op_seg], feed_dict) \n",
    "            avg_loss[0]+=loss_grnd_value\n",
    "            avg_loss[1]+=loss_seg_value\n",
    "            v = [i*n_batch,n_iter_per_epoch*n_batch,avg_loss[0]/float(i+1),avg_loss[1]/float(i+1), (avg_loss[0]+avg_loss[1])/float(i+1)]\n",
    "            sys.stdout.write(f'Sample {v[0]}/{v[1]}, grnd_loss_value:{v[2]:.2f}, seg_loss_value:{v[3]:.2f}, overall_loss:{v[4]:.2f}\\r')\n",
    "            sys.stdout.flush()\n",
    "            train_loss[e] = (avg_loss[0]+avg_loss[1])/float(n_iter_per_epoch+1)\n",
    "            train_grnd_loss[e] = (avg_loss[0])/float(n_iter_per_epoch+1)\n",
    "            train_seg_loss[e] = (avg_loss[1])/float(n_iter_per_epoch+1)         \n",
    "    else:\n",
    "        for i in range(n_iter_per_epoch):\n",
    "            img_batch, cap_batch = batch_gen(valid_ids_train, dict_train, train_images, n_batch)\n",
    "            feed_dict = {input_img: img_batch, text_batch: cap_batch, mode: 'train', lr: lr_value}\n",
    "            loss_grnd_value, _ = sess.run([loss_grnd, train_op_grnd], feed_dict)\n",
    "            avg_loss[0]+=loss_grnd_value\n",
    "            v = [i*n_batch,n_iter_per_epoch*n_batch,avg_loss[0]/float(i+1),avg_loss[1]/float(i+1), (avg_loss[0]+avg_loss[1])/float(i+1)]\n",
    "            sys.stdout.write(f'Sample {v[0]}/{v[1]}, grnd_loss_value:{v[2]:.2f}, seg_loss_value:{v[3]:.2f}, overall_loss:{v[4]:.2f}\\r')\n",
    "            sys.stdout.flush()\n",
    "            train_loss[e] = (avg_loss[0]+avg_loss[1])/float(n_iter_per_epoch+1)\n",
    "    \n",
    "    if e == 10:\n",
    "        print('saving model')\n",
    "        global_saver.save(sess, model_name + str(e))\n",
    "        print('model saved')\n",
    "    \n",
    "print('Training done.')\n",
    "#saving the session\n",
    "print('Saving model...')\n",
    "global_saver.save(sess,  model_name + str(n_epochs))\n",
    "print('Saving done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(validation_loss, label = 'total_val')\n",
    "plt.plot(train_loss, label='train_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If loading pretrain model run this\n",
    "load_name = 'model_AttGrnd_3layerbase_pyramid15'\n",
    "model_path = './Models/' + load_name\n",
    "print('Loading grounding pretrained model...')\n",
    "print(model_path)\n",
    "sess, graph = load_model(model_path,config)\n",
    "print('Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Right Tensors\n",
    "R_i = sess.graph.get_tensor_by_name(\"attention/score_word:0\")#/transpose_2\n",
    "R_s = sess.graph.get_tensor_by_name(\"attention/score_sentence:0\")\n",
    "heatmap_w = sess.graph.get_tensor_by_name(\"attention/heatmap_word:0\")\n",
    "heatmap_s = sess.graph.get_tensor_by_name(\"attention/heatmap_sentence:0\")\n",
    "lvl_idx_wrd = sess.graph.get_tensor_by_name('attention/level_index_word:0')\n",
    "lvl_scr_wrd = sess.graph.get_tensor_by_name('attention/level_score_word:0')\n",
    "lvl_idx_sen = sess.graph.get_tensor_by_name('attention/level_index_sentence:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flickr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flickr\n",
    "print('Test on Flickr30k\\n')\n",
    "num_tst = len(dict_test_flickr)\n",
    "iou_acc_f30k, hit_acc_f30k, att_acc_f30k = validate_flickr30k(dict_test_flickr, num_tst,heatmap_w, R_i, R_s,sess)\n",
    "print('done testing')\n",
    "# Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Iou',iou_acc_f30k, 'Hit',hit_acc_f30k, 'Att', att_acc_f30k  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refer It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_ids_test_ref, test_images_ref = pre_images(ids_test_ref, txn_ref) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tst_ref = len(dict_test_ref)\n",
    "iou_acc_w_ref, hit_acc_w_ref, att_acc_w_ref, iou_acc_s_ref, hit_acc_s_ref, att_acc_s_ref = validate_referit(\n",
    "    dict_test_ref,txn_ref,num_tst_ref, heatmap_w,heatmap_s,R_i, R_s,lvl_idx_wrd, lvl_idx_sen, lvl_scr_wrd, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Iou_w', iou_acc_w_ref,'hit_w', hit_acc_w_ref, 'att_w',att_acc_w_ref,\n",
    "      'iou_s' , iou_acc_s_ref,'hit_s', hit_acc_s_ref,'att_s', att_acc_s_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tst_vg = len(dict_test_vg)\n",
    "#num_tst_vg = 2\n",
    "iou_acc_w_vg, hit_acc_w_vg, att_acc_w_vg,iou_acc_s_vg, hit_acc_s_vg ,att_acc_s_vg = validate_vg(\n",
    "    dict_test_vg, txn_vg,num_tst_vg,heatmap_w,heatmap_s, R_i, R_s, lvl_idx_wrd, lvl_idx_sen, lvl_scr_wrd, sess)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Iou_w', iou_acc_w_vg,'hit_w', hit_acc_w_vg, 'att_w',att_acc_w_vg,\n",
    "      'iou_s' , iou_acc_s_vg,'hit_s', hit_acc_s_vg,'att_s', att_acc_s_vg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
