{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " '''\n",
    " OUR MODEL\n",
    "Input: VQA2, Train: mscoco2014 train, Validation mscoco2014 val ,Test mscoco2015 test\n",
    "https://visualqa.org/download.html\n",
    "Trainval images: 123287(82,783 + 40,504)Trianval questions: 658111\n",
    "Test images: 81434 Test questions:  447793 Test-dev images: 36807\n",
    "Unique answers is 3133\n",
    "\n",
    "Paper: 204,721 images from Microsoft COCO dataset,(443,757 train, 214,354 val, and 447,793 test questions)\n",
    "DIFFERENCE :n_ans = 3133, only answers which appear more than 8 times in val+train dataset\n",
    "Github : https://github.com/cvlab-tohoku/Dense-CoAttention-Network/blob/master/dense_coattn/cost/costs.py\n",
    " '''\n",
    "    \n",
    "'''\n",
    "this model use PNASNet for visual feature selection and use dense layer to select the \n",
    "'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0513 00:16:16.581567 139913678259584 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import done\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow_hub as hub\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "config = tf.ConfigProto(gpu_options=gpu_options,log_device_placement=True, allow_soft_placement=True)\n",
    "import lmdb\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "#from bert_embedding import BertEmbedding\n",
    "\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"/home/david_hc95/tfhub_modules\"\n",
    "# Jupyter caches the utils module, so changes there aren't reflected in a re-import. We delete the utils module from\n",
    "# sys.modules first so that we force-reload all the code in utils.py\n",
    "\n",
    "sys.path.append('../utilities/')\n",
    "if 'utils_data_gen' in sys.modules:\n",
    "    del sys.modules['utils_data_gen']\n",
    "from utils_data_gen import *\n",
    "\n",
    "if 'utils_evaluation' in sys.modules:\n",
    "    del sys.modules['utils_evaluation']\n",
    "from utils_evaluation import *\n",
    "\n",
    "if 'utils_model' in sys.modules:\n",
    "    del sys.modules['utils_model']\n",
    "from utils_model import *\n",
    "\n",
    "print('Import done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cloud ==1:\n",
    "    os.environ[\"TFHUB_CACHE_DIR\"] = \"/home/sonamgoenka/tfhub_modules\"\n",
    "    slim_models_path = '/home/david_hc95/grounding/iccv19_grounding/models/'\n",
    "    sys.path.append(slim_models_path)\n",
    "    data_path = '/home/ha2436/data/'\n",
    "else:\n",
    "    os.environ[\"TFHUB_CACHE_DIR\"] = \"/home/sonam/tfhub_modules\"\n",
    "    slim_models_path = '/home/sonam/models/'\n",
    "    sys.path.append(slim_models_path)\n",
    "    data_path = '/dvmm-filer2/datasets/Groundings/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/ha2436/data/'\n",
    "data_path = base_path + 'mscoco/'\n",
    "lmdb_path = data_path + 'MSCOCO_jpg.lmdb'\n",
    "lmdb_path_new = '/home/david_hc95/mscoco_test2015' \n",
    "vqa_base_path = '/home/david_hc95/VQA_pickles_new/'\n",
    "\n",
    "dict_paths = {'train':'trainval.pickle', 'val':'val.pickle', 'test': 'test.pickle'} \n",
    "ans_paths = 'top_answer_list.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training image size 123287\n",
      "testing image size 81434\n"
     ]
    }
   ],
   "source": [
    "#loading MSCOCO img data \n",
    "lmdb_env = lmdb.open(lmdb_path, map_size=int(1e11), readonly=True, lock=False)\n",
    "txn = lmdb_env.begin(write=False)\n",
    "lmdb_env_test = lmdb.open(lmdb_path_new, map_size=int(1e11), readonly=True, lock=False)\n",
    "txn_test = lmdb_env_test.begin(write=False)\n",
    "\n",
    "with open(vqa_base_path + dict_paths['train'], 'rb') as f:\n",
    "    dict_train = pickle.load(f, encoding='latin1')\n",
    "    ids_train = list(dict_train.keys())\n",
    "\n",
    "with open(vqa_base_path + \"idx2ans.pickle\", 'rb') as file:\n",
    "    idx2ans = pickle.load(file)\n",
    "    \n",
    "with open(vqa_base_path + dict_paths['test'], 'rb') as f:\n",
    "    dict_test = pickle.load(f, encoding='latin1')\n",
    "    ids_test = list(dict_test.keys())\n",
    "\n",
    "print('training image size', len(ids_train))\n",
    "print('testing image size', len(ids_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing images in memory\n",
    "def preprocess_images_vqa(all_ids_list, db, db2):\n",
    "    valid_ids = []\n",
    "    images_dict = {}\n",
    "    for i, id_train in enumerate(all_ids_list):\n",
    "        imgbin = db.get(id_train.encode('utf-8'))\n",
    "        if imgbin == None:\n",
    "            imgbin = db2.get(id_train.encode('utf-8'))\n",
    "            if imgbin == None:\n",
    "                raise Exception(\"image not found! %s\" % id_train)\n",
    "        buff = np.frombuffer(imgbin, dtype='uint8')        \n",
    "        imgbgr = cv2.imdecode(buff, cv2.IMREAD_COLOR)\n",
    "        img = imgbgr[:,:,[2,1,0]]\n",
    "        im = cv2.resize(img,(299,299))  \n",
    "        valid_ids.append(id_train)\n",
    "        images_dict[id_train] = im\n",
    "        if i % 10000 == 0:\n",
    "            print('Processed: ', i, ' images')\n",
    "    print('Done pre-processing images')            \n",
    "    return valid_ids, images_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = 32\n",
    "gamma_1 = 5.0\n",
    "gamma_2 = 10.0\n",
    "n_iter_per_epoch = int(len(dict_train)/n_batch)\n",
    "# n_iter_per_epoch_val = 2  #int(5000/n_batch)\n",
    "n_epochs = 15 #N of epochs\n",
    "reg_val = .0005\n",
    "n_ans = 3133 # number of answers\n",
    "\n",
    "# visual params\n",
    "lr_value = .001\n",
    "log_file = './logs/vgg/vgg'\n",
    "\n",
    "# Architecture\n",
    "#Temp :'Phrase_Grounding_NewAttn_VG_Seg_COCO_Pyramid_VGG'\n",
    "model_name =  './my_model/Model_' + 'VQA_LSTM_LearnableFuse_VQA2_VGG'\n",
    "conv_kernel_size = 3 #1\n",
    "n_layers = 1 #3\n",
    "vis_method = depth_selection_vgg_pyramid # Pyramid\n",
    "# segment_branch = 0\n",
    "# msk_size = 37 #38 for pnasnet, 37 for vgg\n",
    "visual_model = 'vgg' #pnasnet, vgg\n",
    "text_model = \"elmo\" # bert\n",
    "use_new_attn = False\n",
    "use_pyramid = True\n",
    "# latent_space_dim = 512\n",
    "\n",
    "# MAX_SEQ_LENGTH = 50\n",
    "# BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method feeds four attendance features to dense layer to generate a new attendance feature \n",
    "def fusion_learn(v, e_s):\n",
    "    '''\n",
    "    this function use cos similarity for attetion\n",
    "    pass attended visual fetaure and sen emebedding to dense layer for level selection\n",
    "    dense for feature fusion\n",
    "    \n",
    "    '''\n",
    "    with tf.variable_scope('text_image_fusion'):\n",
    "            ###sentence-level###\n",
    "            #heatmap pool\n",
    "            h_s = tf.nn.relu(tf.einsum('bj,blkj->blk',e_s,v)) #pair-wise e_bar*v^T: ?xNx4\n",
    "            #attention\n",
    "            a_s = tf.einsum('bjk,bjki->bik',h_s,v) #?xDx4 attnded visual reps for sen.\n",
    "            #pair-wise score\n",
    "            a_s_norm = tf.nn.l2_normalize(a_s,axis=1) # ? *D*4\n",
    "            e_s_norm = tf.nn.l2_normalize(e_s,axis=1) # ?*D\n",
    "            \n",
    "            # take four attended visual feature and learn one representation\n",
    "            visual = tf.contrib.layers.flatten(a_s_norm)   #?*4D\n",
    "            visual = tf.layers.dense(visual, units=1024)\n",
    "            visual = tf.nn.leaky_relu(visual,alpha=.25)\n",
    "            visual = tf.layers.dense(visual, units=e_s_norm.get_shape().as_list()[1])\n",
    "            \n",
    "            fusion = tf.concat([e_s_norm, visual], axis=1) # ?*2D\n",
    "            \n",
    "            fusion = tf.layers.dense(fusion, units=1024)\n",
    "            fusion = tf.nn.leaky_relu(fusion,alpha=.25)\n",
    "            fusion = tf.layers.dense(fusion, units=1024)\n",
    "            fusion = tf.nn.leaky_relu(fusion,alpha=.25)\n",
    "            fusion = tf.layers.dense(fusion, units=1024)\n",
    "            fusion = tf.nn.leaky_relu(fusion,alpha=.25)\n",
    "            fusion = tf.layers.dense(fusion, units=1024)\n",
    "            fusion = tf.nn.leaky_relu(fusion,alpha=.25)\n",
    "            fusion = tf.layers.dense(fusion, units=n_ans)\n",
    "            fusion = tf.identity(fusion, name='fusion')\n",
    "            \n",
    "            return fusion\n",
    "# this method feeds four attendance features to dense layer to generate a new attendance feature \n",
    "def fusion_learn_pyramid(v, e_s):\n",
    "    '''\n",
    "    this function use cos similarity for attetion\n",
    "    no level selection since we use pyramid\n",
    "    dense for feature fusion\n",
    "    \n",
    "    '''\n",
    "    with tf.variable_scope('text_image_fusion'):\n",
    "            ###sentence-level###\n",
    "            #heatmap pool\n",
    "            h_s = tf.nn.relu(tf.einsum('bj,blkj->blk',e_s,v)) #pair-wise e_bar*v^T: ?xNx1\n",
    "            #attention\n",
    "            a_s = tf.einsum('bjk,bjki->bik',h_s,v) #?xDx1 attnded visual reps for sen.\n",
    "            #pair-wise score\n",
    "            a_s_norm = tf.nn.l2_normalize(a_s,axis=1) # ?xDx1\n",
    "            e_s_norm = tf.nn.l2_normalize(e_s,axis=1) # ?xD\n",
    "            \n",
    "            # take four attended visual feature and learn one representation\n",
    "            visual = tf.squeeze(a_s_norm, axis=-1) #?xD\n",
    "            \n",
    "            fusion = tf.concat([e_s_norm, visual], axis=-1) # ?x2D\n",
    "            \n",
    "            fusion = tf.layers.dense(fusion, units=1024)\n",
    "            fusion = tf.nn.leaky_relu(fusion,alpha=.25)\n",
    "            fusion = tf.layers.dense(fusion, units=1024)\n",
    "            fusion = tf.nn.leaky_relu(fusion,alpha=.25)\n",
    "            fusion = tf.layers.dense(fusion, units=1024)\n",
    "            fusion = tf.nn.leaky_relu(fusion,alpha=.25)\n",
    "            fusion = tf.layers.dense(fusion, units=1024)\n",
    "            fusion = tf.nn.leaky_relu(fusion,alpha=.25)\n",
    "            fusion = tf.layers.dense(fusion, units=n_ans)\n",
    "            fusion = tf.identity(fusion, name='fusion')\n",
    "            \n",
    "            return fusion\n",
    "        \n",
    "        \n",
    "def new_attn_fusion_learn(v, e_s):\n",
    "    '''\n",
    "    this function implement the attention and fusion of paper Tips and Tricks for Visual Question Answering:\n",
    "    Learnings from the 2017 Challenge\n",
    "    \n",
    "    input:\n",
    "        v: four level image feature ?xNx4x2048\n",
    "        e_s: sentence feature ?x512\n",
    "    return:\n",
    "        attn: attendance feature of images #?x512\n",
    "    '''\n",
    "    # use dense layer to get one level of image feature\n",
    "    def new_fusion(attn, e_s):\n",
    "        '''\n",
    "        this function fuse the attendance feature and sentence feature\n",
    "\n",
    "        input:\n",
    "            attn: attendance feature ?x512\n",
    "            e_s: sentence feature ?xD\n",
    "\n",
    "        return:\n",
    "            fusion: final fusion ?x3133\n",
    "        '''\n",
    "        e_s = non_linear(e_s, 512)\n",
    "        fusion = tf.math.multiply(attn, e_s)\n",
    "        fusion = non_linear(fusion, 3133)\n",
    "        fusion = tf.layers.dense(fusion, units=3133)\n",
    "        return fusion\n",
    "    \n",
    "    \n",
    "    def non_linear(in_tensor, num_units):\n",
    "        '''\n",
    "        This function implement the non-linear function in the paper \n",
    "        '''\n",
    "        y_hat = tf.layers.dense(in_tensor, units=num_units)\n",
    "        y_hat = tf.nn.tanh(y_hat)\n",
    "        g = tf.layers.dense(in_tensor, units=num_units)\n",
    "        g = tf.nn.sigmoid(g)\n",
    "        y = tf.math.multiply(y_hat, g)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    # combine four level image features using dense layers\n",
    "    v_shape = v.get_shape().as_list() \n",
    "    v = tf.reshape(v, [-1, v_shape[1], v_shape[2] * v_shape[3]])\n",
    "    v = tf.layers.dense(v, units=2048)\n",
    "    v = tf.nn.l2_normalize(v, axis=-1) # ?xNx2048\n",
    "    \n",
    "    \n",
    "    # expand e_s to have the same compatible shape\n",
    "    e_s_attn = tf.tile(tf.expand_dims(e_s, 1), [1,v.get_shape().as_list()[1], 1]) # ?xNxD check if this function is correct\n",
    "    concat_sen_image = tf.concat([v, e_s_attn], axis=-1)\n",
    "    \n",
    "    attn = non_linear(concat_sen_image, 512) #?xNx512\n",
    "    attn = tf.layers.dense(attn, units=1) #?xNx1\n",
    "    attn = tf.squeeze(attn, -1) #?xN\n",
    "    attn = tf.nn.softmax(attn, axis=-1) # ?xN   \n",
    "    \n",
    "    attn = tf.einsum('bj,bjk->bk',attn, v) # ?x512\n",
    "    attn = non_linear(attn, 512) # ?x512\n",
    "    \n",
    "    fusion = new_fusion(attn, e_s)\n",
    "    fusion = tf.identity(fusion, name='fusion')\n",
    "    \n",
    "    return fusion\n",
    "\n",
    "    \n",
    "def build_bilstm(w_embd,seq_length):\n",
    "    with tf.variable_scope('BiLSTM'):\n",
    "        # Forward direction cell\n",
    "        lstm_fw_cell = tf.contrib.rnn.LSTMCell(512, forget_bias=1.0)\n",
    "        # Backward direction cell\n",
    "        lstm_bw_cell = tf.contrib.rnn.LSTMCell(512, forget_bias=1.0)\n",
    "        outputs, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, w_embd, sequence_length = seq_length,\n",
    "                                                  dtype=tf.float32)\n",
    "    output = tf.concat(outputs,axis=2,name='BiLSTM_out')\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Graph (Base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising for first time\n",
      "Building Text Model...\n",
      "use elmo for textual model\n",
      "WARNING:tensorflow:From /home/david_hc95/anaconda3/envs/grounding/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0512 13:41:21.421391 139673951896960 deprecation.py:323] From /home/david_hc95/anaconda3/envs/grounding/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0512 13:41:22.081396 139673951896960 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-e891fe58444d>:146: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0512 13:41:22.144441 139673951896960 deprecation.py:323] From <ipython-input-6-e891fe58444d>:146: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-e891fe58444d>:150: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0512 13:41:22.146371 139673951896960 deprecation.py:323] From <ipython-input-6-e891fe58444d>:150: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/david_hc95/anaconda3/envs/grounding/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0512 13:41:22.148182 139673951896960 deprecation.py:323] From /home/david_hc95/anaconda3/envs/grounding/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/david_hc95/anaconda3/envs/grounding/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0512 13:41:22.157697 139673951896960 deprecation.py:323] From /home/david_hc95/anaconda3/envs/grounding/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-147e699d0ac6>:32: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0512 13:41:22.450596 139673951896960 deprecation.py:323] From <ipython-input-7-147e699d0ac6>:32: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Visual Model...\n",
      "WARNING:tensorflow:From ../utilities/utils_model.py:197: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0512 13:41:22.805313 139673951896960 deprecation.py:323] From ../utilities/utils_model.py:197: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cosine simliarity attn mechanism\n",
      "use pyramid\n",
      "Model is built\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "try:\n",
    "    sess.close()\n",
    "except:\n",
    "    print('Initialising for first time')\n",
    "    \n",
    "sess = tf.InteractiveSession(config=config)\n",
    "mode = tf.placeholder(tf.string, name='mode')\n",
    "isTraining = tf.equal(mode, 'train')\n",
    "regularizer = tf.contrib.layers.l2_regularizer(reg_val)\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    print('Building Text Model...')        \n",
    "    if text_model == \"elmo\":\n",
    "        print(\"use elmo for textual model\")\n",
    "        #sentence placeholder - list of sentences\n",
    "        text_batch = tf.placeholder('string', shape=[None], name='text_input')\n",
    "        #loading pre-trained ELMo\n",
    "        elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "        #getting ELMo embeddings\n",
    "        elmo_embds = elmo(text_batch, signature=\"default\", as_dict=True)\n",
    "        w_embd = tf.identity(elmo_embds['word_emb'], name='elmo_word_embd') #?xTxD/2\n",
    "        lstm_embd = build_bilstm(w_embd,elmo_embds['sequence_len']) #?xTxD\n",
    "        #taking index of last word in each sentence\n",
    "        idx = elmo_embds['sequence_len']-1\n",
    "        batch_idx = tf.stack([tf.range(0,tf.size(idx),1),idx],axis=1)\n",
    "        # Concatenate first of backward with last of forward to get sentence embeddings\n",
    "        dim = lstm_embd.get_shape().as_list()[-1]\n",
    "        sen_embd = tf.concat([lstm_embd[:,0,int(dim/2):],\n",
    "        tf.gather_nd(lstm_embd[:,:,:int(dim/2)],batch_idx)], axis=-1) #[batch,dim]\n",
    "        e_s = tf.layers.dense(sen_embd, units=1024)\n",
    "        e_s = tf.nn.leaky_relu(e_s,alpha=.25)\n",
    "        e_s = tf.layers.dense(e_s, units=1024)\n",
    "        e_s = tf.nn.leaky_relu(e_s,alpha=.25)\n",
    "        e_s = tf.nn.l2_normalize(e_s, axis=-1, name='sen_embedding')\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"please type a valid text model\")\n",
    "    \n",
    "    #Building visual model\n",
    "    print('Building Visual Model...')\n",
    "    input_img = tf.placeholder(tf.float32, (None,299,299,3), name='input_img')\n",
    "    if visual_model == 'vgg':\n",
    "        pre_processed_img = pre_process(input_img, 'vgg_preprocessing')\n",
    "        vis_model = pre_trained_load(model_name='vgg_16', image_shape=(None,299,299,3),\n",
    "                                  input_tensor=pre_processed_img, session=sess, is_training=False, global_pool=False)\n",
    "    if visual_model == 'pnasnet':\n",
    "        pre_processed_img = pre_process(input_img, 'inception_preprocessing')\n",
    "        vis_model = pre_trained_load(model_name='pnasnet_large', image_shape=(None,299,299,3),\n",
    "                                     input_tensor=pre_processed_img, session=sess, is_training=False, global_pool=True)\n",
    "    v = vis_method(vis_model,regularizer, conv_kernel_size, n_layers) #(?,1225,4,1024)\n",
    "    \n",
    "    if not use_new_attn:\n",
    "        print(\"use cosine simliarity attn mechanism\")\n",
    "        if use_pyramid:\n",
    "            print(\"use pyramid\")\n",
    "            fusion = fusion_learn_pyramid(v, e_s)\n",
    "        else:\n",
    "            print(\"use level selection\")\n",
    "            fusion = fusion_learn(v, e_s)\n",
    "    else:\n",
    "        print(\"use new attn mechanism\")\n",
    "        fusion = new_attn_fusion_learn(v, e_s)\n",
    "    \n",
    "    answer_batch = tf.placeholder(tf.float32, shape=[n_batch, n_ans], name='answer_input')\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=fusion, labels=answer_batch)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    loss = tf.identity(loss, name='loss')\n",
    "\n",
    "    tf.summary.scalar('loss_value', loss)\n",
    "\n",
    "    lr = tf.placeholder(tf.float32, shape=[], name='learning_rate')\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "    opt = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    train_vars = list(set(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)) - set(vis_model.model_weights))\n",
    "    train_op = opt.minimize(loss, var_list=train_vars, name='train_op') \n",
    "\n",
    "    global_saver = tf.train.Saver()\n",
    "    train_writer = tf.summary.FileWriter(log_file, sess.graph)\n",
    "    print('Model is built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'stack_v/v2/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
      "<tf.Variable 'stack_v/v3/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
      "<tf.Variable 'text_image_fusion/dense_1/kernel:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'dense_1/kernel:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'text_image_fusion/dense_1/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'dense_1/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'stack_v/v4/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
      "<tf.Variable 'BiLSTM/bidirectional_rnn/bw/lstm_cell/bias:0' shape=(2048,) dtype=float32_ref>\n",
      "<tf.Variable 'BiLSTM/bidirectional_rnn/bw/lstm_cell/kernel:0' shape=(1024, 2048) dtype=float32_ref>\n",
      "<tf.Variable 'module/aggregation/weights:0' shape=(3,) dtype=float32>\n",
      "<tf.Variable 'text_image_fusion/dense_2/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'module/aggregation/scaling:0' shape=() dtype=float32>\n",
      "<tf.Variable 'dense/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'stack_v/v2/bias:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'stack_v/v3/bias:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'dense/kernel:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'text_image_fusion/dense/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'text_image_fusion/dense/kernel:0' shape=(2048, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'text_image_fusion/dense_2/kernel:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'stack_v/v_all_postConv/v_all_stage_0/conv2d/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'stack_v/v_all_postConv/v_all_stage_0/conv2d/kernel:0' shape=(3, 3, 2048, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'stack_v/v1/bias:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'stack_v/v1/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
      "<tf.Variable 'text_image_fusion/dense_3/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'BiLSTM/bidirectional_rnn/fw/lstm_cell/bias:0' shape=(2048,) dtype=float32_ref>\n",
      "<tf.Variable 'BiLSTM/bidirectional_rnn/fw/lstm_cell/kernel:0' shape=(1024, 2048) dtype=float32_ref>\n",
      "<tf.Variable 'text_image_fusion/dense_4/bias:0' shape=(3133,) dtype=float32_ref>\n",
      "<tf.Variable 'stack_v/v4/bias:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'text_image_fusion/dense_4/kernel:0' shape=(1024, 3133) dtype=float32_ref>\n",
      "<tf.Variable 'text_image_fusion/dense_3/kernel:0' shape=(1024, 1024) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "for x in train_vars:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen_vqa(valid_ids, annot_dict, train_images,n_batch):\n",
    "    img_batch = np.empty((n_batch, 299, 299, 3), dtype='float32')\n",
    "    cap_batch = []\n",
    "    ans_batch = []\n",
    "    chosen_ids = random.sample(valid_ids, n_batch)\n",
    "    for i, chosen_id in enumerate(chosen_ids):\n",
    "        imgbin = train_images[chosen_id]\n",
    "        img_batch[i,:,:,:] = imgbin\n",
    "        questions = []\n",
    "        answers = []\n",
    "        for annot in annot_dict[chosen_id]:\n",
    "            q_id = list(annot.keys())[0] \n",
    "            questions.append(annot[q_id]['question'])\n",
    "            label = np.zeros(3133, dtype='float32')\n",
    "            for answer in annot[q_id]['answer_id']:\n",
    "                label[answer[0]] = answer[1]\n",
    "            answers.append(label)\n",
    "        n_ques = len(questions)\n",
    "        idx = random.choice(range(n_ques))  \n",
    "        cap_batch.append(questions[idx])\n",
    "        ans_batch.append(answers[idx])\n",
    "        \n",
    "    return img_batch, cap_batch, ans_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  0  images\n",
      "Processed:  10000  images\n",
      "Processed:  20000  images\n",
      "Processed:  30000  images\n",
      "Processed:  40000  images\n",
      "Processed:  50000  images\n",
      "Processed:  60000  images\n",
      "Processed:  70000  images\n",
      "Processed:  80000  images\n",
      "Processed:  90000  images\n",
      "Processed:  100000  images\n",
      "Processed:  110000  images\n",
      "Processed:  120000  images\n",
      "Done pre-processing images\n",
      "Done pre-processing training images\n"
     ]
    }
   ],
   "source": [
    "valid_ids_train, train_images = preprocess_images_vqa(ids_train, txn, txn_test)    \n",
    "print('Done pre-processing training images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Training Images\n",
    "# valid_ids_train, train_images = preprocess_images(ids_train[:100], txn, txn_test)\n",
    "# print('Done pre-processing training images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading visual path model...\n",
      "WARNING:tensorflow:From /home/david_hc95/anaconda3/envs/grounding/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0512 14:01:48.980753 139673951896960 deprecation.py:323] From /home/david_hc95/anaconda3/envs/grounding/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/david_hc95/grounding/iccv19_grounding/models/vgg_16.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0512 14:01:48.985939 139673951896960 saver.py:1270] Restoring parameters from /home/david_hc95/grounding/iccv19_grounding/models/vgg_16.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "# print('Initializing...')\n",
    "_ = sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#loading pretrained inception weights\n",
    "print('Loading visual path model...')\n",
    "vis_model.load_weights()\n",
    "print('Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "\n",
      "=====Epoch: 0\n",
      "===Train\n",
      "saving model2/123264, train_loss:0.0043 \n",
      "model saved\n",
      "\n",
      "\n",
      "=====Epoch: 1\n",
      "===Train\n",
      "Sample 123232/123264, train_loss:0.0020 \n",
      "\n",
      "=====Epoch: 2\n",
      "===Train\n",
      "Sample 123232/123264, train_loss:0.0018 \n",
      "\n",
      "=====Epoch: 3\n",
      "===Train\n",
      "Sample 123232/123264, train_loss:0.0016 \n",
      "\n",
      "=====Epoch: 4\n",
      "===Train\n",
      "Sample 101792/123264, train_loss:0.0015 \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model2/123264, train_loss:0.0015 \n",
      "model saved\n",
      "\n",
      "\n",
      "=====Epoch: 6\n",
      "===Train\n",
      "Sample 123232/123264, train_loss:0.0014 \n",
      "\n",
      "=====Epoch: 7\n",
      "===Train\n",
      "Sample 123232/123264, train_loss:0.0014 \n",
      "\n",
      "=====Epoch: 8\n",
      "===Train\n",
      "Sample 123232/123264, train_loss:0.0014 \n",
      "\n",
      "=====Epoch: 9\n",
      "===Train\n",
      "Sample 123232/123264, train_loss:0.0013 \n",
      "\n",
      "=====Epoch: 10\n",
      "===Train\n",
      "saving model2/123264, train_loss:0.0012 \n",
      "model saved\n",
      "\n",
      "\n",
      "=====Epoch: 11\n",
      "===Train\n",
      "Sample 123232/123264, train_loss:0.0012 \n",
      "\n",
      "=====Epoch: 12\n",
      "===Train\n",
      "Sample 123232/123264, train_loss:0.0012 \n",
      "\n",
      "=====Epoch: 13\n",
      "===Train\n",
      "Sample 123232/123264, train_loss:0.0012 \n",
      "\n",
      "=====Epoch: 14\n",
      "===Train\n",
      "Training done.123264, train_loss:0.0012 \n",
      "Saving model...\n",
      "Saving done.\n"
     ]
    }
   ],
   "source": [
    "#loop on training data\n",
    "print('Start training...')\n",
    "max_class_acc = 0\n",
    "val_acc = np.zeros((n_epochs,))\n",
    "train_loss = np.zeros((n_epochs,))\n",
    "#bert_embedding = BertEmbedding()\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    print('\\n\\n=====Epoch: %d'%e)\n",
    "    avg_loss = 0\n",
    "    \n",
    "    if e < 9:\n",
    "        lr_value = lr_value\n",
    "    elif 9 <= e < 14:\n",
    "        lr_value = lr_value / 2.0\n",
    "    elif e >= 14:\n",
    "        lr_value = lr_value/ 4.0\n",
    "        \n",
    "    print('===Train')\n",
    "    for i in range(n_iter_per_epoch):   \n",
    "        \n",
    "        if text_model == 'elmo':\n",
    "            img_batch, cap_batch, ans_batch = batch_gen_vqa(valid_ids_train, dict_train, train_images,n_batch)\n",
    "            feed_dict = {input_img: img_batch, text_batch: cap_batch, answer_batch: ans_batch, mode: 'train', lr: lr_value}\n",
    "        \n",
    "        loss_val, _ = sess.run([loss, train_op], feed_dict)\n",
    "        avg_loss += loss_val\n",
    "        var = [i * n_batch, n_iter_per_epoch * n_batch, avg_loss / float(i + 1)]\n",
    "        prnt = 'Sample {}/{}, train_loss:{:.4f} \\r'.format(var[0], var[1], var[2])\n",
    "        sys.stdout.write(prnt)\n",
    "        sys.stdout.flush()\n",
    "        train_loss[e] = avg_loss/float(n_iter_per_epoch+1)\n",
    "        \n",
    "    if e % 5 == 0:\n",
    "        print('saving model')\n",
    "        global_saver.save(sess, model_name + str(e))\n",
    "        print('model saved')\n",
    "          \n",
    "print('Training done.')\n",
    "#saving the session\n",
    "print('Saving model...')\n",
    "global_saver.save(sess,  model_name + str(n_epochs))\n",
    "print('Saving done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_loss, label='Train loss '+ model_name)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(100. * val_acc, label='Validation classification_acc '+ model_name)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definining session for the first time\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sess.close()\n",
    "except NameError:\n",
    "    print(\"Definining session for the first time\")\n",
    "#tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading grounding pretrained model...\n",
      "./my_model/Model_VQA_LSTM_LearnableFuse_VQA2_VGG15\n",
      "WARNING:tensorflow:From /home/david_hc95/anaconda3/envs/grounding/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0513 00:17:49.362160 139913678259584 deprecation.py:323] From /home/david_hc95/anaconda3/envs/grounding/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model/Model_VQA_LSTM_LearnableFuse_VQA2_VGG15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0513 00:17:49.367622 139913678259584 saver.py:1270] Restoring parameters from ./my_model/Model_VQA_LSTM_LearnableFuse_VQA2_VGG15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "condition = 'Model_VQA_LSTM_LearnableFuse_VQA2_VGG15'\n",
    "print('Loading grounding pretrained model...')\n",
    "model_path = './my_model/'+ condition\n",
    "print(model_path)\n",
    "sess, graph = load_model(model_path,config)\n",
    "print('Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = sess.graph.get_tensor_by_name(\"input_img:0\")\n",
    "text_batch = sess.graph.get_tensor_by_name(\"text_input:0\")\n",
    "mode = sess.graph.get_tensor_by_name(\"mode:0\")\n",
    "fusion = sess.graph.get_tensor_by_name(\"text_image_fusion/fusion:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  0  images\n",
      "Processed:  10000  images\n",
      "Processed:  20000  images\n",
      "Processed:  30000  images\n",
      "Processed:  40000  images\n",
      "Processed:  50000  images\n",
      "Processed:  60000  images\n",
      "Processed:  70000  images\n",
      "Processed:  80000  images\n",
      "Done pre-processing images\n",
      "Done pre-processing test images\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing Testing Images\n",
    "valid_ids_test, test_images = preprocess_images_vqa(ids_test, txn_test, txn)\n",
    "print('Done pre-processing test images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question:\n",
    "    def __init__(self,question_id,image_id,question,answer_id=None):\n",
    "        self.question_id = question_id\n",
    "        self.image_id = image_id\n",
    "        self.question = question\n",
    "        self.answer_id = answer_id\n",
    "        \n",
    "def generate_batches(questions_list, batch_size):\n",
    "    n = len(questions_list)\n",
    "    \n",
    "    questions_batch = []\n",
    "    for i, ques in enumerate(questions_list):\n",
    "        questions_batch.append(ques)\n",
    "        if ((i + 1) % batch_size == 0) or ((i + 1) == n):\n",
    "            yield questions_batch\n",
    "            questions_batch = []\n",
    "\n",
    "def validate_test(final_test_list, test_images, idx2ans, batch_size):\n",
    "    results = list()\n",
    "    print(\"----started\")\n",
    "\n",
    "    l = 0\n",
    "    for question_batch in generate_batches(final_test_list, batch_size):\n",
    "        num_questions = len(question_batch)\n",
    "        img_batch = np.empty((num_questions, 299, 299, 3))\n",
    "        ques_batch = []\n",
    "        ques_ids_batch = []\n",
    "        \n",
    "        for j, ques in enumerate(question_batch):\n",
    "            image_id = ques.image_id\n",
    "            question_id = ques.question_id\n",
    "            question = ques.question\n",
    "\n",
    "            img = np.reshape(test_images[image_id], (299, 299, 3))\n",
    "            img_batch[j,:] = img\n",
    "            ques_batch.append(question)\n",
    "            ques_ids_batch.append(question_id)\n",
    "        \n",
    "        feed_dict = {input_img: img_batch, text_batch: ques_batch, mode:\"test\"}\n",
    "        pred = sess.run(fusion, feed_dict)\n",
    "        idx = np.argmax(pred, axis=1)\n",
    "        for k, each_idx in enumerate(idx):\n",
    "            results.append({\"question_id\": ques_ids_batch[k], \"answer\": idx2ans[each_idx]}) \n",
    "        l += 1\n",
    "        if l % 1000 == 0:\n",
    "            print(\"Number of batches done: \", l, \"; num_results: \", str(len(results)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----started\n",
      "Number of batches done:  100 ; num_results:  6400\n"
     ]
    }
   ],
   "source": [
    "#Generating All Questions in Answer Batch\n",
    "final_test_list = []\n",
    "for k, doc_id in enumerate(dict_test):\n",
    "    image_id = doc_id\n",
    "    for annot in dict_test[doc_id]:\n",
    "        question_id = list(annot.keys())[0]\n",
    "        question = annot[question_id]['question']\n",
    "        ques = Question(question_id ,image_id, question)\n",
    "        final_test_list.append(ques)\n",
    "\n",
    "results = validate_test(final_test_list, test_images, idx2ans, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving as json file to fetch to server\n",
    "import json \n",
    "with open('model_'+ condition + '20_results.json', 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
