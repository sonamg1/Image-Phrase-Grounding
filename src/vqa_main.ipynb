{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " '''\n",
    " OUR MODEL\n",
    "Input: VQA2, Train: mscoco2014 train, Validation mscoco2014 val ,Test mscoco2015 test\n",
    "https://visualqa.org/download.html\n",
    "Trainval images: 123287(82,783 + 40,504)Trianval questions: 658111\n",
    "Test images: 81434 Test questions:  447793 Test-dev images: 36807\n",
    "Unique answers is 3133\n",
    "\n",
    "Paper: 204,721 images from Microsoft COCO dataset,(443,757 train, 214,354 val, and 447,793 test questions)\n",
    "DIFFERENCE :n_ans = 3133, only answers which appear more than 8 times in val+train dataset\n",
    "Github : https://github.com/cvlab-tohoku/Dense-CoAttention-Network/blob/master/dense_coattn/cost/costs.py\n",
    " '''\n",
    "    \n",
    "'''\n",
    "this model use PNASNet for visual feature selection and use dense layer to select the \n",
    "'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonam/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow_hub as hub\n",
    "from utils import *\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "config = tf.ConfigProto(gpu_options=gpu_options,log_device_placement=True, allow_soft_placement=True)\n",
    "import lmdb\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "#from bert_embedding import BertEmbedding\n",
    "\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"/home/sonam/tfhub_modules\"\n",
    "# Jupyter caches the utils module, so changes there aren't reflected in a re-import. We delete the utils module from\n",
    "# sys.modules first so that we force-reload all the code in utils.py\n",
    "\n",
    "if 'utils' in sys.modules:\n",
    "    del sys.modules['utils']\n",
    "from utils import *\n",
    "print('import done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(tf.contrib.eager.num_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in /home/sonam/anaconda3/lib/python3.6/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /home/sonam/anaconda3/lib/python3.6/site-packages (from bert-tensorflow) (1.10.0)\n",
      "\u001b[33mYou are using pip version 19.0.2, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = '/dvmm-filer2/datasets/Groundings/data/'\n",
    "data_path = base_path + 'mscoco/'\n",
    "lmdb_path = data_path + 'MSCOCO_jpg.lmdb'\n",
    "lmdb_path_new = '/home/sonam/mscoco_test2015' \n",
    "\n",
    "dict_paths = {'train':'trainval.pickle', 'val':'val.pickle', 'test': 'test.pickle'} \n",
    "ans_paths = 'top_answer_list.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(dict_paths['train'], 'rb') as f:\n",
    "    dict_train = pickle.load(f, encoding='latin1')\n",
    "    ids_train = list(dict_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainval.pickle'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_paths['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training image size 123287\n",
      "testing image size 81434\n"
     ]
    }
   ],
   "source": [
    "#loading MSCOCO img data \n",
    "lmdb_env = lmdb.open(lmdb_path, map_size=int(1e11), readonly=True, lock=False)\n",
    "txn = lmdb_env.begin(write=False)\n",
    "lmdb_env_test = lmdb.open(lmdb_path_new, map_size=int(1e11), readonly=True, lock=False)\n",
    "txn_test = lmdb_env_test.begin(write=False)\n",
    "with open(dict_paths['train'], 'rb') as f:\n",
    "    dict_train = pickle.load(f, encoding='latin1')\n",
    "    ids_train = list(dict_train.keys())\n",
    "    \n",
    "# with open(ans_paths,'rb') as f:\n",
    "#     unique_ans = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "# with open(dict_paths['val'], 'rb') as f:\n",
    "#     dict_val = pickle.load(f, encoding='latin1')\n",
    "#     ids_val = list(dict_val.keys())\n",
    "\n",
    "with open(\"./idx2ans.pickle\", 'rb') as file:\n",
    "    idx2ans = pickle.load(file)\n",
    "    \n",
    "with open(dict_paths['test'], 'rb') as f:\n",
    "    dict_test = pickle.load(f, encoding='latin1')\n",
    "    ids_test = list(dict_test.keys())\n",
    "\n",
    "print('training image size', len(ids_train))\n",
    "#print('validation image size', len(ids_val))\n",
    "print('testing image size', len(ids_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(len(idx2ans))\n",
    "with open(dict_paths['test'], 'rb') as f:\n",
    "    dict_test = pickle.load(f, encoding='latin1')\n",
    "    ids_test = list(dict_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{393227000: {'question': 'does the guy have a tattoo ?', 'answer': [(['yes'], 1)], 'answer_id': [(3, 1)]}}, {393227001: {'question': 'what is this man riding on ?', 'answer': [(['skateboard'], 1)], 'answer_id': [(11, 1)]}}, {393227002: {'question': \"how many tattoos can be seen on this man 's body ?\", 'answer': [(['1'], 1), (['3'], 0.3), (['2'], 0.3)], 'answer_id': [(12, 1), (32, 0.3), (17, 0.3)]}}, {393227003: {'question': 'what color is his hat ?', 'answer': [(['backwards'], 0.3), (['blue'], 1)], 'answer_id': [(2497, 0.3), (13, 1)]}}, {393227004: {'question': \"is the man 's visor providing his face enough protection ?\", 'answer': [(['no'], 1)], 'answer_id': [(9, 1)]}}]\n",
      "[393227000] 0 {'question': 'does the guy have a tattoo ?', 'answer': [(['yes'], 1)], 'answer_id': [(3, 1)]}\n",
      "[393227001] 1 {'question': 'what is this man riding on ?', 'answer': [(['skateboard'], 1)], 'answer_id': [(11, 1)]}\n",
      "[393227002] 2 {'question': \"how many tattoos can be seen on this man 's body ?\", 'answer': [(['1'], 1), (['3'], 0.3), (['2'], 0.3)], 'answer_id': [(12, 1), (32, 0.3), (17, 0.3)]}\n",
      "[393227003] 3 {'question': 'what color is his hat ?', 'answer': [(['backwards'], 0.3), (['blue'], 1)], 'answer_id': [(2497, 0.3), (13, 1)]}\n",
      "[393227004] 4 {'question': \"is the man 's visor providing his face enough protection ?\", 'answer': [(['no'], 1)], 'answer_id': [(9, 1)]}\n"
     ]
    }
   ],
   "source": [
    "#print(dict_train.keys())\n",
    "print(dict_train['393227'])\n",
    "for i, annot in enumerate(dict_train['393227']):\n",
    "    k = list(annot.keys())\n",
    "    print(k, i, annot[k[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dict_test['262144'])\n",
    "for i, annot in enumerate(dict_test['262144']):\n",
    "    k = list(annot.keys())\n",
    "    print(annot[k[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Image\n",
    "i = 1\n",
    "id_train = ids_train[i]\n",
    "print(id_train)\n",
    "print(txn)\n",
    "imgbin = txn.get(id_train.encode('utf-8'))\n",
    "buff = np.frombuffer(imgbin, dtype='uint8') \n",
    "#print(buff)\n",
    "imgbgr = cv2.imdecode(buff, cv2.IMREAD_COLOR)\n",
    "img = imgbgr[:,:,[2,1,0]]\n",
    "im = cv2.resize(img,(299,299)) \n",
    "print(im.shape)\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Storing images in memory\n",
    "def preprocess_images(all_ids_list, db, db2):\n",
    "    valid_ids = []\n",
    "    images_dict = {}\n",
    "    for i, id_train in enumerate(all_ids_list):\n",
    "        imgbin = db.get(id_train.encode('utf-8'))\n",
    "        if imgbin == None:\n",
    "            imgbin = db2.get(id_train.encode('utf-8'))\n",
    "            if imgbin == None:\n",
    "                raise Exception(\"image not found! %s\" % id_train)\n",
    "        buff = np.frombuffer(imgbin, dtype='uint8')        \n",
    "        imgbgr = cv2.imdecode(buff, cv2.IMREAD_COLOR)\n",
    "        img = imgbgr[:,:,[2,1,0]]\n",
    "        im = cv2.resize(img,(299,299))  \n",
    "        valid_ids.append(id_train)\n",
    "        images_dict[id_train] = im\n",
    "        if i % 3000 == 0:\n",
    "            print('Processed: ', i, ' images')\n",
    "    print('Done pre-processing images')            \n",
    "    return valid_ids, images_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  0  images\n",
      "Processed:  3000  images\n",
      "Processed:  6000  images\n",
      "Processed:  9000  images\n",
      "Processed:  12000  images\n",
      "Processed:  15000  images\n",
      "Processed:  18000  images\n",
      "Processed:  21000  images\n",
      "Processed:  24000  images\n",
      "Processed:  27000  images\n",
      "Processed:  30000  images\n",
      "Processed:  33000  images\n",
      "Processed:  36000  images\n",
      "Processed:  39000  images\n",
      "Processed:  42000  images\n",
      "Processed:  45000  images\n",
      "Processed:  48000  images\n",
      "Processed:  51000  images\n",
      "Processed:  54000  images\n",
      "Processed:  57000  images\n",
      "Processed:  60000  images\n",
      "Processed:  63000  images\n",
      "Processed:  66000  images\n",
      "Processed:  69000  images\n",
      "Processed:  72000  images\n",
      "Processed:  75000  images\n",
      "Processed:  78000  images\n",
      "Processed:  81000  images\n",
      "Processed:  84000  images\n",
      "Processed:  87000  images\n",
      "Processed:  90000  images\n",
      "Processed:  93000  images\n",
      "Processed:  96000  images\n",
      "Processed:  99000  images\n",
      "Processed:  102000  images\n",
      "Processed:  105000  images\n",
      "Processed:  108000  images\n",
      "Processed:  111000  images\n",
      "Processed:  114000  images\n",
      "Processed:  117000  images\n",
      "Processed:  120000  images\n",
      "Processed:  123000  images\n",
      "Done pre-processing images\n",
      "Done pre-processing training images\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing Training Images\n",
    "valid_ids_train, train_images = preprocess_images(ids_train, txn,txn_test)\n",
    "print('Done pre-processing training images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing Validation Images\n",
    "# valid_ids_val, val_images = preprocess_images(ids_val, txn, txn_test)\n",
    "# print('Done pre-processing validation images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_trainids 123287\n"
     ]
    }
   ],
   "source": [
    "print('valid_trainids', len(valid_ids_train))\n",
    "#print('valid ids vals', len(valid_ids_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sonam\n"
     ]
    }
   ],
   "source": [
    "print('sonam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this method select the attendance feature which has the highest score\n",
    "def new_fusion(v, e_s):\n",
    "     with tf.variable_scope('new_loss'):\n",
    "            ###sentence-level###\n",
    "            #heatmap pool\n",
    "            h_s = tf.nn.relu(tf.einsum('bj,blkj->blk',e_s,v)) #pair-wise e_bar*v^T: ?xNx4\n",
    "            #attention\n",
    "            a_s = tf.einsum('bjk,bjki->bik',h_s,v) #?xDx4 attnded visual reps for sen.\n",
    "            #pair-wise score\n",
    "            a_s_norm = tf.nn.l2_normalize(a_s,axis=1) # ? *D*4\n",
    "            e_s_norm = tf.nn.l2_normalize(e_s,axis=1) # ?*D\n",
    "\n",
    "            R_sk = tf.einsum('bik,bi->bk',a_s_norm,e_s_norm) #cosine for (sen,img_reps)\n",
    "            R_sk = tf.identity(R_sk,name='level_score_sentence')\n",
    "            #R_s = tf.reduce_max(R_sk,axis=-1,name='score_sentence') #?\n",
    "            #heatmap\n",
    "            idx_k = tf.argmax(R_sk,axis=-1,name='level_index_sentence') #? index of the featuremap which maximizes R_i\n",
    "            idx_k = tf.cast(idx_k, tf.int32)\n",
    "            \n",
    "            batch_size_tensor = tf.shape(a_s)[0]\n",
    "            #dim_tensor = tf.shape(a_s)[1]\n",
    "            dim_tensor = 1024\n",
    "            \n",
    "            ii, jj = tf.meshgrid(tf.range(batch_size_tensor), tf.range(dim_tensor), indexing='ij')\n",
    "            kk, ll = tf.meshgrid(idx_k, tf.range(dim_tensor), indexing='ij')\n",
    "            ii = tf.cast(ii, tf.int64)\n",
    "            jj = tf.cast(jj, tf.int64)\n",
    "            kk = tf.cast(kk, tf.int64)            \n",
    "            c = tf.stack([tf.reshape(ii, (-1,)), tf.reshape(jj, (-1,)), tf.reshape(kk, (-1,))], axis=1)\n",
    "            c = tf.reshape(c, (batch_size_tensor, dim_tensor, 3))\n",
    "\n",
    "            best_visual = tf.gather_nd(a_s_norm, c)\n",
    "            fusion = tf.concat([e_s_norm, best_visual], axis=1)    #?*2D\n",
    "            return fusion\n",
    "\n",
    "# this method use score as weight for calculating the average of attendance feature\n",
    "def new_fusion_avg(v, e_s):\n",
    "     with tf.variable_scope('new_loss'):\n",
    "            ###sentence-level###\n",
    "            #heatmap pool\n",
    "            h_s = tf.nn.relu(tf.einsum('bj,blkj->blk',e_s,v)) #pair-wise e_bar*v^T: ?xNx4\n",
    "            #attention\n",
    "            a_s = tf.einsum('bjk,bjki->bik',h_s,v) #?xDx4 attnded visual reps for sen.\n",
    "            #pair-wise score\n",
    "            a_s_norm = tf.nn.l2_normalize(a_s,axis=1) # ? *D*4\n",
    "            e_s_norm = tf.nn.l2_normalize(e_s,axis=1) # ?*D\n",
    "\n",
    "            R_sk = tf.einsum('bik,bi->bk',a_s_norm,e_s_norm) #cosine for (sen,img_reps)\n",
    "            R_sk = tf.identity(R_sk,name='level_score_sentence')\n",
    "            \n",
    "            R_sk = tf.expand_dims(R_sk, 2)\n",
    "            \n",
    "            visual = tf.matmul(a_s_norm, R_sk)\n",
    "            visual = tf.squeeze(visual,[-1])\n",
    "            fusion = tf.concat([e_s_norm, visual], axis=1)    #?*2D\n",
    "            return fusion\n",
    "\n",
    "# this method feeds four attendance features to dense layer to generate a new attendance feature \n",
    "def new_fusion_learn(v, e_s):\n",
    "     with tf.variable_scope('new_loss'):\n",
    "            ###sentence-level###\n",
    "            #heatmap pool\n",
    "            h_s = tf.nn.relu(tf.einsum('bj,blkj->blk',e_s,v)) #pair-wise e_bar*v^T: ?xNx4\n",
    "            #attention\n",
    "            a_s = tf.einsum('bjk,bjki->bik',h_s,v) #?xDx4 attnded visual reps for sen.\n",
    "            #pair-wise score\n",
    "            a_s_norm = tf.nn.l2_normalize(a_s,axis=1) # ? *D*4\n",
    "            e_s_norm = tf.nn.l2_normalize(e_s,axis=1) # ?*D\n",
    "\n",
    "            visual = tf.contrib.layers.flatten(a_s_norm)\n",
    "            visual = tf.layers.dense(visual, units=1024)\n",
    "            visual = tf.nn.leaky_relu(visual,alpha=.25)\n",
    "            visual = tf.layers.dense(visual, units=e_s_norm.get_shape().as_list()[1])\n",
    "            \n",
    "            fusion = tf.concat([e_s_norm, visual], axis=1)    #?*2D\n",
    "            return fusion        \n",
    "    \n",
    "def build_bilstm(w_embd,seq_length):\n",
    "    with tf.variable_scope('BiLSTM'):\n",
    "        # Forward direction cell\n",
    "        lstm_fw_cell = tf.contrib.rnn.LSTMCell(512, forget_bias=1.0)\n",
    "        # Backward direction cell\n",
    "        lstm_bw_cell = tf.contrib.rnn.LSTMCell(512, forget_bias=1.0)\n",
    "        outputs, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, w_embd, sequence_length = seq_length,\n",
    "                                                  dtype=tf.float32)\n",
    "    output = tf.concat(outputs,axis=2,name='BiLSTM_out')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_3by3_conv(feat_map,n_layers,n_filters,name,regularizer):\n",
    "    with tf.variable_scope(name+'_postConv'):\n",
    "        for i in range(n_layers):\n",
    "            with tf.variable_scope(name+'_stage_'+str(i)):\n",
    "                feat_map = tf.layers.conv2d(feat_map, filters=n_filters[i], kernel_size=[3,3],\n",
    "                                            kernel_regularizer=regularizer, padding='same')\n",
    "                feat_map = tf.nn.leaky_relu(feat_map, alpha=.25)\n",
    "    return feat_map\n",
    "\n",
    "\n",
    "# def depth_selection(model, conv_kernel_size=3, n_layers = 3):\n",
    "#     conv_method = None\n",
    "#     if conv_kernel_size == 1:\n",
    "#         conv_method = add_1by1_conv\n",
    "#     elif conv_kernel_size == 3:\n",
    "#         conv_method = add_3by3_conv\n",
    "#     else:\n",
    "#         raise ValueError('Invalid conv_kernel_size parameter. Should be either 1 or 3.')\n",
    "#     with tf.variable_scope('stack_v'):\n",
    "#         v1 = tf.identity(model['vgg_16/conv5/conv5_1'],name='v1')\n",
    "#         v1 = conv_method(v1, n_layers, n_filters=[1024,1024,1024], name='v1', regularizer=regularizer)\n",
    "# #         n_filters=[1024]\n",
    "#         size = v1.get_shape().as_list()[1:3]\n",
    "#         resize_method = tf.image.ResizeMethod.BILINEAR\n",
    "#         v2 = tf.identity(model['vgg_16/conv5/conv5_3'],name='v2')\n",
    "#         v2 = tf.image.resize_images(v2, size, method=resize_method)\n",
    "#         v2 = conv_method(v2,n_layers,n_filters=[1024,1024,1024],name='v2',regularizer=regularizer)\n",
    "#         v3 = tf.identity(model['vgg_16/conv4/conv4_1'],name='v3')\n",
    "#         #new\n",
    "#         #v3 = tf.layers.max_pooling2d(v3,[2,2],strides=2)\n",
    "#         #old\n",
    "#         v3 = tf.image.resize_images(v3, size, method=resize_method)\n",
    "#         v3 = conv_method(v3,n_layers,n_filters=[1024,1024,1024],name='v3',regularizer=regularizer)\n",
    "#         v4 = tf.identity(model['vgg_16/conv4/conv4_3'],name='v4')\n",
    "#         v4 = tf.image.resize_images(v4, size, method=resize_method)\n",
    "#         #new\n",
    "#         #v4 = tf.layers.max_pooling2d(v4,[2,2],strides=2)\n",
    "#         v4 = conv_method(v4,n_layers,n_filters=[1024,1024,1024],name='v4',regularizer=regularizer)\n",
    "#         v_all = tf.stack([v1,v2,v3,v4], axis=3)\n",
    "#         v_all = tf.reshape(v_all,[-1,v_all.shape[1]*v_all.shape[2],v_all.shape[3],v_all.shape[4]])\n",
    "#         v_all = tf.nn.l2_normalize(v_all, axis=-1, name='stacked_image_feature_maps')\n",
    "#     return v_all\n",
    "\n",
    "def depth_selection_with_maxpool_pnasnet(model, conv_kernel_size=3, n_layers = 3):\n",
    "    conv_method = None\n",
    "    if conv_kernel_size == 1:\n",
    "        conv_method = add_1by1_conv\n",
    "    elif conv_kernel_size == 3:\n",
    "        conv_method = add_3by3_conv\n",
    "    else:\n",
    "        raise ValueError('Invalid conv_kernel_size parameter. Should be either 1 or 3.')\n",
    "    with tf.variable_scope('stack_v'):\n",
    "        v1 = tf.identity(model['Cell_5'],name='v1')\n",
    "        v1 = conv_method(v1,n_layers,n_filters=[1024],name='v1',regularizer=regularizer)\n",
    "        size = v1.get_shape().as_list()[1:3]\n",
    "        resize_method = tf.image.ResizeMethod.BILINEAR\n",
    "        v2 = tf.identity(model['Cell_7'],name='v2')\n",
    "       #v2 = tf.image.resize_images(v2, size, method=resize_method)\n",
    "        v2 = conv_method(v2,n_layers,n_filters=[1024],name='v2',regularizer=regularizer)\n",
    "        v3 = tf.identity(model['Cell_9'],name='v3')\n",
    "        v3 = tf.image.resize_images(v3, size, method=resize_method)\n",
    "        v3 = conv_method(v3,n_layers,n_filters=[1024],name='v3',regularizer=regularizer)\n",
    "        v4 = tf.identity(model['Cell_11'],name='v4')\n",
    "        v4 = tf.image.resize_images(v4, size, method=resize_method)\n",
    "        v4 = conv_method(v4,n_layers,n_filters=[1024],name='v4',regularizer=regularizer)\n",
    "        v_all = tf.stack([v1,v2,v3,v4], axis=3)\n",
    "        v_all = tf.reshape(v_all,[-1,v_all.shape[1]*v_all.shape[2],v_all.shape[3],v_all.shape[4]])\n",
    "        v_all = tf.nn.l2_normalize(v_all, axis=-1, name='stacked_image_feature_maps')\n",
    "    return v_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_batch = 32\n",
    "reg_val = .0005\n",
    "n_layers = 1\n",
    "conv_kernel_size = 3\n",
    "n_ans = 3133\n",
    "lr_value_0 = .0001\n",
    "n_epochs = 20\n",
    "n_iter_per_epoch = int(len(dict_train) / n_batch)\n",
    "MAX_SEQ_LENGTH = 50\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "condition = 'VQA_MainFile_LSTM_learnable_fuse-3moredense_epoch15-PNASNet' + str(n_layers) +'bert_finetune'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_print(tensor, transform=None):\n",
    "\n",
    "    # Insert a custom python operation into the graph that does nothing but print a tensors value \n",
    "    def print_tensor(x):\n",
    "        # x is typically a numpy array here so you could do anything you want with it,\n",
    "        # but adding a transformation of some kind usually makes the output more digestible\n",
    "        print(x if transform is None else transform(x))\n",
    "        return x\n",
    "    \n",
    "    log_op = tf.py_func(print_tensor, [tensor], [tensor.dtype])[0]\n",
    "    with tf.control_dependencies([log_op]):\n",
    "        res = tf.identity(tensor)\n",
    "\n",
    "    # Return the given tensor\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Build Graph Using PreTrained Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sess.close()\n",
    "# tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     sess.close()\n",
    "# except NameError:\n",
    "#     print(\"Definining session for the first time\")\n",
    "# tf.reset_default_graph()\n",
    "# condition = 'model_Performance_AttnGrnd_VGG_Depth_Max_VG_3by3_resize_nlayer1'\n",
    "# print('Loading grounding pretrained model...')\n",
    "# model_path = '../saved_models/'+ condition\n",
    "# print(model_path)\n",
    "# #model_path = '../model_CNN_avg'\n",
    "# sess, graph = load_model(model_path,config)\n",
    "# print('Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saved_model_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "# print(len(saved_model_variables))\n",
    "# visual_model_variables = []\n",
    "# for x in saved_model_variables:\n",
    "#     if 'vgg_16' in str(x):\n",
    "#         visual_model_variables.append(x)   \n",
    "# print(len(visual_model_variables))\n",
    "# print(type(visual_model_variables))\n",
    "# print(type(saved_model_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_names(graph=tf.get_default_graph()):\n",
    "#     return [t.name for op in graph.get_operations() for t in op.values()]\n",
    "# print(get_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# v = sess.graph.get_tensor_by_name(\"stack_v/stacked_image_feature_maps:0\")\n",
    "# e_s = sess.graph.get_tensor_by_name(\"sen_embedding:0\")\n",
    "# fusion = new_fusion(v, e_s)\n",
    "\n",
    "# #with tf.variable_scope('VQAClassifier'):\n",
    "# fusion = tf.layers.dense(fusion, units=1024)\n",
    "# fusion = tf.nn.leaky_relu(fusion,alpha=.25)\n",
    "# fusion = tf.layers.dense(fusion, units=n_ans)\n",
    "# fusion = tf.identity(fusion, name='fusion')\n",
    "# #answer_batch = tf.placeholder('int64', shape=[None], name='answer_input')\n",
    "# answer_batch = tf.placeholder(tf.float32, shape=[n_batch, n_ans], name='answer_input')\n",
    "# #loss = tf.losses.sparse_softmax_cross_entropy(logits=fusion, labels=answer_batch)\n",
    "# # loss = attn_loss(e_w,v,e_s) + tf.losses.get_regularization_loss()\n",
    "# loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=fusion, labels=answer_batch)\n",
    "# loss = tf.reduce_mean(loss)\n",
    "# loss = tf.identity(loss, name='loss')\n",
    "\n",
    "# tf.summary.scalar('loss_value', loss)\n",
    "# lr = tf.placeholder(tf.float32, shape=[], name='learning_rate')\n",
    "# tf.summary.scalar('learning_rate', lr)\n",
    "# opt = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "\n",
    "# train_vars = list(set(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)) - set(visual_model_variables))\n",
    "# for x in train_vars:\n",
    "#     print(x)\n",
    "    \n",
    "# train_op = opt.minimize(loss, var_list=train_vars, name='train_op')\n",
    "\n",
    "# global_saver = tf.train.Saver()\n",
    "    \n",
    "# train_writer = tf.summary.FileWriter('./logs/vgg/vg', sess.graph)\n",
    "# merged = tf.summary.merge_all()\n",
    "# print('Model is built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for x in train_vars:\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Graph (Base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Visual Model...\n",
      "Building Text Model...\n",
      "Model is built\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sess.close()\n",
    "except NameError:\n",
    "    print(\"Definining session for the first time\")\n",
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "# T: max_length, D = 1024\n",
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    bert_module = hub.Module(BERT_MODEL_HUB, trainable=True)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                                tokenization_info[\"do_lower_case\"]])\n",
    "    tokenizer = bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "\n",
    "    mode = tf.placeholder(tf.string, name='mode')\n",
    "    isTraining = tf.equal(mode, 'train')\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(reg_val)\n",
    "    #Building visual model\n",
    "    print('Building Visual Model...')\n",
    "    input_img = tf.placeholder(tf.float32, (None,299,299,3), name='input_img')\n",
    "    \n",
    "#     #For VG Training\n",
    "#     pre_processed_img = pre_process(input_img, 'vgg_preprocessing')\n",
    "#     vis_model = pre_trained_load(model_name='vgg_16', image_shape=(None,299,299,3),\n",
    "#                               input_tensor=pre_processed_img, session=sess, is_training=False, global_pool=True)\n",
    "#     v = depth_selection(vis_model, conv_kernel_size = 3, n_layers = n_layers) #(?,1225,4,1024)\n",
    "\n",
    "#     For PNAS Net Training\n",
    "    pre_processed_img = pre_process(input_img, 'inception_preprocessing')\n",
    "    vis_model = pre_trained_load(model_name='pnasnet_large', image_shape=(None,299,299,3),\n",
    "                                 input_tensor=pre_processed_img, session=sess, is_training=False, global_pool=True)\n",
    "    v = depth_selection_with_maxpool_pnasnet(vis_model, n_layers = n_layers) #(?,1225,4,1024)\n",
    "\n",
    "    #Building text model\n",
    "    print('Building Text Model...')\n",
    "    #sentence placeholder - list of sentences\n",
    "#     text_batch = tf.placeholder('string', shape=[None], name='text_input')\n",
    "#     #loading pre-trained ELMo\n",
    "#     elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "#     #getting ELMo embeddings\n",
    "#     print('Loading Embeddings')\n",
    "#     elmo_embds = elmo(text_batch, signature=\"default\", as_dict=True)\n",
    "#     lstm1_embd = elmo_embds['lstm_outputs1'] #?xTXD\n",
    "#     lstm2_embd = elmo_embds['lstm_outputs2'] #?xTXD\n",
    "#     w_embd = tf.identity(elmo_embds['elmo'], name='elmo_word_embd') #?xTXD\n",
    "#     #taking index of last word in each sentence\n",
    "#     idx = elmo_embds['sequence_len']-1 # [l1,l2,.....?]-1\n",
    "    \n",
    "#     #tf.size(idx) :batchsize\n",
    "#     batch_idx = tf.stack([tf.range(0,tf.size(idx),1),idx],axis=1) # batchsize*2 , 1 col: 0,1,2... 2 col: lengths-1\n",
    "    \n",
    "#     # Concatenate first of backward with last of forward to get sentence embeddings\n",
    "#     print('Getting Sentence Embeddings')\n",
    "#     dim = lstm1_embd.get_shape().as_list()[-1] #?,?, 1024\n",
    "    \n",
    "#     sen_embd_1 = tf.concat([lstm1_embd[:,0,int(dim/2):],\n",
    "#                             tf.gather_nd(lstm1_embd[:,:,:int(dim/2)],batch_idx)], axis=-1) #[batch,dim]\n",
    "    \n",
    "#     sen_embd_2 = tf.concat([lstm2_embd[:,0,int(dim/2):],\n",
    "#                             tf.gather_nd(lstm2_embd[:,:,:int(dim/2)],batch_idx)], axis=-1) #[batch,dim]\n",
    "    \n",
    "#     sen_embd = tf.concat([tf.expand_dims(sen_embd_1,axis=2),\n",
    "#                                tf.expand_dims(sen_embd_2,axis=2)], axis=2, name='elmo_sen_embd') #[batch,dim]\n",
    "    \n",
    "#     e_s = tf.layers.dense(sen_embd,units=1,use_bias=False) #?xDx1\n",
    "\n",
    "#     e_s = tf.squeeze(e_s,axis=2) #?*D\n",
    "#     e_s = tf.layers.dense(e_s, units=1024)\n",
    "#     e_s = tf.nn.leaky_relu(e_s,alpha=.25)\n",
    "#     e_s = tf.layers.dense(e_s, units=1024)\n",
    "#     e_s = tf.nn.leaky_relu(e_s,alpha=.25) #?*D\n",
    "#     e_s = tf.nn.l2_normalize(e_s, axis=-1, name='sen_embedding')\n",
    "    \n",
    "\n",
    "#     e_w = tf.layers.dense(w_embd, units=1024) #?xTXD\n",
    "#     e_w = tf.nn.leaky_relu(e_w,alpha=.25)\n",
    "#     e_w = tf.layers.dense(e_w, units=1024)\n",
    "#     e_w = tf.nn.leaky_relu(e_w,alpha=.25)\n",
    "#     e_w = tf.nn.l2_normalize(e_w, axis=-1, name='w_embedding') #?xTXD\n",
    "   \n",
    "\n",
    "     \n",
    "#     #sentence placeholder - list of sentences\n",
    "#     #text_batch = tf.placeholder('string', shape=[None], name='text_input')\n",
    "    #loading pre-trained Bert\n",
    "    input_ids = tf.placeholder(\"int32\", shape=[None,MAX_SEQ_LENGTH], name='input_ids_input')\n",
    "    input_mask = tf.placeholder(\"int32\", shape=[None,MAX_SEQ_LENGTH], name='input_mask_input')\n",
    "    segment_ids = tf.placeholder(\"int32\", shape=[None,MAX_SEQ_LENGTH], name='segment_ids_input')\n",
    "    bert_inputs = dict(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(bert_inputs, signature=\"tokens\", as_dict=True)\n",
    "    sen_embd = bert_outputs[\"pooled_output\"]\n",
    "    #sequence_output = bert_outputs[\"sequence_output\"]\n",
    "    #sen_embd = tf.placeholder('float', shape=[None, 768], name='text_input')\n",
    "    \n",
    "    \n",
    "#     w_embd = tf.identity(elmo_embds['word_emb'], name='elmo_word_embd') #?xTxD/2\n",
    "#     lstm_embd = build_bilstm(w_embd,elmo_embds['sequence_len']) #?xTxD\n",
    "    \n",
    "#     #taking index of last word in each sentence\n",
    "#     idx = elmo_embds['sequence_len']-1\n",
    "#     batch_idx = tf.stack([tf.range(0,tf.size(idx),1),idx],axis=1)\n",
    "#     # Concatenate first of backward with last of forward to get sentence embeddings\n",
    "#     dim = lstm_embd.get_shape().as_list()[-1]\n",
    "#     sen_embd = tf.concat([lstm_embd[:,0,int(dim/2):],\n",
    "#     tf.gather_nd(lstm_embd[:,:,:int(dim/2)],batch_idx)], axis=-1) #[batch,dim]\n",
    "\n",
    "    e_s = tf.layers.dense(sen_embd, units=1024)\n",
    "    e_s = tf.nn.leaky_relu(e_s,alpha=.25)\n",
    "    e_s = tf.layers.dense(e_s, units=1024)\n",
    "    e_s = tf.nn.leaky_relu(e_s,alpha=.25)\n",
    "    e_s = tf.nn.l2_normalize(e_s, axis=-1, name='sen_embedding')\n",
    "\n",
    "#     w_embd_tiled = tf.tile(w_embd,[1,1,2])\n",
    "#     w_embd = tf.concat([tf.expand_dims(w_embd_tiled,axis=3),tf.expand_dims(lstm_embd,axis=3)],axis=3)\n",
    "#     w_embd = tf.layers.dense(w_embd, units=1)[:,:,:,0]\n",
    "#     e_w = tf.layers.dense(w_embd, units=1024)\n",
    "#     e_w = tf.nn.leaky_relu(e_w,alpha=.25)\n",
    "#     e_w = tf.layers.dense(e_w, units=1024)\n",
    "#     e_w = tf.nn.leaky_relu(e_w,alpha=.25)\n",
    "#     e_w = tf.nn.l2_normalize(e_w, axis=-1, name='w_embedding')\n",
    "    \n",
    "    \n",
    "# print('Generating Heatmaps')\n",
    "   # heatmap_w,heatmap_s,R_i,R_s = attn(e_w,v,e_s)\n",
    "    \n",
    "    with tf.variable_scope('VQAClassifier'):\n",
    "        fusion = new_fusion_learn(v, e_s)\n",
    "        fusion = tf.layers.dense(fusion, units=1024)\n",
    "        fusion = tf.nn.leaky_relu(fusion,alpha=.25)\n",
    "        fusion = tf.layers.dense(fusion, units=1024)\n",
    "        fusion = tf.nn.leaky_relu(fusion,alpha=.25)\n",
    "        fusion = tf.layers.dense(fusion, units=1024)\n",
    "        fusion = tf.nn.leaky_relu(fusion,alpha=.25)\n",
    "        fusion = tf.layers.dense(fusion, units=1024)\n",
    "        fusion = tf.nn.leaky_relu(fusion,alpha=.25)\n",
    "        fusion = tf.layers.dense(fusion, units=n_ans)\n",
    "        fusion = tf.identity(fusion, name='fusion')\n",
    "    \n",
    "    answer_batch = tf.placeholder(tf.float32, shape=[n_batch, n_ans], name='answer_input')\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=fusion, labels=answer_batch)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    loss = tf.identity(loss, name='loss')\n",
    "\n",
    "    tf.summary.scalar('loss_value', loss)\n",
    "\n",
    "    lr = tf.placeholder(tf.float32, shape=[], name='learning_rate')\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "    opt = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    train_vars = list(set(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)) - set(vis_model.model_weights))\n",
    "    train_op = opt.minimize(loss, var_list=train_vars, name='train_op') \n",
    "\n",
    "    train_writer = tf.summary.FileWriter('./logs/vgg/vg', sess.graph)\n",
    "    merged = tf.summary.merge_all()\n",
    "    print('Model is built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if restoring variables\n",
    "# condition = 'model_Performance_AttnGrnd_VGG_Depth_Max_VG_3by3_resize_nlayer1'\n",
    "# ckpt_path = '../saved_models/'+ condition   \n",
    "# vars_to_restore = []\n",
    "# vars_to_initialize = []\n",
    "# for x in tf.global_variables():\n",
    "#     if 'VQAClassifier' not in str(x):\n",
    "#         vars_to_restore.append(x) \n",
    "#     else:\n",
    "#         vars_to_initialize.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in train_vars:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for x in tf.global_variables():\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_gen(valid_ids, annot_dict, train_images,n_batch):\n",
    "    img_batch = np.empty((n_batch, 299, 299, 3), dtype='float32')\n",
    "    cap_batch = []\n",
    "    ans_batch = []\n",
    "    chosen_ids = random.sample(valid_ids, n_batch)\n",
    "    for i, chosen_id in enumerate(chosen_ids):\n",
    "        imgbin = train_images[chosen_id]\n",
    "        img_batch[i,:,:,:] = imgbin\n",
    "        questions = []\n",
    "        answers = []\n",
    "        for annot in annot_dict[chosen_id]:\n",
    "            q_id = list(annot.keys())[0] \n",
    "            questions.append(annot[q_id]['question'])\n",
    "            label = np.zeros(3133, dtype='float32')\n",
    "            for answer in annot[q_id]['answer_id']:\n",
    "                label[answer[0]] = answer[1]\n",
    "            answers.append(label)\n",
    "        n_ques = len(questions)\n",
    "        idx = random.choice(range(n_ques))  \n",
    "        cap_batch.append(questions[idx])\n",
    "        ans_batch.append(answers[idx])\n",
    "        \n",
    "    return img_batch, cap_batch, ans_batch\n",
    "\n",
    "def preprocessbertinputs(text_batch, tokenizer):\n",
    "    label_list = [0,1]\n",
    "    train_InputExamples = [bert.run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in text_batch ]\n",
    "    train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer) \n",
    "    input_ids_bert =  [x.input_ids for x in train_features]\n",
    "    input_mask_bert = [x.input_mask for x in train_features]\n",
    "    segment_ids_bert = [x.segment_ids for x in train_features]\n",
    "    return input_ids_bert, input_mask_bert, segment_ids_bert\n",
    "\n",
    "def bert_sentenceembedding(text_batch,bert_embedding):\n",
    "    n_batch = len(text_batch)\n",
    "    sen_emb = np.zeros(n_batch, 768)\n",
    "    result = bert_embedding(text_batch)\n",
    "    sen_embd = [sum(x[1])/len(x[1]) for x in result]\n",
    "    return sen_embd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Question:\n",
    "    def __init__(self,question_id,image_id,question,answer_id=None):\n",
    "        self.question_id = question_id\n",
    "        self.image_id = image_id\n",
    "        self.question = question\n",
    "        self.answer_id = answer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Generating All Questions in Validation Batch\n",
    "# final_val_list = []\n",
    "# for k, doc_id in enumerate(dict_val):\n",
    "#     image_id = doc_id\n",
    "#     for annot in dict_val[doc_id]:\n",
    "#         question_id = list(annot.keys())[0]\n",
    "#         question = annot[question_id]['question']\n",
    "#         answer_id = annot[question_id]['answer_id']\n",
    "#         ques = Question(question_id ,image_id, question,answer_id)\n",
    "#         final_val_list.append(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('Validation_Questions', len(final_val_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def generate_batches(questions_list, batch_size):\n",
    "#     n = len(questions_list)\n",
    "    \n",
    "#     questions_batch = []\n",
    "#     for i, ques in enumerate(questions_list):\n",
    "#         questions_batch.append(ques)\n",
    "#         if ((i + 1) % batch_size == 0) or ((i + 1) == n):\n",
    "#             yield questions_batch\n",
    "#             questions_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating validation data\n",
    "# def validate_val(final_val_list, val_images, idx2ans, batch_size):\n",
    "#     val_acc = 0\n",
    "#     total_val_acc = 0\n",
    "#     total_cnt = 0\n",
    "#     print(\"----started\")\n",
    "\n",
    "#     l = 0\n",
    "#     for question_batch in generate_batches(final_val_list, batch_size):\n",
    "#         num_questions = len(question_batch)\n",
    "        \n",
    "#         img_batch = np.empty((num_questions, 299, 299, 3))\n",
    "#         ques_batch = []\n",
    "#         ques_ids_batch = []\n",
    "#         true_answer_batch =[]\n",
    "#         results = list()\n",
    "#         total_cnt += num_questions\n",
    "        \n",
    "#         for j, ques in enumerate(question_batch):\n",
    "#             image_id = ques.image_id\n",
    "#             question_id = ques.question_id\n",
    "#             question = ques.question\n",
    "#             true_answer = ques.answer_id\n",
    "\n",
    "#             img = np.reshape(val_images[image_id], (299, 299, 3))\n",
    "#             img_batch[j,:] = img\n",
    "#             ques_batch.append(question)\n",
    "#             ques_ids_batch.append(question_id)\n",
    "#             true_answer_batch.append(true_answer)\n",
    "        \n",
    "#         feed_dict = {input_img: img_batch, text_batch: ques_batch, mode:\"test\"}\n",
    "#         pred = sess.run(fusion, feed_dict)\n",
    "#         idx = np.argmax(pred, axis=1)\n",
    "        \n",
    "        \n",
    "#         for k, each_idx in enumerate(idx):\n",
    "#             ground_truth_list = true_answer_batch[k]\n",
    "#             score = 0\n",
    "#             for ansid, ans_score in ground_truth_list:\n",
    "#                 if ansid==each_idx:\n",
    "#                     score =ans_score\n",
    "#                     break        \n",
    "#             total_val_acc += score\n",
    "            \n",
    "#             results.append({\"question_id\": ques_ids_batch[k], \"answer\": idx2ans[each_idx]}) \n",
    "#         l += 1\n",
    "#         if l % 100 == 0:\n",
    "#             print(\"Number of batches done: \", l, \"; num_results: \", str(len(results)))\n",
    "        \n",
    "#         val_acc = total_val_acc/total_cnt\n",
    "        \n",
    "#     return val_acc*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print('Initializing...')\n",
    "_ = sess.run([tf.global_variables_initializer()])\n",
    "#_ = sess.run([tf.global_variables_initializer(initializer_variable_list)])\n",
    "#loading pretrained vgg weights\n",
    "#print('Loading visual path model (vgg)...')\n",
    "#vis_model.load_weights()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading visual path model (vgg)...\n"
     ]
    }
   ],
   "source": [
    "print('Loading visual path model (vgg)...')\n",
    "vis_model.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sonam\n"
     ]
    }
   ],
   "source": [
    "global_saver = tf.train.Saver()\n",
    "print('sonam')\n",
    "# global_saver.restore(sess, ckpt_path)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'module/bert/encoder/layer_1/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_8/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
      "<tf.Variable 'dense/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'dense/kernel:0' shape=(768, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'VQAClassifier/dense_2/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_7/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_0/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_5/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_7/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_0/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_5/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_7/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_0/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_5/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_7/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_0/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_5/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_7/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_0/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_5/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_7/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_0/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_5/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
      "<tf.Variable 'VQAClassifier/new_loss/dense/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_7/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_0/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
      "<tf.Variable 'VQAClassifier/new_loss/dense/kernel:0' shape=(4096, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_5/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'VQAClassifier/dense_4/kernel:0' shape=(1024, 3133) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_7/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_0/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_6/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_7/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
      "<tf.Variable 'dense_1/kernel:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_0/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_6/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_7/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_1/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_6/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_7/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_1/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_6/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_8/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_1/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_6/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'stack_v/v1_postConv/v1_stage_0/conv2d/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_8/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_1/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_6/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'VQAClassifier/new_loss/dense_1/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_8/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_1/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_6/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'dense_1/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_8/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_1/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_6/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_8/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_1/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'VQAClassifier/new_loss/dense_1/kernel:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_6/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_8/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_1/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_6/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_8/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_1/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_6/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_8/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_1/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_6/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_8/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_1/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_5/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_7/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'VQAClassifier/dense_2/kernel:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_5/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/cls/predictions/transform/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/embeddings/word_embeddings:0' shape=(30522, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_8/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_3/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_10/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'VQAClassifier/dense/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'VQAClassifier/dense_4/bias:0' shape=(3133,) dtype=float32_ref>\n",
      "<tf.Variable 'stack_v/v1_postConv/v1_stage_0/conv2d/kernel:0' shape=(3, 3, 2160, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_3/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
      "<tf.Variable 'stack_v/v2_postConv/v2_stage_0/conv2d/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_2/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_8/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_8/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
      "<tf.Variable 'VQAClassifier/dense/kernel:0' shape=(2048, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_2/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/cls/predictions/transform/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_10/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_8/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_2/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_3/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_8/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/cls/predictions/output_bias:0' shape=(30522,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_2/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_3/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_9/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_3/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
      "<tf.Variable 'stack_v/v2_postConv/v2_stage_0/conv2d/kernel:0' shape=(3, 3, 2160, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_9/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_2/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_3/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_9/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_3/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'stack_v/v3_postConv/v3_stage_0/conv2d/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_9/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_3/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'VQAClassifier/dense_1/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_2/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_4/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_9/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_3/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_2/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_10/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_4/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'stack_v/v3_postConv/v3_stage_0/conv2d/kernel:0' shape=(3, 3, 4320, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_4/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_10/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_9/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_2/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_11/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_4/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'stack_v/v4_postConv/v4_stage_0/conv2d/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_4/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_10/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_9/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_2/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_11/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_4/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_4/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_9/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_2/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_11/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_4/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_4/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_9/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'stack_v/v4_postConv/v4_stage_0/conv2d/kernel:0' shape=(3, 3, 4320, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'VQAClassifier/dense_3/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_2/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
      "<tf.Variable 'VQAClassifier/dense_1/kernel:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_11/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_5/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_11/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_4/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_9/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_5/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_11/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'VQAClassifier/dense_3/kernel:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'module/bert/encoder/layer_4/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_2/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_11/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_5/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_2/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_11/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_4/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_2/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_5/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_2/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_11/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_4/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_6/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/cls/predictions/transform/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_0/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_3/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_11/attention/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_6/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/cls/predictions/transform/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_4/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_9/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_0/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_3/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_6/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_4/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_0/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_0/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_10/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_3/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_6/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_0/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_4/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_10/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_7/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_3/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_1/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_5/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_10/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_7/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_3/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_11/output/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_1/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_2/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_5/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_7/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_10/attention/self/key/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_1/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_0/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_3/attention/self/value/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_5/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_7/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_10/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_1/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_3/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_0/attention/self/query/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/pooler/dense/bias:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_8/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>\n",
      "<tf.Variable 'module/bert/encoder/layer_10/attention/self/value/bias:0' shape=(768,) dtype=float32>\n"
     ]
    }
   ],
   "source": [
    "for x in train_vars:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA_MainFile_LSTM_learnable_fuse-3moredense_epoch15-PNASNet1bert_finetune\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(condition)\n",
    "lr_value_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "\n",
      "=====Epoch: 20\n",
      "===Train\n",
      "Sample 123232/123264, train_loss:0.0022 \r"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 20 is out of bounds for axis 0 with size 20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-2074701849f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m#train_loss[e] = avg_loss/float(n_iter_per_epoch+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter_per_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\nTraining done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 20 is out of bounds for axis 0 with size 20"
     ]
    }
   ],
   "source": [
    "#loop on training data\n",
    "print('Start training...')\n",
    "max_class_acc = 0\n",
    "val_acc = np.zeros((n_epochs,))\n",
    "train_loss = np.zeros((n_epochs,))\n",
    "#bert_embedding = BertEmbedding()\n",
    "\n",
    "for e in range(n_epochs,30):\n",
    "    print('\\n\\n=====Epoch: %d'%e)\n",
    "    avg_loss = 0\n",
    "#     if e < 9:\n",
    "#         lr_value = lr_value_0\n",
    "#     elif 9 <= e < 14:\n",
    "#         lr_value = lr_value_0 / 2.0\n",
    "#     elif e >= 14:\n",
    "#         lr_value = lr_value_0 / 4.0\n",
    "        \n",
    "    print('===Train')\n",
    "    for i in range(n_iter_per_epoch):\n",
    "        img_batch, cap_batch, ans_batch = batch_gen(valid_ids_train, dict_train, train_images,n_batch)\n",
    "        input_ids_bert,input_mask_bert,segment_ids_bert =  preprocessbertinputs(cap_batch, tokenizer)\n",
    "        #sen_emb_batch = bert_sentenceembedding(cap_batch,bert_embedding)\n",
    "        #feed_dict = {input_img: img_batch, text_batch: sen_emb_batch, answer_batch: ans_batch, mode: 'train', lr: lr_value}\n",
    "        feed_dict = {input_img: img_batch, input_ids:input_ids_bert,input_mask:input_mask_bert,segment_ids:segment_ids_bert, answer_batch: ans_batch, mode: 'train', lr: lr_value}\n",
    "        loss_val, _ = sess.run([loss, train_op], feed_dict)\n",
    "        avg_loss += loss_val\n",
    "        var = [i * n_batch, n_iter_per_epoch * n_batch, avg_loss / float(i + 1)]\n",
    "        prnt = 'Sample {}/{}, train_loss:{:.4f} \\r'.format(var[0], var[1], var[2])\n",
    "        sys.stdout.write(prnt)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    if e == 15:\n",
    "        print('Saving model...')\n",
    "        global_saver.save(sess, '../saved_models/model_' + condition + \"15\")\n",
    "            \n",
    "    #train_loss[e] = avg_loss/float(n_iter_per_epoch+1)\n",
    "    train_loss[e] = avg_loss/float(n_iter_per_epoch+1)  \n",
    "        \n",
    "print('\\n\\nTraining done.')\n",
    "#saving the session\n",
    "print('Saving model...')\n",
    "global_saver.save(sess, '../saved_models/model_' + condition + \"20\")\n",
    "with open('./logs/loss_' + condition + '.pickle', 'wb') as f:\n",
    "    pickle.dump({'train_loss':train_loss,\n",
    "                 'class_acc':val_acc},\n",
    "                f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "print('Saving done.')\n",
    "     \n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(train_loss, label='Train loss '+ condition)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(100. * val_acc, label='Validation classification_acc '+ condition)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAD8CAYAAAD5V+dGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIrcYFoiIiMit/w9DKwndw3MJnQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff2bc95f3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmgAAAD8CAYAAADDh1g1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb839f33da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_loss, label='Train loss '+ condition)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(100. * val_acc, label='Validation classification_acc '+ condition)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA_MainFile_LSTM_learnable_fuse-3moredense_epoch15-PNASNet1bert_finetune\n",
      "20\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "print(condition)\n",
    "print(n_epochs)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sess.close()\n",
    "except NameError:\n",
    "    print(\"Definining session for the first time\")\n",
    "#tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deleting Variables to Free Space\n",
    "del train_images, val_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "condition = 'VQA_MainFile_LSTM_learnable_fuse-3moredense_epoch15-PNASNet115'\n",
    "print('Loading grounding pretrained model...')\n",
    "model_path = '../saved_models/model_'+ condition\n",
    "print(model_path)\n",
    "sess, graph = load_model(model_path,config)\n",
    "print('Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_img = sess.graph.get_tensor_by_name(\"input_img:0\")\n",
    "text_batch = sess.graph.get_tensor_by_name(\"text_input:0\")\n",
    "mode = sess.graph.get_tensor_by_name(\"mode:0\")\n",
    "fusion = sess.graph.get_tensor_by_name(\"VQAClassifier/fusion:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  0  images\n",
      "Processed:  3000  images\n",
      "Processed:  6000  images\n",
      "Processed:  9000  images\n",
      "Processed:  12000  images\n",
      "Processed:  15000  images\n",
      "Processed:  18000  images\n",
      "Processed:  21000  images\n",
      "Processed:  24000  images\n",
      "Processed:  27000  images\n",
      "Processed:  30000  images\n",
      "Processed:  33000  images\n",
      "Processed:  36000  images\n",
      "Processed:  39000  images\n",
      "Processed:  42000  images\n",
      "Processed:  45000  images\n",
      "Processed:  48000  images\n",
      "Processed:  51000  images\n",
      "Processed:  54000  images\n",
      "Processed:  57000  images\n",
      "Processed:  60000  images\n",
      "Processed:  63000  images\n",
      "Processed:  66000  images\n",
      "Processed:  69000  images\n",
      "Processed:  72000  images\n",
      "Processed:  75000  images\n",
      "Processed:  78000  images\n",
      "Processed:  81000  images\n",
      "Done pre-processing images\n",
      "Done pre-processing test images\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing Testing Images\n",
    "valid_ids_test, test_images = preprocess_images(ids_test, txn_test, txn)\n",
    "print('Done pre-processing test images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{262144000: {'question': 'is the ball flying towards the batter ?'}},\n",
       " {262144001: {'question': 'what sport is this ?'}},\n",
       " {262144002: {'question': 'can you see the ball ?'}},\n",
       " {262144003: {'question': 'is the pitcher wearing a hat ?'}},\n",
       " {262144004: {'question': 'will he catch the ball in time ?'}},\n",
       " {262144005: {'question': 'what credit card company is on the banner in the background ?'}}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(list(dict_test.keys()))\n",
    "dict_test['262144']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generating All Questions in Answer Batch\n",
    "final_test_list = []\n",
    "for k, doc_id in enumerate(dict_test):\n",
    "    image_id = doc_id\n",
    "    #print(doc_id)\n",
    "    for annot in dict_test[doc_id]:\n",
    "        question_id = list(annot.keys())[0]\n",
    "        #print(annot[question_id])\n",
    "        question = annot[question_id]['question']\n",
    "        ques = Question(question_id ,image_id, question)\n",
    "        final_test_list.append(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "447793"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_test_list)\n",
    "#final_test_list[0].question_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batches(questions_list, batch_size):\n",
    "    n = len(questions_list)\n",
    "    \n",
    "    questions_batch = []\n",
    "    for i, ques in enumerate(questions_list):\n",
    "        questions_batch.append(ques)\n",
    "        if ((i + 1) % batch_size == 0) or ((i + 1) == n):\n",
    "            yield questions_batch\n",
    "            questions_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_test(final_test_list, test_images, idx2ans, batch_size):\n",
    "    results = list()\n",
    "    print(\"----started\")\n",
    "\n",
    "    l = 0\n",
    "    for question_batch in generate_batches(final_test_list, batch_size):\n",
    "        num_questions = len(question_batch)\n",
    "        img_batch = np.empty((num_questions, 299, 299, 3))\n",
    "        ques_batch = []\n",
    "        ques_ids_batch = []\n",
    "        \n",
    "        for j, ques in enumerate(question_batch):\n",
    "            image_id = ques.image_id\n",
    "            question_id = ques.question_id\n",
    "            question = ques.question\n",
    "\n",
    "            img = np.reshape(test_images[image_id], (299, 299, 3))\n",
    "            img_batch[j,:] = img\n",
    "            ques_batch.append(question)\n",
    "            ques_ids_batch.append(question_id)\n",
    "        \n",
    "        input_ids_test,input_mask_test,segment_ids_test =  preprocessbertinputs(ques_batch, tokenizer)\n",
    "        feed_dict = {input_img: img_batch, input_ids:input_ids_test,input_mask:input_mask_test,segment_ids:segment_ids_test, mode:\"test\"}\n",
    "        #feed_dict = {input_img: img_batch, text_batch: ques_batch, mode:\"test\"}\n",
    "        pred = sess.run(fusion, feed_dict)\n",
    "        idx = np.argmax(pred, axis=1)\n",
    "        for k, each_idx in enumerate(idx):\n",
    "            results.append({\"question_id\": ques_ids_batch[k], \"answer\": idx2ans[each_idx]}) \n",
    "        l += 1\n",
    "        if l % 100 == 0:\n",
    "            print(\"Number of batches done: \", l, \"; num_results: \", str(len(results)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----started\n",
      "Number of batches done:  100 ; num_results:  6400\n",
      "Number of batches done:  200 ; num_results:  12800\n",
      "Number of batches done:  300 ; num_results:  19200\n",
      "Number of batches done:  400 ; num_results:  25600\n",
      "Number of batches done:  500 ; num_results:  32000\n",
      "Number of batches done:  600 ; num_results:  38400\n",
      "Number of batches done:  700 ; num_results:  44800\n",
      "Number of batches done:  800 ; num_results:  51200\n",
      "Number of batches done:  900 ; num_results:  57600\n",
      "Number of batches done:  1000 ; num_results:  64000\n",
      "Number of batches done:  1100 ; num_results:  70400\n",
      "Number of batches done:  1200 ; num_results:  76800\n",
      "Number of batches done:  1300 ; num_results:  83200\n",
      "Number of batches done:  3200 ; num_results:  204800\n",
      "Number of batches done:  3300 ; num_results:  211200\n",
      "Number of batches done:  3400 ; num_results:  217600\n",
      "Number of batches done:  3500 ; num_results:  224000\n",
      "Number of batches done:  3600 ; num_results:  230400\n",
      "Number of batches done:  3700 ; num_results:  236800\n"
     ]
    }
   ],
   "source": [
    "results = validate_test(final_test_list, test_images, idx2ans, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447793\n"
     ]
    }
   ],
   "source": [
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#writing results to disk\n",
    "import pickle\n",
    "f = open('model_'+ condition +'_results.pkl', \"wb\")\n",
    "pickle.dump(results, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving as json file to fetch to server\n",
    "import json \n",
    "with open('model_'+ condition + '20_results.json', 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQA_MainFile_LSTM_learnable_fuse-3moredense_epoch15-PNASNet1bert_finetune\n"
     ]
    }
   ],
   "source": [
    "print(condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./logs/loss_VQA_MainFile_Pretrained1.pickle', 'rb') as f:\n",
    "    p = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
