{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow_hub as hub\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "config = tf.ConfigProto(gpu_options=gpu_options, log_device_placement=True, allow_soft_placement=True)\n",
    "import lmdb\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"/home/sonam/tfhub_modules\"\n",
    "import time\n",
    "print('Number of GPUs: ', tf.contrib.eager.num_gpus())\n",
    "\n",
    "# Jupyter caches the utils module, so changes there aren't reflected in a re-import. We delete the utils module from\n",
    "# sys.modules first so that we force-reload all the code in utils.py\n",
    "if 'utils' in sys.modules:\n",
    "    del sys.modules['utils']\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_images(all_ids_list, db):\n",
    "    valid_ids = []\n",
    "    images_dict = {}\n",
    "    for i, id_train in enumerate(all_ids_list):\n",
    "        imgbin = db.get(id_train.encode('utf-8'))\n",
    "        if imgbin == None:\n",
    "            continue\n",
    "        buff = np.frombuffer(imgbin, dtype='uint8')        \n",
    "        imgbgr = cv2.imdecode(buff, cv2.IMREAD_COLOR)\n",
    "        img = imgbgr[:,:,[2,1,0]]\n",
    "        im = cv2.resize(img,(299,299))  \n",
    "        valid_ids.append(id_train)\n",
    "        images_dict[id_train] = im\n",
    "        if i % 10000 == 0:\n",
    "            print('Processed: ', i, ' images')\n",
    "    print('Finished preprocessing images.')\n",
    "    return valid_ids, images_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = '/dvmm-filer2/datasets/Groundings/data/'\n",
    "data_path = base_path + 'visualgenome/'\n",
    "dict_paths = {'train': data_path + 'train.pickle', 'val': data_path + 'val.pickle', 'test': data_path + 'test.pickle'} \n",
    "lmdb_path = data_path + 'data.lmdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading visual genome img data \n",
    "lmdb_env = lmdb.open(lmdb_path, map_size=int(1e11), readonly=True, lock=False)\n",
    "txn = lmdb_env.begin(write=False)\n",
    "\n",
    "with open(dict_paths['train'], 'rb') as f:\n",
    "    dict_train = pickle.load(f, encoding='latin1')\n",
    "    ids_train = list(dict_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_ids_train, train_images = preprocess_images(ids_train, txn)    \n",
    "print('Done pre-processing training images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading referit data\n",
    "ref_path = base_path + 'referit/referit_splits/'\n",
    "ref_lmdb_path = base_path + 'referit/refclef_data/all_data.lmdb'\n",
    "ref_val_path = ref_path + 'val.pickle' \n",
    "\n",
    "with open(ref_val_path, 'rb') as f:\n",
    "    dict_val = pickle.load(f, encoding='latin1')\n",
    "    ids_val = list(dict_val.keys())\n",
    "    \n",
    "ref_lmdb_env = lmdb.open(ref_lmdb_path, map_size=int(1e11), readonly=True, lock=False)\n",
    "ref_txn = ref_lmdb_env.begin(write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_ids_val, val_images = preprocess_images(ids_val, ref_txn)\n",
    "print('Done pre-processing validation images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_batch = 32\n",
    "gamma_1 = 5.0\n",
    "gamma_2 = 10.0\n",
    "n_iter_per_epoch = int(len(dict_train) / n_batch)\n",
    "n_iter_per_epoch_val = int(len(dict_val) / n_batch)\n",
    "#n_iter_per_epoch = 10\n",
    "n_epochs = 10 #N of epochs\n",
    "reg_val = .0005\n",
    "# num_val_samples = 50\n",
    "num_val_samples = 500\n",
    "compute_bbox_correctness = True\n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating validation data\n",
    "def validate_referit(dict_test, txn, num_samples):\n",
    "    cnt_overall = 0\n",
    "    cnt_correct_w = 0\n",
    "    cnt_correct_hit_w = 0\n",
    "    cnt_correct_s = 0\n",
    "    cnt_correct_hit_s = 0\n",
    "    for k, doc_id in enumerate(dict_test):\n",
    "        if k > num_samples:\n",
    "            continue\n",
    "        imgbin = txn.get(doc_id.encode('utf-8'))\n",
    "        if imgbin==None:\n",
    "            print (\"Image not found\")\n",
    "            continue\n",
    "        buff = np.frombuffer(imgbin, dtype='uint8')\n",
    "        if len(buff) == 0:\n",
    "            print (\"Image not found\")\n",
    "            continue\n",
    "        imgbgr = cv2.imdecode(buff, cv2.IMREAD_COLOR)\n",
    "        imgrgb = imgbgr[:,:,[2,1,0]]\n",
    "\n",
    "        img = np.reshape(cv2.resize(imgrgb,(299,299)),(1,299,299,3))\n",
    "        orig_img_shape = dict_test[doc_id]['size'][:2]\n",
    "\n",
    "        for i, annot in enumerate(dict_test[doc_id]['annotations']):\n",
    "            if len(annot['bbox_norm'])== 0:\n",
    "                continue\n",
    "            if not check_percent(union(annot['bbox_norm'])):\n",
    "                continue\n",
    "            if any(b > 1 for b in annot['bbox_norm']):\n",
    "                continue\n",
    "            unq_qry = set(annot['query'])\n",
    "            sen_batch = [sen for sen in unq_qry if 0<len(sen.split())<=50] #only unique queries with 0<length<=50\n",
    "            img_batch = np.repeat(img,len(sen_batch),axis=0)\n",
    "            tensor_list = [heatmap_w, heatmap_s, R_i, R_s]\n",
    "            feed_dict = {input_img: img_batch, text_batch: sen_batch, mode: 'test'}\n",
    "            qry_heats, qry_heat, qry_scores, sen_score = sess.run(tensor_list, feed_dict)\n",
    "            # add length of unique queries\n",
    "            cnt_overall += len(sen_batch)\n",
    "            for c,sen in enumerate(sen_batch):\n",
    "                idx = [j for j in range(len(sen.split()))]\n",
    "                if np.mean(qry_scores[c,idx])==0:\n",
    "                    pred = {}\n",
    "                else:\n",
    "                    heatmap_wrd = np.average(qry_heats[c,idx,:], weights = qry_scores[c,idx], axis=0)\n",
    "                    heatmap_sen = qry_heat[c,:]\n",
    "                    bbox_c_w, hit_c_w = calc_correctness(annot, heatmap_wrd, orig_img_shape,\n",
    "                                                         compute_bbox_correctness=compute_bbox_correctness)\n",
    "                    bbox_c_s, hit_c_s = calc_correctness(annot, heatmap_sen, orig_img_shape,\n",
    "                                                         compute_bbox_correctness=compute_bbox_correctness)\n",
    "                    cnt_correct_w+=bbox_c_w\n",
    "                    cnt_correct_hit_w+=hit_c_w\n",
    "                    cnt_correct_s+=bbox_c_s\n",
    "                    cnt_correct_hit_s+=hit_c_s\n",
    "\n",
    "        var = [k,num_samples,cnt_correct_w/cnt_overall,cnt_correct_hit_w/cnt_overall]\n",
    "        var_s = [cnt_correct_s/cnt_overall,cnt_correct_hit_s/cnt_overall]\n",
    "        prnt0 = 'Sample {}/{}, IoU_acc_w:{:.2f}, IoU_acc_s:{:.2f}'.format(var[0],var[1],var[2],var_s[0])\n",
    "        prnt1 = ', Hit_acc_w:{:.2f}, Hit_acc_s:{:.2f} \\r'.format(var[3],var_s[1])\n",
    "        sys.stdout.write(prnt0+prnt1)                \n",
    "        sys.stdout.flush()\n",
    "\n",
    "    hit_acc_w = cnt_correct_hit_w/cnt_overall\n",
    "    iou_acc_w = cnt_correct_w/cnt_overall\n",
    "    hit_acc_s = cnt_correct_hit_s/cnt_overall\n",
    "    iou_acc_s = cnt_correct_s/cnt_overall\n",
    "    \n",
    "    return iou_acc_w,hit_acc_w,iou_acc_s,hit_acc_s\n",
    "\n",
    "# Make changes to load data into memory\n",
    "def fast_validate_referit(valid_ids_val, dict_val, val_images, num_samples):\n",
    "    cnt_overall = 0\n",
    "    cnt_correct_w = 0\n",
    "    cnt_correct_hit_w = 0\n",
    "    cnt_correct_s = 0\n",
    "    cnt_correct_hit_s = 0\n",
    "    for k, doc_id in enumerate(valid_ids_val):\n",
    "        if k > num_samples:\n",
    "            continue\n",
    "        img = np.reshape(val_images[doc_id],(1,299,299,3))\n",
    "        orig_img_shape = dict_val[doc_id]['size'][:2]\n",
    "\n",
    "        for i, annot in enumerate(dict_val[doc_id]['annotations']):\n",
    "            if len(annot['bbox_norm']) == 0:\n",
    "                continue\n",
    "            if not check_percent(union(annot['bbox_norm'])):\n",
    "                continue\n",
    "            if any(b > 1 for b in annot['bbox_norm']):\n",
    "                continue\n",
    "            unq_qry = set(annot['query'])\n",
    "            sen_batch = [sen for sen in unq_qry if 0 < len(sen.split()) <= 50]  # only unique queries with 0<length<=50\n",
    "            img_batch = np.repeat(img, len(sen_batch), axis=0)\n",
    "            tensor_list = [heatmap_w, heatmap_s, R_i, R_s]\n",
    "            feed_dict = {input_img: img_batch, text_batch: sen_batch, mode: 'test'}\n",
    "            qry_heats, qry_heat, qry_scores, sen_score = sess.run(tensor_list, feed_dict)\n",
    "            # add length of unique queries\n",
    "            cnt_overall += len(sen_batch)\n",
    "            for c, sen in enumerate(sen_batch):\n",
    "                idx = [j for j in range(len(sen.split()))]\n",
    "                if np.mean(qry_scores[c,idx]) == 0:\n",
    "                    pred = {}\n",
    "                else:\n",
    "                    heatmap_wrd = np.average(qry_heats[c,idx,:], weights = qry_scores[c,idx], axis=0)\n",
    "                    heatmap_sen = qry_heat[c,:]\n",
    "                    bbox_c_w, hit_c_w = calc_correctness(annot, heatmap_wrd, orig_img_shape,\n",
    "                                                         compute_bbox_correctness=compute_bbox_correctness)\n",
    "                    bbox_c_s, hit_c_s = calc_correctness(annot, heatmap_sen, orig_img_shape,\n",
    "                                                         compute_bbox_correctness=compute_bbox_correctness)\n",
    "                    cnt_correct_w += bbox_c_w\n",
    "                    cnt_correct_hit_w += hit_c_w\n",
    "                    cnt_correct_s += bbox_c_s\n",
    "                    cnt_correct_hit_s += hit_c_s\n",
    "\n",
    "        var = [k, num_samples, cnt_correct_w / cnt_overall, cnt_correct_hit_w / cnt_overall]\n",
    "        var_s = [cnt_correct_s / cnt_overall, cnt_correct_hit_s / cnt_overall]\n",
    "        prnt0 = 'Sample {}/{}, IoU_acc_w:{:.3f}, IoU_acc_s:{:.3f}'.format(var[0], var[1], var[2], var_s[0])\n",
    "        prnt1 = ', Hit_acc_w:{:.3f}, Hit_acc_s:{:.3f} \\r'.format(var[3], var_s[1])\n",
    "        sys.stdout.write(prnt0 + prnt1)                \n",
    "        sys.stdout.flush()\n",
    "\n",
    "    hit_acc_w = cnt_correct_hit_w / cnt_overall\n",
    "    iou_acc_w = cnt_correct_w / cnt_overall\n",
    "    hit_acc_s = cnt_correct_hit_s / cnt_overall\n",
    "    iou_acc_s = cnt_correct_s / cnt_overall\n",
    "    \n",
    "    return iou_acc_w, hit_acc_w, iou_acc_s, hit_acc_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating training batches.\n",
    "def batch_gen(ids, annot_dict, txn):\n",
    "    img_batch = np.empty((n_batch, 299, 299, 3), dtype='float32')\n",
    "    cap_batch = []\n",
    "    #currently, it takes negative samples randomly from \"all\" dataset\n",
    "    #it randomly picks any batch, so it doesn't have ending\n",
    "    seen = {}\n",
    "    # TODO(sonamg): Improve this code by identifying which images are available in lmdb by doing a pre-processing pass.\n",
    "    # Store that list of choice_ids in a file maybe and then randomly pick n_batch ids to generate a batch.\n",
    "    for i in range(n_batch):\n",
    "        choice_id = random.choice(ids)\n",
    "        while choice_id in seen: #we don't want to have repetitive img/caps in a batch \n",
    "            num_collisions += 1\n",
    "            choice_id = random.choice(ids)\n",
    "        imgbin = txn.get(choice_id.encode('utf-8'))\n",
    "        if imgbin != None:\n",
    "            buff = np.frombuffer(imgbin, dtype='uint8')\n",
    "        else:\n",
    "            buff = []\n",
    "        while choice_id in seen or len(buff)==0:\n",
    "            choice_id = random.choice(ids)\n",
    "            imgbin = txn.get(choice_id.encode('utf-8'))\n",
    "            if imgbin != None:\n",
    "                buff = np.frombuffer(imgbin, dtype='uint8')\n",
    "            else:\n",
    "                buff = []\n",
    "        seen[choice_id] = 1\n",
    "        \n",
    "        imgbgr = cv2.imdecode(buff, cv2.IMREAD_COLOR)\n",
    "        img = imgbgr[:,:,[2,1,0]]\n",
    "        img_batch[i,:,:,:] = cv2.resize(img,(299,299))\n",
    "        queries = [annot['query'] for annot in dict_train[choice_id]['annotations']]\n",
    "        sentence = random.choice(queries)\n",
    "        cap_batch.append(sentence)\n",
    "    return img_batch, cap_batch\n",
    "\n",
    "def fast_batch_gen(valid_ids, annot_dict, train_images):\n",
    "    img_batch = np.empty((n_batch, 299, 299, 3), dtype='float32')\n",
    "    cap_batch = []\n",
    "    chosen_ids = random.sample(valid_ids, n_batch)\n",
    "    for i, chosen_id in enumerate(chosen_ids):\n",
    "        imgbin = train_images[chosen_id]\n",
    "        img_batch[i,:,:,:] = imgbin\n",
    "        queries = [annot['query'] for annot in annot_dict[chosen_id]['annotations']]\n",
    "        sentence = random.choice(queries)\n",
    "        cap_batch.append(sentence)\n",
    "    return img_batch, cap_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attn_loss(e_w,v,e_s):\n",
    "    #e: ?xTxD, v: ?xNx4xD, e_bar: ?xD\n",
    "    with tf.variable_scope('attention_loss'):\n",
    "        ###word-level###\n",
    "        #heatmap\n",
    "        h = tf.nn.relu(tf.einsum('bij,cklj->bcikl',e_w,v)) #pair-wise ev^T: ?x?xTxNx4\n",
    "        #attention\n",
    "        a = tf.einsum('bcijl,cjlk->bcikl',h,v) #?x?xTxDx4 attnded visual reps for each of T words for all pairs\n",
    "        #pair-wise score\n",
    "        a_norm = tf.nn.l2_normalize(a,axis=3)\n",
    "        e_w_norm = tf.nn.l2_normalize(e_w,axis=2)\n",
    "        R_ik = tf.einsum('bcilk,bil->bcik',a_norm,e_w_norm) #cosine for T (words,img_reps) for all pairs\n",
    "        R_i = tf.reduce_max(R_ik,axis=-1) #?x?xT\n",
    "        R = tf.log(tf.pow(tf.reduce_sum(tf.exp(gamma_1*R_i),axis=2),1/gamma_1)) #?x? cap-img pairs\n",
    "        #posterior probabilities\n",
    "        P_DQ = tf.diag_part(tf.nn.softmax(gamma_2*R,axis=0)) #P(cap match img)\n",
    "        P_QD = tf.diag_part(tf.nn.softmax(gamma_2*R,axis=1)) #p(img match cap)\n",
    "        #losses\n",
    "        L1_w = -tf.reduce_mean(tf.log(P_DQ))\n",
    "        L2_w = -tf.reduce_mean(tf.log(P_QD))\n",
    "        \n",
    "        ###sentence-level###\n",
    "        #heatmap\n",
    "        h_s = tf.nn.relu(tf.einsum('bj,cklj->bckl',e_s,v)) #pair-wise e_bar*v^T: ?x?xNx4\n",
    "        #attention\n",
    "        a_s = tf.einsum('bcjk,cjkl->bclk',h_s,v) #?x?xDx4 attnded visual reps for sen. for all pairs\n",
    "        #pair-wise score\n",
    "        a_s_norm = tf.nn.l2_normalize(a_s,axis=2)\n",
    "        e_s_norm = tf.nn.l2_normalize(e_s,axis=1)\n",
    "        R_sk = tf.einsum('bclk,bl->bck',a_s_norm,e_s_norm) #cosine for (sen,img_reps) for all pairs\n",
    "        R_s = tf.reduce_max(R_sk,axis=-1) #?x?\n",
    "        #posterior probabilities\n",
    "        P_DQ_s = tf.diag_part(tf.nn.softmax(gamma_2*R_s,axis=0)) #P(cap match img)\n",
    "        P_QD_s = tf.diag_part(tf.nn.softmax(gamma_2*R_s,axis=1)) #P(img match cap)\n",
    "        #losses\n",
    "        L1_s = -tf.reduce_mean(tf.log(P_DQ_s))\n",
    "        L2_s = -tf.reduce_mean(tf.log(P_QD_s))\n",
    "        #overall loss\n",
    "        loss = L1_w + L2_w + L1_s + L2_s\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "def attn(e_w,v,e_s):\n",
    "    ## Inputs: local and global cap and img features ##\n",
    "    ## Output: Heatmap for each word, Global Heatmap, Attnded Vis features, Corr-vals\n",
    "    #e: ?xTxD, v: ?xNx4xD, e_bar: ?xD\n",
    "    with tf.variable_scope('attention'):\n",
    "        ###word-level###\n",
    "        #heatmap pool\n",
    "        h = tf.nn.relu(tf.einsum('bij,bklj->bikl',e_w,v)) #pair-wise ev^T: ?xTxNx4\n",
    "        #attention\n",
    "        a = tf.einsum('bijk,bjkl->bilk',h,v) #?xTxDx4 attnded visual reps for each of T words\n",
    "        #pair-wise score\n",
    "        a_norm = tf.nn.l2_normalize(a,axis=2)\n",
    "        e_w_norm = tf.nn.l2_normalize(e_w,axis=2)\n",
    "        R_ik = tf.einsum('bilk,bil->bik',a_norm,e_w_norm) #cosine for T (words,img_reps) for all pairs\n",
    "        R_ik = tf.identity(R_ik,name='level_score_word')\n",
    "        R_i = tf.reduce_max(R_ik,axis=-1,name='score_word') #?xT\n",
    "        #R = tf.log(tf.pow(tf.reduce_sum(tf.exp(gamma_1*R_i),axis=1),1/gamma_1)) #? corrs\n",
    "        #heatmap\n",
    "        idx_i = tf.argmax(R_ik,axis=-1,name='level_index_word') #?xT index of the featuremap which maximizes R_i\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.histogram('histogram_w', idx_i)\n",
    "        ii,jj = tf.meshgrid(tf.range(tf.shape(idx_i)[0]),tf.range(tf.shape(idx_i)[1]),indexing='ij')\n",
    "        ii = tf.cast(ii,tf.int64)\n",
    "        jj = tf.cast(jj,tf.int64)\n",
    "        batch_idx_i = tf.stack([tf.reshape(ii,(-1,)),\n",
    "                                tf.reshape(jj,(-1,)),\n",
    "                                tf.reshape(idx_i,(-1,))],axis=1) #?Tx3 indices of argmax\n",
    "        N0=int(np.sqrt(h.get_shape().as_list()[2]))\n",
    "        h_max = tf.gather_nd(tf.transpose(h,[0,1,3,2]),batch_idx_i) #?TxN retrieving max heatmaps\n",
    "        heatmap_wd = tf.reshape(h_max,[tf.shape(h)[0],tf.shape(h)[1],N0,N0],name='heatmap_word')\n",
    "        heatmap_wd_l = tf.reshape(h,[tf.shape(h)[0],tf.shape(h)[1],N0,N0,tf.shape(h)[3]],name='level_heatmap_word')\n",
    "        \n",
    "        ###sentence-level###\n",
    "        #heatmap pool\n",
    "        h_s = tf.nn.relu(tf.einsum('bj,blkj->blk',e_s,v)) #pair-wise e_bar*v^T: ?xNx4\n",
    "        #attention\n",
    "        a_s = tf.einsum('bjk,bjki->bik',h_s,v) #?xDx4 attnded visual reps for sen.\n",
    "        #pair-wise score\n",
    "        a_s_norm = tf.nn.l2_normalize(a_s,axis=1)\n",
    "        e_s_norm = tf.nn.l2_normalize(e_s,axis=1)\n",
    "        R_sk = tf.einsum('bik,bi->bk',a_s_norm,e_s_norm) #cosine for (sen,img_reps)\n",
    "        R_sk = tf.identity(R_sk,name='level_score_sentence')\n",
    "        R_s = tf.reduce_mean(R_sk,axis=-1,name='score_sentence') #?\n",
    "        #heatmap\n",
    "        idx_k = tf.argmax(R_sk,axis=-1,name='level_index_sentence') #? index of the featuremap which maximizes R_i\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.histogram('histogram_s', idx_k)\n",
    "        ii_k = tf.cast(tf.range(tf.shape(idx_k)[0]),dtype='int64')\n",
    "        batch_idx_k = tf.stack([ii_k,idx_k],axis=1)\n",
    "        N0_g=int(np.sqrt(h_s.get_shape().as_list()[1]))\n",
    "        h_s_max = tf.gather_nd(tf.transpose(h_s,[0,2,1]),batch_idx_k) #?xN retrieving max heatmaps\n",
    "        heatmap_sd = tf.reshape(h_s_max,[-1,N0_g,N0_g],name='heatmap_sentence')\n",
    "        heatmap_sd_l = tf.reshape(h_s,[-1,N0_g,N0_g,tf.shape(h)[3]],name='level_heatmap_sentence')\n",
    "        \n",
    "    return heatmap_wd, heatmap_sd, R_i, R_s  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_1by1_conv(feat_map,n_layers,n_filters,name,regularizer):\n",
    "    with tf.variable_scope(name+'_postConv'):\n",
    "        for i in range(n_layers):\n",
    "            with tf.variable_scope(name+'_stage_'+str(i)):\n",
    "                feat_map = tf.layers.conv2d(feat_map,filters=n_filters[i],kernel_size=[1,1],kernel_regularizer=regularizer)\n",
    "                feat_map = tf.nn.leaky_relu(feat_map,alpha=.25)\n",
    "    return feat_map\n",
    "\n",
    "def add_3by3_conv(feat_map,n_layers,n_filters,name,regularizer):\n",
    "    with tf.variable_scope(name+'_postConv'):\n",
    "        for i in range(n_layers):\n",
    "            with tf.variable_scope(name+'_stage_'+str(i)):\n",
    "                feat_map = tf.layers.conv2d(feat_map, filters=n_filters[i], kernel_size=[3,3],\n",
    "                                            kernel_regularizer=regularizer, padding='same')\n",
    "                feat_map = tf.nn.leaky_relu(feat_map, alpha=.25)\n",
    "    return feat_map\n",
    "\n",
    "def depth_selection(model):\n",
    "    with tf.variable_scope('stack_v'):\n",
    "        v1 = tf.identity(model['vgg_16/conv5/conv5_1'],name='v1')\n",
    "        v1 = add_1by1_conv(v1,n_layers=3,n_filters=[1024,1024,1024],name='v1',regularizer=regularizer)\n",
    "        size = v1.get_shape().as_list()[1:3]\n",
    "        resize_method = tf.image.ResizeMethod.BILINEAR\n",
    "        v2 = tf.identity(model['vgg_16/conv5/conv5_3'],name='v2')\n",
    "        #v2 = tf.image.resize_images(v2, size, method=resize_method)\n",
    "        v2 = add_1by1_conv(v2,n_layers=3,n_filters=[1024,1024,1024],name='v2',regularizer=regularizer)\n",
    "        v3 = tf.identity(model['vgg_16/conv4/conv4_1'],name='v3')\n",
    "        v3 = tf.image.resize_images(v3, size, method=resize_method)\n",
    "        v3 = add_1by1_conv(v3,n_layers=3,n_filters=[1024,1024,1024],name='v3',regularizer=regularizer)\n",
    "        v4 = tf.identity(model['vgg_16/conv4/conv4_3'],name='v4')\n",
    "        v4 = tf.image.resize_images(v4, size, method=resize_method)\n",
    "        v4 = add_1by1_conv(v4,n_layers=3,n_filters=[1024,1024,1024],name='v4',regularizer=regularizer)\n",
    "        v_all = tf.stack([v1,v2,v3,v4], axis=3)\n",
    "        v_all = tf.reshape(v_all,[-1,v_all.shape[1]*v_all.shape[2],v_all.shape[3],v_all.shape[4]])\n",
    "        v_all = tf.nn.l2_normalize(v_all, axis=-1, name='stacked_image_feature_maps')\n",
    "    return v_all\n",
    "\n",
    "def depth_selection_with_maxpool_pnasnet(model, conv_kernel_size=3, n_layers = 3):\n",
    "    conv_method = None\n",
    "    if conv_kernel_size == 1:\n",
    "        conv_method = add_1by1_conv\n",
    "    elif conv_kernel_size == 3:\n",
    "        conv_method = add_3by3_conv\n",
    "    else:\n",
    "        raise ValueError('Invalid conv_kernel_size parameter. Should be either 1 or 3.')\n",
    "    with tf.variable_scope('stack_v'):\n",
    "        v1 = tf.identity(model['Cell_5'],name='v1')\n",
    "        v1 = conv_method(v1,n_layers,n_filters=[1024,1024,1024],name='v1',regularizer=regularizer)\n",
    "        size = v1.get_shape().as_list()[1:3]\n",
    "        resize_method = tf.image.ResizeMethod.BILINEAR\n",
    "        v2 = tf.identity(model['Cell_7'],name='v2')\n",
    "        #v2 = tf.image.resize_images(v2, size, method=resize_method)\n",
    "        v2 = conv_method(v2,n_layers,n_filters=[1024,1024,1024],name='v2',regularizer=regularizer)\n",
    "        v3 = tf.identity(model['Cell_9'],name='v3')\n",
    "        v3 = tf.image.resize_images(v3, size, method=resize_method)\n",
    "        v3 = conv_method(v3,n_layers,n_filters=[1024,1024,1024],name='v3',regularizer=regularizer)\n",
    "        v4 = tf.identity(model['Cell_11'],name='v4')\n",
    "        v4 = tf.image.resize_images(v4, size, method=resize_method)\n",
    "        v4 = conv_method(v4,n_layers,n_filters=[1024,1024,1024],name='v4',regularizer=regularizer)\n",
    "        v_all = tf.stack([v1,v2,v3,v4], axis=3)\n",
    "        v_all = tf.reshape(v_all,[-1,v_all.shape[1]*v_all.shape[2],v_all.shape[3],v_all.shape[4]])\n",
    "        v_all = tf.nn.l2_normalize(v_all, axis=-1, name='stacked_image_feature_maps')\n",
    "    return v_all\n",
    "    \n",
    "\n",
    "def depth_selection_with_maxpool(model, conv_kernel_size=3,n_layers = 3):\n",
    "    conv_method = None\n",
    "    if conv_kernel_size == 1:\n",
    "        conv_method = add_1by1_conv\n",
    "    elif conv_kernel_size == 3:\n",
    "        conv_method = add_3by3_conv\n",
    "    else:\n",
    "        raise ValueError('Invalid conv_kernel_size parameter. Should be either 1 or 3.')\n",
    "    with tf.variable_scope('stack_v'):\n",
    "        v1 = tf.identity(model['vgg_16/conv5/conv5_1'],name='v1')\n",
    "        v1 = conv_method(v1, n_layers, n_filters=[1024,1024,1024], name='v1', regularizer=regularizer)\n",
    "        size = v1.get_shape().as_list()[1:3]\n",
    "        resize_method = tf.image.ResizeMethod.BILINEAR\n",
    "        v2 = tf.identity(model['vgg_16/conv5/conv5_3'],name='v2')\n",
    "        v2 = tf.image.resize_images(v2, size, method=resize_method)\n",
    "        v2 = conv_method(v2,n_layers,n_filters=[1024,1024,1024],name='v2',regularizer=regularizer)\n",
    "        v3 = tf.identity(model['vgg_16/conv4/conv4_1'],name='v3')\n",
    "        #new\n",
    "        #v3 = tf.layers.max_pooling2d(v3,[2,2],strides=2)\n",
    "        #old\n",
    "        v3 = tf.image.resize_images(v3, size, method=resize_method)\n",
    "        v3 = conv_method(v3,n_layers,n_filters=[1024,1024,1024],name='v3',regularizer=regularizer)\n",
    "        v4 = tf.identity(model['vgg_16/conv4/conv4_3'],name='v4')\n",
    "        v4 = tf.image.resize_images(v4, size, method=resize_method)\n",
    "        #new\n",
    "        #v4 = tf.layers.max_pooling2d(v4,[2,2],strides=2)\n",
    "        v4 = conv_method(v4,n_layers,n_filters=[1024,1024,1024],name='v4',regularizer=regularizer)\n",
    "        v_all = tf.stack([v1,v2,v3,v4], axis=3)\n",
    "        v_all = tf.reshape(v_all,[-1,v_all.shape[1]*v_all.shape[2],v_all.shape[3],v_all.shape[4]])\n",
    "        v_all = tf.nn.l2_normalize(v_all, axis=-1, name='stacked_image_feature_maps')\n",
    "    return v_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess1.close()\n",
    "sess1 = tf.Session()\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=False)\n",
    "elmo_embds = elmo([\"my, name is sonam\",\"rt trew wtr\",\"sonam is\",\"a b c d e f\"], signature=\"default\", as_dict=True)\n",
    "lstm1_embd = elmo_embds['lstm_outputs1'] #?xTXD\n",
    "lstm2_embd = elmo_embds['lstm_outputs2'] #?xTXD\n",
    "w_embd = tf.identity(elmo_embds['elmo'], name='elmo_word_embd') #?xTXD\n",
    "print(elmo_embds['sequence_len'])\n",
    "print(w_embd.shape)\n",
    "#sess1.run(w_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(w_embd[3][0:5][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(n_layers)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Graph building code\n",
    "try:\n",
    "    sess.close()\n",
    "except NameError:\n",
    "    print(\"Definining session for the first time\")\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "mode = tf.placeholder(tf.string, name='mode')\n",
    "isTraining = tf.equal(mode, 'train')\n",
    "regularizer = tf.contrib.layers.l2_regularizer(reg_val)\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    #building visual model\n",
    "    print('Building Visual Model...')\n",
    "    input_img = tf.placeholder(tf.float32, (None,299,299,3), name='input_img')\n",
    "    \n",
    "    # For VG Training\n",
    "    pre_processed_img = pre_process(input_img, 'vgg_preprocessing')\n",
    "    vis_model = pre_trained_load(model_name='vgg_16', image_shape=(None,299,299,3),\n",
    "                              input_tensor=pre_processed_img, session=sess, is_training=True, global_pool=True)\n",
    "\n",
    "    v = depth_selection_with_maxpool(vis_model, n_layers = n_layers) #(?,1225,4,1024)\n",
    "    \n",
    "    # For PNAS Net Training\n",
    "#     pre_processed_img = pre_process(input_img, 'inception_preprocessing')\n",
    "#     vis_model = pre_trained_load(model_name='pnasnet_large', image_shape=(None,299,299,3),\n",
    "#                                  input_tensor=pre_processed_img, session=sess, is_training=True, global_pool=True)\n",
    "#     v = depth_selection_with_maxpool_pnasnet(vis_model, n_layers = n_layers) #(?,1225,4,1024)\n",
    "\n",
    "    \n",
    "    #building text model\n",
    "    print('Building Text Model...')\n",
    "    #sentence placeholder - list of sentences\n",
    "    text_batch = tf.placeholder('string', shape=[None], name='text_input')\n",
    "    #loading pre-trained ELMo\n",
    "    elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "    #getting ELMo embeddings\n",
    "    print('Loading Embeddings')\n",
    "    elmo_embds = elmo(text_batch, signature=\"default\", as_dict=True)\n",
    "    print(elmo_embds['sequence_len'].shape)\n",
    "    lstm1_embd = elmo_embds['lstm_outputs1'] #?xTXD\n",
    "    lstm2_embd = elmo_embds['lstm_outputs2'] #?xTXD\n",
    "    w_embd = tf.identity(elmo_embds['elmo'], name='elmo_word_embd') #?xTXD\n",
    "    #taking index of last word in each sentence\n",
    "    idx = elmo_embds['sequence_len']-1\n",
    "    batch_idx = tf.stack([tf.range(0,tf.size(idx),1),idx],axis=1)\n",
    "    # Concatenate first of backward with last of forward to get sentence embeddings\n",
    "    print('Getting Sentence Embeddings')\n",
    "    dim = lstm1_embd.get_shape().as_list()[-1]\n",
    "    sen_embd_1 = tf.concat([lstm1_embd[:,0,int(dim/2):],\n",
    "                            tf.gather_nd(lstm1_embd[:,:,:int(dim/2)],batch_idx)], axis=-1) #[batch,dim]\n",
    "    sen_embd_2 = tf.concat([lstm2_embd[:,0,int(dim/2):],\n",
    "                            tf.gather_nd(lstm2_embd[:,:,:int(dim/2)],batch_idx)], axis=-1) #[batch,dim]\n",
    "    sen_embd = tf.concat([tf.expand_dims(sen_embd_1,axis=2),\n",
    "                               tf.expand_dims(sen_embd_2,axis=2)], axis=2, name='elmo_sen_embd') #[batch,dim,2]\n",
    "    e_s = tf.layers.dense(sen_embd,units=1,use_bias=False) #?xDx1\n",
    "    e_s = tf.squeeze(e_s,axis=2)\n",
    "    e_s = tf.layers.dense(e_s, units=1024)\n",
    "    e_s = tf.nn.leaky_relu(e_s,alpha=.25)\n",
    "    e_s = tf.layers.dense(e_s, units=1024)\n",
    "    e_s = tf.nn.leaky_relu(e_s,alpha=.25)\n",
    "    e_s = tf.nn.l2_normalize(e_s, axis=-1, name='sen_embedding')\n",
    "        \n",
    "    e_w = tf.layers.dense(w_embd, units=1024)\n",
    "    e_w = tf.nn.leaky_relu(e_w,alpha=.25)\n",
    "    e_w = tf.layers.dense(e_w, units=1024)\n",
    "    e_w = tf.nn.leaky_relu(e_w,alpha=.25)\n",
    "    e_w = tf.nn.l2_normalize(e_w, axis=-1, name='w_embedding')\n",
    "    \n",
    "    print('Generating Heatmaps')\n",
    "    heatmap_w,heatmap_s,R_i,R_s = attn(e_w,v,e_s)\n",
    "        \n",
    "loss = attn_loss(e_w,v,e_s) + tf.losses.get_regularization_loss()\n",
    "loss = tf.identity(loss, name='loss')\n",
    "tf.summary.scalar('loss_value', loss)\n",
    "        \n",
    "lr = tf.placeholder(tf.float32, shape=[], name='learning_rate')\n",
    "tf.summary.scalar('learning_rate', lr)\n",
    "opt = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "train_vars = list(set(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)) - set(vis_model.model_weights))\n",
    "train_op = opt.minimize(loss, var_list=train_vars, name='train_op')\n",
    "\n",
    "global_saver = tf.train.Saver()\n",
    "    \n",
    "train_writer = tf.summary.FileWriter('./logs/vgg/vg', sess.graph)\n",
    "merged = tf.summary.merge_all()\n",
    "print('Model is built')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "condition = 'Performance_AttnGrnd_PNAS_Depth_Max_VG_3by3_resize_nlayer' + str(n_layers)\n",
    "print('Initializing...')\n",
    "_ = sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#loading pretrained vgg weights\n",
    "print('Loading visual path model (vgg)...')\n",
    "vis_model.load_weights()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iter_per_epoch = 5\n",
    "n_iter_per_epoch_val = 2\n",
    "#n_iter_per_epoch = 10\n",
    "n_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loop on training data\n",
    "print('Start training...')\n",
    "iou_acc_w = np.zeros((n_epochs,))\n",
    "hit_acc_w = np.zeros((n_epochs,))\n",
    "iou_acc_s = np.zeros((n_epochs,))\n",
    "hit_acc_s = np.zeros((n_epochs,))\n",
    "train_loss = np.zeros((n_epochs,))\n",
    "lr_value_0 = .001\n",
    "max_val_iou = 0\n",
    "max_val_hit = 0\n",
    "for e in range(n_epochs):\n",
    "    print('\\n\\n=====Epoch: %d'%e)\n",
    "    avg_loss = 0\n",
    "    if e < 9:\n",
    "        lr_value = lr_value_0\n",
    "    elif 9 <= e < 14:\n",
    "        lr_value = lr_value_0 / 2.0\n",
    "    elif e >= 14:\n",
    "        lr_value = lr_value_0 / 4.0\n",
    "        \n",
    "    print('===Train')\n",
    "    for i in range(n_iter_per_epoch):\n",
    "        # img_batch, cap_batch = batch_gen(ids_train, dict_train, txn)\n",
    "        img_batch, cap_batch = fast_batch_gen(valid_ids_train, dict_train, train_images)\n",
    "        feed_dict = {input_img: img_batch, text_batch: cap_batch, mode: 'train', lr: lr_value}\n",
    "        summary, loss_val, _ = sess.run([merged, loss, train_op], feed_dict)\n",
    "        if i%100==0:\n",
    "            train_writer.add_summary(summary, n_iter_per_epoch*e + i)\n",
    "        avg_loss += loss_val\n",
    "        var = [i * n_batch, n_iter_per_epoch * n_batch, avg_loss / float(i + 1)]\n",
    "        prnt = 'Sample {}/{}, train_loss:{:.4f} \\r'.format(var[0], var[1], var[2])\n",
    "        sys.stdout.write(prnt)                \n",
    "        sys.stdout.flush()     \n",
    "    train_loss[e] = avg_loss/float(n_iter_per_epoch+1)\n",
    "    \n",
    "    #validation phase\n",
    "    print('\\n===Validation')\n",
    "    # iou_acc_w[e], hit_acc_w[e], iou_acc_s[e], hit_acc_s[e] = validate_referit(dict_val, ref_txn, num_val_samples)\n",
    "    iou_acc_w[e], hit_acc_w[e], iou_acc_s[e], hit_acc_s[e] = fast_validate_referit(valid_ids_val, dict_val, val_images, num_val_samples)\n",
    "    sv = 'Epoch:{}, Train_loss:{}, Val_iou_acc:{}, Val_hit_acc:{}\\r'.format(e, train_loss[e], iou_acc_w[e], hit_acc_w[e])\n",
    "    open('./logs/log_' + condition + '.txt', 'w').write(sv)\n",
    "    if hit_acc_s[e] > max_val_hit:\n",
    "        max_val_hit = hit_acc_s[e]\n",
    "        print('\\nHit accuracy improved. Saving best model...\\r')\n",
    "        global_saver.save(sess, './saved_models/model_' + condition + '_best_hit')\n",
    "#     if iou_acc_s[e] > max_val_iou:\n",
    "#         max_val_iou = iou_acc_s[e]\n",
    "#         print('\\nIoU accuracy improved. Saving best model...\\r')\n",
    "#         global_saver.save(sess, './saved_models/model_' + condition + '_best_iou')\n",
    "        \n",
    "print('\\n\\nTraining done.')\n",
    "#saving the session\n",
    "print('Saving model...')\n",
    "global_saver.save(sess, './saved_models/model_' + condition)\n",
    "with open('./logs/loss_' + condition + '.pickle', 'wb') as f:\n",
    "    pickle.dump({'train_loss':train_loss,\n",
    "                 'val_iou_w':iou_acc_w,\n",
    "                 'val_hit_w':hit_acc_w,\n",
    "                 'val_iou_s':iou_acc_s,\n",
    "                 'val_hit_s':hit_acc_s},\n",
    "                f,\n",
    "                protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('Saving done.')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_loss, label='Train loss '+ condition)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(100. * iou_acc_w, label='Validation iou_acc_w '+ condition)\n",
    "plt.plot(100. * hit_acc_w, label='Validation hit_acc_w '+ condition)\n",
    "plt.plot(100. * iou_acc_s, label='Validation iou_acc_s '+ condition)\n",
    "plt.plot(100. * hit_acc_s, label='Validation hit_acc_s '+ condition)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global_saver.restore(sess, './saved_models/model_' + condition + '_best_hit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReferIt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_dict_path = ref_path + 'test.pickle' \n",
    "with open(ref_dict_path, 'rb') as f:\n",
    "    dict_test = pickle.load(f, encoding='latin1')\n",
    "    ids_test = list(dict_test.keys())\n",
    "    \n",
    "valid_ids_test, test_images = preprocess_images(ids_test, ref_txn)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_test_samples = len(dict_test)\n",
    "# iou_acc_w_test,hit_acc_w_test,iou_acc_s_test,hit_acc_s_test=validate_referit(dict_test, ref_txn, num_test_samples)\n",
    "iou_acc_w_test, hit_acc_w_test, iou_acc_s_test, hit_acc_s_test = fast_validate_referit(\n",
    "    valid_ids_test, dict_test, test_images, num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./evaluations/referit/'+condition+'.pickle', 'wb') as f:\n",
    "    pickle.dump({'iou_acc_w':iou_acc_w_test,\n",
    "                 'hit_acc_w':hit_acc_w_test,\n",
    "                 'iou_acc_s':iou_acc_s_test,\n",
    "                 'hit_acc_s':hit_acc_s_test},f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'iou_acc_w':iou_acc_w_test,\n",
    "                 'hit_acc_w':hit_acc_w_test,\n",
    "                 'iou_acc_s':iou_acc_s_test,\n",
    "                 'hit_acc_s':hit_acc_s_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading vg data\n",
    "with open(dict_paths['test'], 'rb') as f:\n",
    "    dict_test = pickle.load(f, encoding='latin1')\n",
    "    vg_ids_test = list(dict_test.keys())\n",
    "    \n",
    "num_test_samples = len(dict_test)\n",
    "valid_vg_ids_test, test_images = preprocess_images(vg_ids_test, txn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fast_validate_vg(ids_test, dict_test, test_images, num_test_samples):\n",
    "    print('start validating')\n",
    "    cnt_overall = 0\n",
    "    cnt_correct_w = 0\n",
    "    cnt_correct_hit_w = 0\n",
    "    cnt_correct_s = 0\n",
    "    cnt_correct_hit_s = 0\n",
    "    for k, doc_id in enumerate(ids_test):\n",
    "        if k > num_test_samples:\n",
    "            continue\n",
    "        img = np.reshape(test_images[doc_id],(1,299,299,3))\n",
    "        orig_img_shape = dict_test[doc_id]['size'][:2]\n",
    "        \n",
    "        sen_batch = []\n",
    "        annot_batch = []\n",
    "        #create batch of queries in a doc_id annotation\n",
    "        for i, annot in enumerate(dict_test[doc_id]['annotations']):\n",
    "            sen = annot['query']\n",
    "            if len(sen.split()) == 0 or len(annot['bbox_norm']) == 0:\n",
    "                continue\n",
    "            if not check_percent(union(annot['bbox_norm'])):\n",
    "                continue\n",
    "            if any(b > 1 for b in annot['bbox_norm']):\n",
    "                continue\n",
    "            sen_batch.append(sen)\n",
    "            annot_batch.append(annot)\n",
    "        if len(sen_batch) == 0:\n",
    "            continue\n",
    "        cnt_overall += len(sen_batch)\n",
    "\n",
    "        img_batch = np.repeat(img, len(sen_batch), axis=0)\n",
    "        tensor_list = [heatmap_w, heatmap_s, R_i, R_s]\n",
    "        feed_dict = {input_img: img_batch, text_batch: sen_batch, mode: 'test'}\n",
    "        qry_heats, qry_heat, qry_scores, sen_score = sess.run(tensor_list, feed_dict)\n",
    "        \n",
    "        for c, sen in enumerate(sen_batch):\n",
    "            idx = [j for j in range(len(sen.split()))]\n",
    "            if np.mean(qry_scores[c, idx]) == 0:\n",
    "                pred = {}\n",
    "            else:\n",
    "                heatmap_wrd = np.average(qry_heats[c,idx,:], weights = qry_scores[c,idx], axis=0)\n",
    "                heatmap_sen = qry_heat[c,:]\n",
    "                bbox_c_w, hit_c_w = calc_correctness(annot_batch[c], heatmap_wrd, orig_img_shape,\n",
    "                                                     compute_bbox_correctness=False)\n",
    "                bbox_c_s, hit_c_s = calc_correctness(annot_batch[c], heatmap_sen, orig_img_shape,\n",
    "                                                     compute_bbox_correctness=False)\n",
    "                cnt_correct_w += bbox_c_w\n",
    "                cnt_correct_hit_w += hit_c_w\n",
    "                cnt_correct_s += bbox_c_s\n",
    "                cnt_correct_hit_s += hit_c_s\n",
    "\n",
    "        var = [k, num_test_samples, cnt_correct_w / cnt_overall, cnt_correct_hit_w / cnt_overall]\n",
    "        var_s = [cnt_correct_s / cnt_overall, cnt_correct_hit_s / cnt_overall]\n",
    "        prnt0 = 'Sample {}/{}, IoU_acc_w:{:.4f}, IoU_acc_s:{:.4f}'.format(var[0], var[1], var[2], var_s[0])\n",
    "        prnt1 = ', Hit_acc_w:{:.4f}, Hit_acc_s:{:.4f} \\r'.format(var[3], var_s[1])\n",
    "        sys.stdout.write(prnt0 + prnt1)                \n",
    "        sys.stdout.flush()\n",
    "\n",
    "    hit_acc_w = cnt_correct_hit_w / cnt_overall\n",
    "    iou_acc_w = cnt_correct_w / cnt_overall\n",
    "    hit_acc_s = cnt_correct_hit_s / cnt_overall\n",
    "    iou_acc_s = cnt_correct_s / cnt_overall\n",
    "    print('end validating')\n",
    "    \n",
    "    return iou_acc_w,hit_acc_w,iou_acc_s,hit_acc_s\n",
    "\n",
    "def validate_vg(dict_test, txn):\n",
    "    cnt_overall = 0\n",
    "    cnt_correct_w = 0\n",
    "    cnt_correct_hit_w = 0\n",
    "    cnt_correct_s = 0\n",
    "    cnt_correct_hit_s = 0\n",
    "    for k, doc_id in enumerate(dict_test):\n",
    "        if k > num_tst:\n",
    "            continue\n",
    "        imgbin = txn.get(doc_id.encode('utf-8'))\n",
    "        if imgbin==None:\n",
    "            continue\n",
    "        buff = np.frombuffer(imgbin, dtype='uint8')\n",
    "        if len(buff) == 0:\n",
    "            continue\n",
    "        imgbgr = cv2.imdecode(buff, cv2.IMREAD_COLOR)\n",
    "        imgrgb = imgbgr[:,:,[2,1,0]]\n",
    "\n",
    "        img = np.reshape(cv2.resize(imgrgb,(299,299)),(1,299,299,3))\n",
    "        orig_img_shape = dict_test[doc_id]['size'][:2]\n",
    "        \n",
    "        sen_batch = []\n",
    "        annot_batch = []\n",
    "        #create batch of queries in a doc_id annotation\n",
    "        for i, annot in enumerate(dict_test[doc_id]['annotations']):\n",
    "            sen = annot['query']\n",
    "            if len(sen.split())==0 or len(annot['bbox_norm'])== 0:\n",
    "                continue\n",
    "            if not check_percent(union(annot['bbox_norm'])):\n",
    "                continue\n",
    "            if any(b > 1 for b in annot['bbox_norm']):\n",
    "                continue\n",
    "            sen_batch.append(sen)\n",
    "            annot_batch.append(annot)\n",
    "        if len(sen_batch) == 0:\n",
    "            continue\n",
    "        cnt_overall += len(sen_batch)\n",
    "\n",
    "        img_batch = np.repeat(img, len(sen_batch), axis=0)\n",
    "        tensor_list = [heatmap_w, heatmap_s, R_i, R_s]\n",
    "        feed_dict = {input_img: img_batch, text_batch: sen_batch, mode: 'test'}\n",
    "        qry_heats, qry_heat, qry_scores, sen_score = sess.run(tensor_list, feed_dict)\n",
    "        \n",
    "        for c, sen in enumerate(sen_batch):\n",
    "            idx = [j for j in range(len(sen.split()))]\n",
    "            if np.mean(qry_scores[c, idx]) == 0:\n",
    "                pred = {}\n",
    "            else:\n",
    "                heatmap_wrd = np.average(qry_heats[c,idx,:], weights = qry_scores[c,idx], axis=0)\n",
    "                heatmap_sen = qry_heat[c,:]\n",
    "                bbox_c_w, hit_c_w = calc_correctness(annot_batch[c], heatmap_wrd, orig_img_shape)\n",
    "                bbox_c_s, hit_c_s = calc_correctness(annot_batch[c], heatmap_sen, orig_img_shape)\n",
    "                cnt_correct_w += bbox_c_w\n",
    "                cnt_correct_hit_w += hit_c_w\n",
    "                cnt_correct_s += bbox_c_s\n",
    "                cnt_correct_hit_s += hit_c_s\n",
    "\n",
    "        var = [k, num_tst, cnt_correct_w / cnt_overall, cnt_correct_hit_w / cnt_overall]\n",
    "        var_s = [cnt_correct_s / cnt_overall, cnt_correct_hit_s / cnt_overall]\n",
    "        prnt0 = 'Sample {}/{}, IoU_acc_w:{:.2f}, IoU_acc_s:{:.2f}'.format(var[0], var[1], var[2], var_s[0])\n",
    "        prnt1 = ', Hit_acc_w:{:.2f}, Hit_acc_s:{:.2f} \\r'.format(var[3], var_s[1])\n",
    "        sys.stdout.write(prnt0 + prnt1)                \n",
    "        sys.stdout.flush()\n",
    "\n",
    "    hit_acc_w = cnt_correct_hit_w / cnt_overall\n",
    "    iou_acc_w = cnt_correct_w / cnt_overall\n",
    "    hit_acc_s = cnt_correct_hit_s / cnt_overall\n",
    "    iou_acc_s = cnt_correct_s / cnt_overall\n",
    "    \n",
    "    return iou_acc_w,hit_acc_w,iou_acc_s,hit_acc_s\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iou_acc_w_test,hit_acc_w_test,iou_acc_s_test,hit_acc_s_test=validate_vg(dict_test,vg_txn)\n",
    "iou_acc_w_test_vg, hit_acc_w_test_vg, iou_acc_s_test_vg, hit_acc_s_test_vg = fast_validate_vg(\n",
    "    valid_vg_ids_test, dict_test, test_images, num_test_samples)\n",
    "with open('./evaluations/vg/'+condition+'.pickle', 'wb') as f:\n",
    "    pickle.dump({'iou_acc_w':iou_acc_w_test_vg,\n",
    "                 'hit_acc_w':hit_acc_w_test_vg,\n",
    "                 'iou_acc_s':iou_acc_s_test_vg,\n",
    "                 'hit_acc_s':hit_acc_s_test_vg},f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'iou_acc_w':iou_acc_w_test_vg,\n",
    "                 'hit_acc_w':hit_acc_w_test_vg,\n",
    "                 'iou_acc_s':iou_acc_s_test_vg,\n",
    "                 'hit_acc_s':hit_acc_s_test_vg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
