{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQA Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code of this file comes from: https://github.com/cvlab-tohoku/Dense-CoAttention-Network\n",
    "Before run this file, please run the file preprocess/preprocess.ipynb to genertate the following file:\n",
    "- mscoco_train.json\n",
    "- mscoco_val.json\n",
    "- mscoco_trainval.json\n",
    "- mscoco_testdev.json\n",
    "- mscoco_test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "from data_util import *\n",
    "import json\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map_vqa = {\n",
    "\t\"train\": \"mscoco_train.json\",\n",
    "\t\"val\": \"mscoco_val.json\",\n",
    "\t\"trainval\": \"mscoco_trainval.json\",\n",
    "\t\"testdev\": \"mscoco_testdev.json\",\n",
    "\t\"test\": \"mscoco_test.json\",\n",
    "\t\"train_comp_path\": \"/Users/david/desktop/CV_research/VQA/pairs/v2_mscoco_train2014_complementary_pairs.json\",\n",
    "\t\"val_comp_path\": \"/Users/david/desktop/CV_research/VQA/pairs/v2_mscoco_val2014_complementary_pairs.json\",\n",
    "}\n",
    "\n",
    "data_path = './dataset'\n",
    "num_occurs = 8\n",
    "glove_path = '/Users/david/desktop/CV_research/VQA/dataset/glove_840B.pt'\n",
    "max_ques = 14\n",
    "max_ans = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Trainval Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process train dataset...\n",
      "Number of unique answers: 22531\n",
      "Total number of answers: 443757\n",
      "Top 2412 answers account for 92.117307%\n",
      "Sample frequent answers:\n",
      "('net', 33)\n",
      "('pitcher', 92)\n",
      "('orange', 1425)\n",
      "('yes', 84978)\n",
      "('white', 8916)\n",
      "('skiing', 866)\n",
      "('red', 5201)\n",
      "('frisbee', 1641)\n",
      "('brushing teeth', 124)\n",
      "('no', 82516)\n",
      "('black and white', 766)\n",
      "('skateboard', 701)\n",
      "('1', 12540)\n",
      "('blue', 5455)\n",
      "('green', 3750)\n",
      "('motorcycle', 490)\n",
      "('gray', 2113)\n",
      "('2', 12215)\n",
      "('purse', 84)\n",
      "('skis', 292)\n",
      "Tokenizing questions and answers...\n",
      "[['what', 'is', 'this', 'photo', 'taken', 'looking', 'through', '?']]\n",
      "[[(['net'], 1)]]\n",
      "[['what', 'position', 'is', 'this', 'man', 'playing', '?']]\n",
      "[[(['pitcher'], 1), (['catcher'], 0.3)]]\n",
      "[['what', 'color', 'is', 'the', 'players', 'shirt', '?']]\n",
      "[[(['orange'], 1)]]\n",
      "[['is', 'this', 'man', 'a', 'professional', 'baseball', 'player', '?']]\n",
      "[[(['yes'], 1), (['no'], 0.3)]]\n",
      "[['what', 'color', 'is', 'the', 'snow', '?']]\n",
      "[[(['white'], 1)]]\n",
      "gloves type: <class 'dict'>9.15% done)\t\n",
      "Most frequent words in the dataset:\n",
      "(443931, '?')\n",
      "(322836, 'the')\n",
      "(277695, 'is')\n",
      "(185348, 'what')\n",
      "(124813, 'yes')\n",
      "(123513, 'no')\n",
      "(104165, 'are')\n",
      "(87771, 'this')\n",
      "(71073, 'in')\n",
      "(70111, 'on')\n",
      "(59103, 'of')\n",
      "(57974, 'a')\n",
      "(53768, 'how')\n",
      "(51619, 'many')\n",
      "(45498, 'color')\n",
      "(37866, 'there')\n",
      "(28006, 'white')\n",
      "(27863, 'man')\n",
      "(22460, 'does')\n",
      "(22343, '2')\n",
      "Total number of words: 4210381\n",
      "Number of unique words in dataset: 13913\n",
      "Number of words in GloVe: 2196018\n",
      "Number of unique words in unk: 241/13913 = 1.73%\n",
      "Total number of unk words: 401/4210381 = 0.01%\n",
      "Max length question: 25\n",
      "Length distribution of questions (length, count):\n",
      " 0:          0 \t 0.000000%\n",
      " 1:          0 \t 0.000000%\n",
      " 2:          0 \t 0.000000%\n",
      " 3:         17 \t 0.003831%\n",
      " 4:      11791 \t 2.657085%\n",
      " 5:      58943 \t 13.282720%\n",
      " 6:     114340 \t 25.766354%\n",
      " 7:      88154 \t 19.865377%\n",
      " 8:      74504 \t 16.789369%\n",
      " 9:      45991 \t 10.364006%\n",
      "10:      22717 \t 5.119243%\n",
      "11:      12086 \t 2.723563%\n",
      "12:       6901 \t 1.555130%\n",
      "13:       3725 \t 0.839423%\n",
      "14:       1904 \t 0.429064%\n",
      "15:       1129 \t 0.254419%\n",
      "16:        666 \t 0.150082%\n",
      "17:        348 \t 0.078421%\n",
      "18:        245 \t 0.055210%\n",
      "19:        137 \t 0.030873%\n",
      "20:         90 \t 0.020281%\n",
      "21:         46 \t 0.010366%\n",
      "22:          9 \t 0.002028%\n",
      "23:         13 \t 0.002930%\n",
      "24:          0 \t 0.000000%\n",
      "25:          1 \t 0.000225%\n",
      "Max length answer: 6\n",
      "Length distribution of answers (length, count):\n",
      " 0:          0 \t 0.000000%\n",
      " 1:     840646 \t 92.898197%\n",
      " 2:      41509 \t 4.587081%\n",
      " 3:      21374 \t 2.362000%\n",
      " 4:       1215 \t 0.134267%\n",
      " 5:         83 \t 0.009172%\n",
      " 6:         84 \t 0.009283%\n",
      "Process val dataset...\n",
      "Tokenizing questions and answers...\n",
      "[['where', 'is', 'he', 'looking', '?']]\n",
      "[[(['down'], 1), (['skateboard'], 0.3), (['table'], 0.3)]]\n",
      "[['what', 'are', 'the', 'people', 'in', 'the', 'background', 'doing', '?']]\n",
      "[[(['watching'], 1)]]\n",
      "[['what', 'is', 'he', 'on', 'top', 'of', '?']]\n",
      "[[(['table'], 0.9), (['skateboard'], 0.3)]]\n",
      "[['what', 'website', 'copyrighted', 'the', 'picture', '?']]\n",
      "[[]]\n",
      "[['is', 'this', 'a', 'creamy', 'soup', '?']]\n",
      "[[(['no'], 1)]]\n",
      "processing 210000/214354 (97.97% done)\t\r"
     ]
    }
   ],
   "source": [
    "print(\"Process train dataset...\")\n",
    "train_set = json.load(open(os.path.join(data_path, data_map_vqa[\"train\"]), \"r\"))\n",
    "comp_train = json.load(open(data_map_vqa[\"train_comp_path\"], \"r\"))\n",
    "\n",
    "ans2idx, idx2ans, word2idx, idx2word, train_set, max_len_ques, poss_answers = process_dataset(train_set, num_occurs, glove_path, max_ques, max_ans)\n",
    "\n",
    "print(\"Process val dataset...\")\n",
    "val_set = json.load(open(os.path.join(data_path, data_map_vqa[\"val\"]), \"r\"))\n",
    "comp_val = json.load(open(data_map_vqa[\"val_comp_path\"], \"r\"))\n",
    "\n",
    "val_set = filter_answers(val_set, ans2idx)\n",
    "val_set = process_text(val_set)\n",
    "val_set = filter_unk_word(val_set, word2idx)\n",
    "val_set = encode_ans(val_set, ans2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_set\n",
    "train_dict = dict()\n",
    "for data in dataset:\n",
    "    ques_dict = dict()\n",
    "    ques_dict[data['ques_id'][0]] = {\"question\": \" \".join(data[\"processed_ques\"][0][0:14]), \\\n",
    "                                    \"answer\": data[\"processed_ans\"][0],\\\n",
    "                                    \"answer_id\": data['ans_id'][0]}\n",
    "    \n",
    "    if str(data['id']) in train_dict:\n",
    "        train_dict[str(data['id'])].append(ques_dict)\n",
    "    if str(data['id']) not in train_dict:\n",
    "        train_dict[str(data['id'])] = [ques_dict]\n",
    "        \n",
    "with open(\"./VQA_pickles/train.pickle\",\"wb\") as file:\n",
    "    pickle.dump(train_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = val_set\n",
    "val_dict = dict()\n",
    "for data in dataset:\n",
    "    ques_dict = dict()\n",
    "    ques_dict[data['ques_id'][0]] = {\"question\": \" \".join(data[\"processed_ques\"][0][0:14]), \\\n",
    "                                    \"answer\": data[\"processed_ans\"][0],\\\n",
    "                                    \"answer_id\": data['ans_id'][0]}\n",
    "    \n",
    "    if str(data['id']) in val_dict:\n",
    "        val_dict[str(data['id'])].append(ques_dict)\n",
    "    if str(data['id']) not in val_dict:\n",
    "        val_dict[str(data['id'])] = [ques_dict]\n",
    "        \n",
    "with open(\"./VQA_pickles/val.pickle\",\"wb\") as file:\n",
    "    pickle.dump(val_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Trainval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process trainval dataset...\n",
      "Number of unique answers: 29332\n",
      "Total number of answers: 658111\n",
      "Top 3133 answers account for 93.132921%\n",
      "Sample frequent answers:\n",
      "('net', 51)\n",
      "('pitcher', 123)\n",
      "('orange', 2211)\n",
      "('yes', 125706)\n",
      "('white', 13227)\n",
      "('skiing', 1265)\n",
      "('red', 7684)\n",
      "('frisbee', 2397)\n",
      "('brushing teeth', 170)\n",
      "('no', 122598)\n",
      "('black and white', 1220)\n",
      "('skateboard', 1005)\n",
      "('1', 18633)\n",
      "('blue', 8188)\n",
      "('green', 5736)\n",
      "('motorcycle', 742)\n",
      "('gray', 3213)\n",
      "('2', 17960)\n",
      "('purse', 117)\n",
      "('skis', 410)\n",
      "Tokenizing questions and answers...\n",
      "[['what', 'is', 'this', 'photo', 'taken', 'looking', 'through', '?']]\n",
      "[[(['net'], 1)]]\n",
      "[['what', 'position', 'is', 'this', 'man', 'playing', '?']]\n",
      "[[(['pitcher'], 1), (['catcher'], 0.3)]]\n",
      "[['what', 'color', 'is', 'the', 'players', 'shirt', '?']]\n",
      "[[(['orange'], 1)]]\n",
      "[['is', 'this', 'man', 'a', 'professional', 'baseball', 'player', '?']]\n",
      "[[(['yes'], 1), (['no'], 0.3)]]\n",
      "[['what', 'color', 'is', 'the', 'snow', '?']]\n",
      "[[(['white'], 1)]]\n",
      "gloves type: <class 'dict'>8.77% done)\t\n",
      "Most frequent words in the dataset:\n",
      "(658393, '?')\n",
      "(479360, 'the')\n",
      "(411881, 'is')\n",
      "(274603, 'what')\n",
      "(185388, 'yes')\n",
      "(184586, 'no')\n",
      "(153607, 'are')\n",
      "(129382, 'this')\n",
      "(105304, 'on')\n",
      "(105095, 'in')\n",
      "(87906, 'of')\n",
      "(85738, 'a')\n",
      "(79767, 'how')\n",
      "(76626, 'many')\n",
      "(68052, 'color')\n",
      "(55419, 'there')\n",
      "(41953, 'white')\n",
      "(41758, 'man')\n",
      "(33767, 'does')\n",
      "(33226, '2')\n",
      "Total number of words: 6310482\n",
      "Number of unique words in dataset: 16362\n",
      "Number of words in GloVe: 2196018\n",
      "Number of unique words in unk: 356/16362 = 2.18%\n",
      "Total number of unk words: 613/6310482 = 0.01%\n",
      "Max length question: 25\n",
      "Length distribution of questions (length, count):\n",
      " 0:          0 \t 0.000000%\n",
      " 1:          0 \t 0.000000%\n",
      " 2:          0 \t 0.000000%\n",
      " 3:         25 \t 0.003799%\n",
      " 4:      17744 \t 2.696202%\n",
      " 5:      87723 \t 13.329514%\n",
      " 6:     169979 \t 25.828318%\n",
      " 7:     131214 \t 19.937974%\n",
      " 8:     109221 \t 16.596137%\n",
      " 9:      68442 \t 10.399765%\n",
      "10:      33265 \t 5.054618%\n",
      "11:      17921 \t 2.723097%\n",
      "12:      10300 \t 1.565086%\n",
      "13:       5375 \t 0.816732%\n",
      "14:       2865 \t 0.435337%\n",
      "15:       1687 \t 0.256340%\n",
      "16:       1006 \t 0.152862%\n",
      "17:        525 \t 0.079774%\n",
      "18:        386 \t 0.058653%\n",
      "19:        204 \t 0.030998%\n",
      "20:        134 \t 0.020361%\n",
      "21:         65 \t 0.009877%\n",
      "22:         12 \t 0.001823%\n",
      "23:         15 \t 0.002279%\n",
      "24:          1 \t 0.000152%\n",
      "25:          2 \t 0.000304%\n",
      "Max length answer: 6\n",
      "Length distribution of answers (length, count):\n",
      " 0:          0 \t 0.000000%\n",
      " 1:    1278080 \t 91.934643%\n",
      " 2:      73781 \t 5.307203%\n",
      " 3:      36069 \t 2.594509%\n",
      " 4:       1883 \t 0.135448%\n",
      " 5:        279 \t 0.020069%\n",
      " 6:        113 \t 0.008128%\n"
     ]
    }
   ],
   "source": [
    "print(\"Process trainval dataset...\")\n",
    "trainval_set = json.load(open(os.path.join(data_path, data_map_vqa[\"trainval\"]), \"r\"))\n",
    "comp_trainval = json.load(open(data_map_vqa[\"train_comp_path\"], \"r\"))\n",
    "comp_trainval.extend(json.load(open(data_map_vqa[\"val_comp_path\"], \"r\")))\n",
    "\n",
    "ans2idx, idx2ans, word2idx, idx2word, trainval_set, max_len_ques, poss_answers = \\\n",
    "    process_dataset(trainval_set, num_occurs, glove_path, max_ques, max_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = trainval_set\n",
    "trainval_dict = dict()\n",
    "for data in dataset:\n",
    "    ques_dict = dict()\n",
    "    ques_dict[data['ques_id'][0]] = {\"question\": \" \".join(data[\"processed_ques\"][0][0:14]), \\\n",
    "                                    \"answer\": data[\"processed_ans\"][0],\\\n",
    "                                    \"answer_id\": data['ans_id'][0]}\n",
    "    \n",
    "    if str(data['id']) in trainval_dict:\n",
    "        trainval_dict[str(data['id'])].append(ques_dict)\n",
    "    if str(data['id']) not in trainval_dict:\n",
    "        trainval_dict[str(data['id'])] = [ques_dict]\n",
    "        \n",
    "with open(\"./VQA_pickles/trainval.pickle\",\"wb\") as file:\n",
    "    pickle.dump(trainval_dict, file)\n",
    "\n",
    "with open(\"./VQA_pickles/idx2ans.pickle\",\"wb\") as file:\n",
    "    pickle.dump(idx2ans, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process testdev dataset...\n",
      "Tokenizing questions and answers...\n",
      "[['what', 'credit', 'card', 'company', 'is', 'on', 'the', 'banner', 'in', 'the', 'background', '?']]\n",
      "[['is', 'the', 'pitcher', 'wearing', 'a', 'hat', '?']]\n",
      "[['is', 'the', 'ball', 'flying', 'towards', 'the', 'batter', '?']]\n",
      "[['are', 'the', 'horses', 'playing', 'a', 'game', '?']]\n",
      "[['what', 'is', 'the', 'color', 'of', 'water', 'in', 'the', 'image', '?']]\n",
      "Process test dataset...4 (93.12% done)\t\n",
      "Tokenizing questions and answers...\n",
      "[['is', 'the', 'ball', 'flying', 'towards', 'the', 'batter', '?']]\n",
      "[['what', 'sport', 'is', 'this', '?']]\n",
      "[['can', 'you', 'see', 'the', 'ball', '?']]\n",
      "[['is', 'the', 'pitcher', 'wearing', 'a', 'hat', '?']]\n",
      "[['will', 'he', 'catch', 'the', 'ball', 'in', 'time', '?']]\n",
      "processing 440000/447793 (98.26% done)\t\r"
     ]
    }
   ],
   "source": [
    "print(\"Process testdev dataset...\")\n",
    "testdev_set = json.load(open(os.path.join(data_path, data_map_vqa[\"testdev\"]), \"r\"))\n",
    "\n",
    "testdev_set = process_text(testdev_set, without_ans=True)\n",
    "testdev_set = filter_unk_word(testdev_set, word2idx, without_ans=True)\n",
    "\n",
    "print(\"Process test dataset...\")\n",
    "test_set = json.load(open(os.path.join(data_path, data_map_vqa[\"test\"]), \"r\"))\n",
    "\n",
    "test_set = process_text(test_set, without_ans=True)\n",
    "test_set = filter_unk_word(test_set, word2idx, without_ans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dev_data = testdev_set\n",
    "test_dev_dict = dict()\n",
    "for data in test_dev_data:\n",
    "    ques_dict = dict()\n",
    "    ques_dict[data['ques_id'][0]] = {\"question\": \" \".join(data[\"processed_ques\"][0][0:14])}\n",
    "    \n",
    "    if str(data['id']) in test_dev_dict:\n",
    "        test_dev_dict[str(data['id'])].append(ques_dict)\n",
    "    if str(data['id']) not in test_dev_dict:\n",
    "        test_dev_dict[str(data['id'])] = [ques_dict]\n",
    "        \n",
    "with open(\"./VQA_pickles/test_dev.pickle\",\"wb\") as file:\n",
    "    pickle.dump(test_dev_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_set\n",
    "test_dict = dict()\n",
    "for data in test_data:\n",
    "    ques_dict = dict()\n",
    "    ques_dict[data['ques_id'][0]] = {\"question\": \" \".join(data[\"processed_ques\"][0][0:14])}\n",
    "    \n",
    "    if str(data['id']) in test_dict:\n",
    "        test_dict[str(data['id'])].append(ques_dict)\n",
    "    if str(data['id']) not in test_dict:\n",
    "        test_dict[str(data['id'])] = [ques_dict]\n",
    "\n",
    "with open(\"./VQA_pickles/test.pickle\",\"wb\") as file:\n",
    "    pickle.dump(test_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
